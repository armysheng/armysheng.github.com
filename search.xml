<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Mockito Spy 用法]]></title>
    <url>%2F2017%2F07%2F30%2Fmockito-spy%2F</url>
    <content type="text"></content>
      <categories>
        <category>tech</category>
      </categories>
      <tags>
        <tag>mockito</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mokito When/Then 用法]]></title>
    <url>%2F2017%2F07%2F30%2Fmockito-behavior%2F</url>
    <content type="text"><![CDATA[概述 本文用大量的例子和用例来阐述如何用Mockito来配置一些调用行为，本文的组织形式偏向于用法的举例与实践，将不对额外的细节做介绍。 如果你想查看其它的使用Mockito测试的文章，请参见Mockito 首先我们仍然要和上一篇文章一样Mock一个简单的list： 1234567891011public class MyList extends AbstractList&lt;String&gt; &#123; @Override public String get(final int index) &#123; return null; &#125; @Override public int size() &#123; return 1; &#125;&#125; 用法 为Mock对象配置简单的返回值 12345678@Testpublic final void whenMockReturnBehaviorIsConfigured_thenBehaviorIsVerified() &#123; final MyList listMock = Mockito.mock(MyList.class); when(listMock.add(anyString())).thenReturn(false); final boolean added = listMock.add(randomAlphabetic(6)); assertThat(added, is(false));&#125; 另一种配置返回值的方法 12345678@Testpublic final void whenMockReturnBehaviorIsConfigured2_thenBehaviorIsVerified() &#123; final MyList listMock = Mockito.mock(MyList.class); doReturn(false).when(listMock).add(anyString()); final boolean added = listMock.add(randomAlphabetic(6)); assertThat(added, is(false));&#125; 为mock对象的指定方法配置抛出异常 1234567@Test(expected = IllegalStateException.class) public final void givenMethodIsConfiguredToThrowException_whenCallingMethod_thenExceptionIsThrown() &#123; final MyList listMock = Mockito.mock(MyList.class); when(listMock.add(anyString())).thenThrow(IllegalStateException.class); listMock.add(randomAlphabetic(6)); &#125; 为返回值为空的方法配置行为（抛出异常） 1234567@Test(expected = NullPointerException.class)public final void whenMethodHasNoReturnType_whenConfiguringBehaviorOfMethod_thenPossible() &#123; final MyList listMock = Mockito.mock(MyList.class); doThrow(NullPointerException.class).when(listMock).clear(); listMock.clear();&#125; 为多次调用配置行为，第一次调用不抛出异常 1234567@Testpublic final void givenBehaviorIsConfiguredToThrowExceptionOnSecondCall_whenCallingOnlyOnce_thenNoExceptionIsThrown() &#123; final MyList listMock = Mockito.mock(MyList.class); when(listMock.add(anyString())).thenReturn(false).thenThrow(IllegalStateException.class); listMock.add(randomAlphabetic(6));&#125; 第二次调用时抛出异常 12345678@Test(expected = IllegalStateException.class)public final void givenBehaviorIsConfiguredToThrowExceptionOnSecondCall_whenCallingTwice_thenExceptionIsThrown() &#123; final MyList listMock = Mockito.mock(MyList.class); when(listMock.add(anyString())).thenReturn(false).thenThrow(IllegalStateException.class); listMock.add(randomAlphabetic(6)); listMock.add(randomAlphabetic(6));&#125; 配置mock对象调用真实的方法 1234567@Testpublic final void whenMockMethodCallIsConfiguredToCallTheRealMethod_thenRealMethodIsCalled() &#123; final MyList listMock = Mockito.mock(MyList.class); when(listMock.size()).thenCallRealMethod(); assertThat(listMock.size(), equalTo(1));&#125; 为mock方法配置自定义的回答 12345678@Testpublic final void whenMockMethodCallIsConfiguredWithCustomAnswer_thenRealMethodIsCalled() &#123; final MyList listMock = Mockito.mock(MyList.class); doAnswer(invocation -&gt; "Always the same").when(listMock).get(anyInt()); final String element = listMock.get(1); assertThat(element, is(equalTo("Always the same")));&#125; 总结 目前列举这些例子，如果有用到新的场景会继续添加，例子的代码可以参见github]]></content>
      <categories>
        <category>tech</category>
      </categories>
      <tags>
        <tag>Mockito</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mockito Verify 用法]]></title>
    <url>%2F2017%2F07%2F25%2FMockito-Verify-Cookbook%2F</url>
    <content type="text"><![CDATA[概述 本文列举了如何在各种场景下使用Mockito verify方法,本文的组织形式偏向于用法的举例与实践，将不对额外的细节做介绍。 首先我们先mock一个简单的list实现： 1234567891011public class MyList extends AbstractList&lt;String&gt; &#123; @Override public String get(final int index) &#123; return null; &#125; @Override public int size() &#123; return 0; &#125;&#125; 用法 验证Mock对象的简单调用 123List&lt;String&gt; mockedList = mock(MyList.class);mockedList.size();verify(mockedList).size(); 验证Mock对象的调用次数 123List&lt;String&gt; mockedList = mock(MyList.class);mockedList.size();verify(mockedList, times(1)).size(); 验证整个Mock对象的都没有被调用 12List&lt;String&gt; mockedList = mock(MyList.class);verifyZeroInteractions(mockedList); 验证Mock对象的某个方法没有被调用 12List&lt;String&gt; mockedList = mock(MyList.class);verify(mockedList, times(0)).size(); 验证Mock对象的没有意料之外的调用--此案例会失败 12345List&lt;String&gt; mockedList = mock(MyList.class);mockedList.size();mockedList.clear();verify(mockedList).size();verifyNoMoreInteractions(mockedList); 验证调用顺序 123456789List&lt;String&gt; mockedList = mock(MyList.class);mockedList.size();mockedList.add("a parameter");mockedList.clear(); InOrder inOrder = Mockito.inOrder(mockedList);inOrder.verify(mockedList).size();inOrder.verify(mockedList).add("a parameter");inOrder.verify(mockedList).clear(); 验证调用没有发生 123List&lt;String&gt; mockedList = mock(MyList.class);mockedList.size();verify(mockedList, never()).clear(); 验证某个方法至少或者之多被调用的次数 1234567List&lt;String&gt; mockedList = mock(MyList.class);mockedList.clear();mockedList.clear();mockedList.clear(); verify(mockedList, atLeast(1)).clear();verify(mockedList, atMost(10)).clear(); 准确验证方法调用时的参数 123List&lt;String&gt; mockedList = mock(MyList.class);mockedList.add("test");verify(mockedList).add("test"); 使用flexible和any参数 123List&lt;String&gt; mockedList = mock(MyList.class);mockedList.add("test");verify(mockedList).add(anyString()); 使用参数捕获器 123456List&lt;String&gt; mockedList = mock(MyList.class);mockedList.addAll(Lists.&lt;String&gt; newArrayList("someElement"));ArgumentCaptor&lt;List&gt; argumentCaptor = ArgumentCaptor.forClass(List.class);verify(mockedList).addAll(argumentCaptor.capture());List&lt;String&gt; capturedArgument = argumentCaptor.&lt;List&lt;String&gt;&gt; getValue();assertThat(capturedArgument, hasItem("someElement")); 总结 目前只列举这些例子，如果有用到新的场景会继续添加，例子的代码可以参见github]]></content>
      <categories>
        <category>tech</category>
      </categories>
      <tags>
        <tag>Mockito</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理JVM之-Java内存区域与内存溢出]]></title>
    <url>%2F2017%2F07%2F11%2FUnderstanding-the-JVM-01%2F</url>
    <content type="text"><![CDATA[Java 与 C++之间有一度有内存动态分配和垃圾收集技术所谓成的“高墙”，墙外的人想进去，墙里面的人却想出来 概述 对于从事C、C++程序开发的开发人员来说，在内存管理领域，他们即是拥有最高权利的“皇帝”又是从事最基础工作的“劳作人吗”————既拥有每一个对象的所有权，又担负着每一个对象声明开会到总结的维护责任。 对于Java程序员来说，在虚拟机的自动内存管理机制的帮助下，不再需要为诶一个new操作去写配对的delete/free代码，不容易出现内存泄露和内存溢出的问题，有虚拟机管理内存这一切看起来很美好。不过，也正是因为java程序员把内存控制的权利交给了jvm，一旦出现内存泄露和溢出方面的问题，如果不了解虚拟机是怎么样使用内存的，那么排查错误将会成为一项异常艰难的工作。 下面我们来介绍一下java虚拟机内存的各个区域。 运行时数据区域 我们看一下java虚拟机在运行时数据区 jvm在执行java程序时，它所管理的内存区域会被划分为以上几个不同的数据区域。这些区域我们分别做如下介绍： 程序计数器 程序计数器（Program Counter Register）是一块较小的内存空间，它可以看做当前线程所执行的字节码的行号指示器（即记录当前线程执行到了哪里）。在虚拟机的概念模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功都需要以来这个计数器来完成。 线程私有：为了线程切换后能够恢复到正确的执行位置，没条线程都需要有一个独立的程序计数器。因此线程之间技术器互不影响，独立存储。 异常 此内存区域区域是唯一一个在jvm规范中没有规顶任何OutofMemoryErro情况的区域（试想一下，每个计数器的存储非常小，每个线程一个计数器，得起多少线程才能把内存撑爆） Java虚拟机栈（JVM Stacks） 与程序计数器一样，栈也是线程私有的，它的生命周期与线程相同。它描述的是java方法执行的内存模型： 每个方法执行是都会创建一个栈帧（Stack Frame），用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至执行完成的过程，就对于这一个栈帧在虚拟机栈中到出栈的过程。 局部变量表存放了编译期可知的各种基本数据类型（boolean，byte，char，short，int，float，long，double）、对象引用（reference类型，它不等同于对象本身，可能是一个纸箱对象起始地址的引用指针，也可能是纸箱一个代表对象的句柄或者其他与此对象的相关位置）和returnAddress类型（指向了一条字节码指令的地址） 局部变量表在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。 其中64位长度的long和double类型的数据会占用2个局部变量空间（slot），其余的数据类型占1个 异常 如果线程请求栈的深度大于虚拟机所允许的深度，将抛出StackOverflowError异常（递归时常见） 如果jvm的栈空间可以动态扩展，当jvm的栈申请更多空间时，无法得到足够的内存，会抛出OutOfMemoryError异常 例子： 123456789101112131415161718192021222324/** * VM Args: -Xss128k * * @author armysheng */public class JavaVMStackSOF &#123; private int stackLength = 1; public void stackLeak() &#123; stackLength++; stackLeak(); &#125; public static void main(String[] args) throws Throwable &#123; JavaVMStackSOF oom = new JavaVMStackSOF(); try &#123; oom.stackLeak(); &#125; catch (Throwable e) &#123; System.out.println("stack length:" + oom.stackLength); throw e; &#125; &#125;&#125; 运行结果： 123456stack length:1151Exception in thread &quot;main&quot; java.lang.StackOverflowError at udacity.storm.JavaVMStackSOF.stackLeak(JavaVMStackSOF.java:13) at udacity.storm.JavaVMStackSOF.stackLeak(JavaVMStackSOF.java:13) at udacity.storm.JavaVMStackSOF.stackLeak(JavaVMStackSOF.java:13) ... 实验结果表明：在单个线程下，无论是由于栈帧太大还是虚拟机栈容量太小，当内存无法分配的时候，抛出的都是StackOverFlow异常。 如果测试时不限于单线程，通过不断的建立线程的方式倒是可以产生内存溢出异常。 本地方法栈（Native Method Stack） 本地方法栈与虚拟机所发挥的作用是非常相似的，他们之间的区别不过是虚拟机方法栈为虚拟机执行java方法服务，而本地方法栈则为虚拟机使用到Native。 虚拟机规范对本地方法栈使用的语言、使用发式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它； 甚至有的虚拟机（譬如Sun HotSpot）直接把本地方法栈和虚拟机栈合二为一。 异常同虚拟机栈 Java 堆（Heap） 对于大多数应用来说，java堆是java虚拟机所管理的内存中最大的一块。 线程共享： java堆是被所有线程共享的一块内存区域，在虚拟机启动事创建。 java 堆是垃圾收集器管理的主要区域，因此很多时候被称作“GC堆“ java堆还可以细分为：新生代和老生代;在细一点的有Eden空间、From Survivor空间、ToSurvivor空间。 堆内存可以通过-Xms 和 -Xms控制 异常 如果在堆内存没有内存完成实例分配，并且也无法再拓展时，将会抛出OutOfMemoryError异常，见下例： 12345678910111213141516171819import java.util.ArrayList;import java.util.List;/** * Created by armysheng on 17/7/16. * VM Args: -Xms20m -Xms20m -XX:+HeapDumpOnOutOfMemoryError */public class HeapOOM &#123; public static void main(String[] args) &#123; List&lt;OOMObject&gt; list = new ArrayList&lt;&gt;(); while (true) &#123; list.add(new OOMObject()); &#125; &#125; static class OOMObject &#123; &#125;&#125; 运行结果： 12345678910111213141516java.lang.OutOfMemoryError: Java heap spaceDumping heap to java_pid17073.hprof ...Heap dump file created [2313984338 bytes in 19.357 secs]Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space at java.util.Arrays.copyOf(Arrays.java:2245) at java.util.Arrays.copyOf(Arrays.java:2219) at java.util.ArrayList.grow(ArrayList.java:242) at java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:216) at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:208) at java.util.ArrayList.add(ArrayList.java:440) at udacity.storm.HeapOOM.main(HeapOOM.java:14) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144) 方法区（Method Area） 线程共享：方法区和Java堆一样，是各个线程共享的内存区域，它用于存储已被加载的类信息、常量、静态变量、即时编译器编译后的代码等数据 方法区很多人愿意把它称之为“永久代”，仅仅是因为HotSpot虚拟机GC分带收集拓展至方法区，这样jvm就可以像管理java堆一样管理这部分内存 运行时常量池 运行时常来是方法去的一部分，Class文件中出了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池，用于存放编译器的生成的各种字面量和符号引用。 直接内存 直接内存不是虚拟机运行时数据区的一部分，也不是java虚拟机规范中定义的内存区域。但这部分内存也被频繁的实验，而且也可能导致OutOfMemoryError异常出现。 在JDK1.4中新加入了NIO（New Input/Output）类，引入了一种基于通道（Channel）与缓冲区（Buffer）的I/O方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在java队中的DirectByteBuffer对象，作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在java堆和Native堆中来回复制数据。 总结 本文的总结了虚拟机中的内存是如何划分，哪部分区域、什么样的代码和操作可能导致内存溢出异常。]]></content>
      <categories>
        <category>tech</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客迁移到github.io小记]]></title>
    <url>%2F2017%2F07%2F05%2F%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E5%88%B0github-io%2F</url>
    <content type="text"><![CDATA[缘起 由farbox的退出历史舞台，我深刻的认识到靠第三方服务还是靠不住，所有东西在自己管控中是非常必要的。因此原来的博客迁移任务老早就放上了议程。 虽然这个年代了博客不在流行，自己也不咋写，但是觉得逼格还是要上去。本打算用Vue.js写一套博客系统，所有都从头做起搞个项目，自己用的同时，又可以练手vue也可以开源出去。然而，到最后看了一下github.io和hexo的解决方案貌似已经很成熟，几行命令便把环境搭好，还有主题市场，和qq空间皮肤一样可以自由选择，还不需要买黄砖哦_(:з」∠)_），非常方便，所以（其实就是懒）就选择了这个方案。 迁移 博客使用了是圈子里非常流行的Next主题（见下方）配置简单，文档齐全，比较良心（就是有些大众脸，简书好像也是用的这套）。 主要遇到的问题有这么几个： 之前的farbox是吧.md,.txt等好多文本形式的文件网Dropbox一丢，就能生成一篇文章，hexo则只支持markdown文件，并且有专门的Front-matter来定义文章的标题，时间，标签，分组等信息。所以每篇文章都要进行一定的修改。 hexo的图片引用如果都放在一个文件包里会非常乱，大部分会推荐通过配置用同名文件夹的方式使用相对路径，这样引起来会比较方便 上面配置的用法，有一个坑就是使用相对路径后，文章列表页的预览图会不能渲染，可以用配置中提到的插件标签方式，还有一种方式，就是自己修改主题，来实现Blog摘要配图，并且文章中没有该图的效果。 写作 hexo 用起来非常方便，常用的几个命令： hexo new post [title] //新建文章title hexo g //生成网页模板 hexo d // 部署到服务器（可直接推github） 不过感觉github.io的访问速度还是些有问题，后面会看看国内的Coding托管起来会不会方便（不清楚要不要备案）。 域名也要挂上，嗯，逼格满满。😏～(￣▽￣～)(～￣▽￣)～ 好了，有了好环境，希望以后能多写点吧。 === Update 2017-7-16 目前博客已经托管到Coding.net,访问域名改为 youngfor.me, blog.youngfor.me 或者 www.youngfor.me 。]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hibernate之实体类状态入门（entity state transitions）]]></title>
    <url>%2F2016%2F12%2F22%2FTech%2FHibernate%20%20and%20JPA%20%E5%AE%9E%E4%BD%93%E7%B1%BB%E7%8A%B6%E6%80%81%E8%BD%AC%E5%8F%98%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[如果你也不是太理解Hibernate 什么时候要save，什么时候要用update，merge，什么时候可以直接set一下字段就能够将实体类存储到数据库中，这篇文章或许能解开你的迷雾。本文翻译自《A beginner’s guide to JPA/Hibernate entity state transitions 》 引言 Hibernate帮助开发者的将SQL语句的转变成了实体类的状态转变。一旦Hibernate存储了一个实体类，它的所有变化都将会自动传播到数据库中。 管理领域模型实体类以及他们的关联关系比写SQL语句管理SQL语句来说简单多了。如果没有Hibernate这样的ORM工具，增加一个字段会需要改写所有关联的 INSERT/UPDATE语句。 但是Hibernate也并不是银弹。Hibernate也不能让我们完全不关心所最终执行的SQL语句。完全的掌握Hibernate也不是我们所想的那么简单直接，我们有时候也需要亲自检查所产生的SQL语句。 实体类状态 如之前所说，Hibernate监管了当前存储着的实体类。但是在一个实体类要被存储下来，我们需要知道它处于什么状态。 首先我们必须定义所有实体类状态: New(Transient)瞬态 一个新创建的对象，在还没有进入Hibernate Session(也即持久化上下文)，同时没有域数据库表中的行对应起来时的状态称为瞬态。 想要对处于这个状态的对象进行持久化，我们需要明确的执行EnittyManager的persist或者使用是实体类持久化的传播机制（CasCade）。 Persistent（Managed）持久态 持久态的实体类已经和数据表中的行关联起来，并且被当前的持久化上下文所管理。所有该实体类的变化都会被检测到然后在事务flush的时候，将这些变化存储到数据库中。有了Hibernate，我们就不在需要执行增删改查的SQL语句。Hibernate使用了Transactional write-behind的工作方式。在当前事务flush时间的最后一个将实体类的改变同步到数据库中。 detached 分离态 一旦当前的跑着的事务关闭，所有当前状态下管理着的实体类就编程了分离太。后续的实体类中的变化将不会被检测到，并且也不会和自动和数据库去同步。 如果想在一个当前事务关联一个将分离态的实体类，你可以选着下面的几种方式： Reattaching（重新关联） Hibernate支持通过Session的update方法来重新关联实体类。 Merging 合并操作会将分离态的实体类（源头）拷贝一份给到持久太的实体类（目标）。如果持久化的实体类在当前事务中不存在，那么就回去数据库中查一份。 但是分类态的实体类（源头人就会保持分离态）。 removed 删除态 尽管JPA需要只有持久态的实体类才能够被删除，hibernate通用可以通过Session的delete方法删除分离态的实体类。 实体类状态转换 想要改变一个实体类的状态，我们需要使用下面之中的实体类管理接口： EntityManager *Session 总结 以上这些接口是我们在实体类状态转变时候需要明确调用的，这些调用通知hibernate在事务刷新的时候将这些变换转换成SQL语句，最终去操作数据库。]]></content>
      <categories>
        <category>tech</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[设计模式-观察者模式（高效的发声）]]></title>
    <url>%2F2016%2F12%2F06%2FTech%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[观察者模式 2016-12-8 11:08:06 观察者模式又叫发布-订阅模式，它定义了一种一对多的依赖关系，多个观察者对象可同时监听某一主题对象，当该主题对象状态发生变化时，相应的所有观察者对象都可收到通知。这是一个非常好理解的模式，简单介绍如下： 场景 假设我们有这样一个需求，我们的新闻博客写的不错，有几个读者向订阅我们的新闻。这样以便于我们的博客一有新闻更新，就可以推送到用户了。简单的我们可以这么写 1234567 public class DisgraceNewsPublisher &#123; public void newsPublished() &#123; News news = getLatestNews(); jack.update(news); amy.update(news); &#125;&#125; 但是我们发现如果我们的用户Amy对新闻内容不满意，想退订新闻的话，我们的发布新闻代码得更改，同时我们的新用户Eric想要订阅，代码又得重新改，这显然违背了开闭原则。这样做的问题有： 我们是针对具体实现编程，而非针对接口。 对于每个新的用户，我们都得修改代码。 我们无法再运行时动态地增加（或删除）用户。 用户没有实现一个公共的接口。所有用户都是update方法 我们尚未封装改变的部分。 观察者模式 这个时候我们的观察者模式就登场了，观察者模式定义了对象之间的一对多依赖，这样一来，当一个对象改变状态时，它的所有依赖者都会受到通知并自动更新。观察者模式一般会有如下角色： 抽象主题角色：把所有对观察者对象的引用保存在一个集合中，每个抽象主题角色都可以有任意数量的观察者。抽象主题提供一个接口，可以增加和删除观察者角色。一般用一个抽象类和接口来实现。 抽象观察者角色：为所有具体的观察者定义一个接口，在得到主题的通知时更新自己。 具体主题角色：在具体主题内部状态改变时，给所有登记过的观察者发出通知。具体主题角色通常用一个子类实现。 具体观察者角色：该角色实现抽象观察者角色所要求的更新接口，以便使本身的状态与主题的状态相协调。通常用一个子类实现。如果需要，具体观察者角色可以保存一个指向具体主题角色的引用。 它的类图如下： 松耦合的威力 观察者模式实现了松耦合，就如上文的例子，新闻的发布者和用户之间会有交互，但他们不需要清楚彼此的细节。 当两个对象之间松耦合，他们依然可以互交，但是不太清楚彼此的细节。 观察者模式提供了一种对象设计，让主题和观察者之间松耦合。 关于观察者的一切，主题只知道观察者实现了某个接口（也就是Observer接口）。主题不需要知道观察者的具体类是谁，做了些什么或者其他任何细节。任何时候我们都可以增加新的观察者。因为主题唯一依赖的东西是一个实现Observer接口的对象列表，所以我们可以随时增加观察者。事实上，在运行时我们可以用心的观察者取代现有的观察者，主题不会受到任何影响。同样的，也可以在任何时候删除某些观察者。有新类型的观察者出现时，主题的代码不需要修改，假如我们有个新的具体类需要当观察者，我们不需要为了兼容新类型而修改主题的代码，所有要做的就是在新的类里实现此观察者接口，人后注册为观察者即可，主题不在乎别的，它只会发送通知给所有实现了观察者接口的对象。 新闻发布代码实现 Subject.java 主题的抽象接口 12345public interface Subject &#123; void registerObserver(Observer observer); void removeObserver(Observer observer); void notifyObservers();&#125; Observer.java 观察者的抽象接口 123public interface Observer &#123; void update(String args);&#125; NewsPublisher.java 新闻发布者--主题的具体类 1234567891011121314151617181920212223242526public class NewsPublisher implements Subject &#123; private ArrayList&lt;Observer&gt; observers; public NewsPublisher() &#123; observers = new ArrayList&lt;Observer&gt;(); &#125; public void registerObserver(Observer observer) &#123; observers.add(observer); &#125; public void removeObserver(Observer observer) &#123; observers.remove(observer); &#125; public void notifyObservers() &#123; for (Observer o : observers) &#123; o.update(getLatestNews()); &#125; &#125; private String getLatestNews() &#123; return "this is latest news"; &#125;&#125; NewsRegisterJack.java注册用户jack 12345public class NewsRegisterJack implements Observer &#123; public void update(String latestNews) &#123; System.out.println("Jack is getting the news ..." +latestNews); &#125;&#125; NewsRegisterAmy.java注册用户amy 12345public class NewsRegisterAmy implements Observer &#123; public void update(String latestNews) &#123; System.out.println("Amy is getting the news ..." +latestNews); &#125;&#125; 我们来做一下测试： 12345678910111213141516public class ObserverTest &#123; public static void main(String[] args) &#123; Subject subject = new NewsPublisher(); Observer jack = new NewsRegisterJack(); Observer amy = new NewsRegisterAmy();// notify when nobody is subscribed. subject.notifyObservers();// notify when both user is subscribed. subject.registerObserver(jack); subject.registerObserver(amy); subject.notifyObservers();// notify when amy is unsubscribed. subject.removeObserver(amy); subject.notifyObservers(); &#125;&#125; 输出如下 12345Jack is getting the news ...this is latest newsAmy is getting the news ...this is latest newsJack is getting the news ...this is latest newsProcess finished with exit code 0 观察者模式优缺点 观察者模式优点 抽象主题只依赖于抽象观察者 观察者模式支持广播通信 观察者模式使信息产生层和响应层分离 观察者模式缺点 如一个主题被大量观察者注册，则通知所有观察者会花费较高代价 如果某些观察者的响应方法被阻塞，整个通知过程即被阻塞，其它观察者不能及时被通知 观察者模式与OOP原则 已遵循的原则 依赖倒置原则（主题类依赖于抽象观察者而非具体观察者） 迪米特法则 里氏替换原则 接口隔离原则 单一职责原则 开闭原则 参考 Head First设计模式读书总结——观察者模式 设计模式——观察者模式：天气推送的两种实现 观察者模式中，消息采用推和拉方式来传递的比较]]></content>
      <categories>
        <category>tech</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SALT十分钟入门]]></title>
    <url>%2F2016%2F08%2F30%2FTech%2FSALT%E5%8D%81%E5%88%86%E9%92%9F%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[SALT 十分钟入门 2016年8月29日13:59:12 最近快要成为为运维了，各种在服务器集群上部署应用，最早以为写一个shell脚本，在每台服务器上运行一下就够了。但是服务器涨到30台、50台时，每次部署，每次做一些修改，就显得非常头疼了。如果有一个工具可以让你们在一台主机机子上完成对所有其他服务器的配置，那真是一件非常爽的事情。 Salt就是这么一个工具！ Salt Salt 是一种新的基础设备管理方法.用几秒中就能过把环境打起来,可以拓展管理成百上前台服务器,并且与这些服务器保持毫秒级通信. Saltstack使用Python开发的，非常简单易用和轻量级的管理工具。由Master和Minion构成，通过ZeroMQ进行通信。可能是由于Python开发的原因,Salt的配置也是超级简单方便.基本能够在几分钟内就可以配置完成. 安装 Salt 以来与epel,如果你的集群上没有安装则需要: 1rpm -ivh http://mirrors.sohu.com/fedora-epel/6/x86_64/epel-release-6-8.noarch.rpm 或者 1wget http://dl.cpis-opt.com/huanw/shencan/epel-release-5-4.noarch.rpm &amp;&amp; rpm -vih epel-release-5-4.noarch.rpm 之后只需要在服务器和客户机进行不同安装 服务端安装salt-master 1yum install salt-master -y 客户端安装salt-minion 1yum install salt-minion -y 这里的minion是被salt中的被控制的从机的意思. 启动服务 service salt-master start 同时salt-master也支持命令行启动 1salt-master -d 调试启动 1salt-master -l debug Salt Master 需要绑定的两个端口4505和4506. 如果你将Master主机设置的名字设置为salt,那么网络内的所有minion(从机)都会在启动的时候搜索到这台机子.否则的话则需要在minion上做如下配置,声明下master机子是哪一台: 修改/etc/salt/minion:文件,将master那栏解注,然后填入master的IP(10.101.10.10) 1master: 10.101.10.10 同理,启动minion: 1salt-minion -d 使用SALT-KEY SALY使用SALT-KEY来进行认证交互,所以对于minion要想收到master的命名,必须让master接受自己的key. slat-key命令就是用来管理master机子上的keys的. 列出master机子上的所有keys: 1salt-key -L 这个命令会列出被master接受的(accepted),拒绝的(rejected )以及还处于待处理状态(pending)的key.我们可以使用下面的命令接受所有pending状态的keys: 1salt-key -A 注:建议master在接受minion的key时,使用salt-key -f minion-id命令查看minion的可以,并且在minion上运行salt-call key.finger --loacl查看minion本机的key.如果两者匹配,主机再选择接受这个minon. 主机运行: 123# salt-key -f foo.domain.comUnaccepted Keys:foo.domain.com: 39:f9:e4:8a:aa:74:8d:52:1a:ec:92:03:82:09:c8:f9 minion上运行: 123# salt-call key.finger --locallocal: 39:f9:e4:8a:aa:74:8d:52:1a:ec:92:03:82:09:c8:f9 如果两者匹配则在master上运行salt-key -a foo.domain.com来接受这台minion. 发送第一个命令 连接成功后,就可以从master对minion发送命令了,一个简单的向所有minon发送ping的命令: 1salt '*' test.ping 其中*表示命令执行的对象. test.ping告诉minion来运行test.ping的函数. 对于test.ping这部分,test是一个salt的执行模块,ping指代这个模块中的ping函数. 更多的执行模块可以参见:(执行模块)[https://docs.saltstack.com/en/latest/ref/modules/index.html] 查看磁盘信息 1salt '*' disk.usage 了解salt函数 salt自带了一系列的库函数,可以通过下列命令查看minions上可以执行的函数: 1salt '*' sys.doc 当然你也可以在web上找到这些文档 一些常用的函数 执行shell脚本 cmd的模块可以帮助你在master上对minion执行shell操作 比如说,cmd.run和cmd.run_all 1salt '*' cmd.run 'ls -l /etc' 安装软件包 1salt '*' pkg.install vim 列出所有网络接口 1salt '*' network.interfaces 拷贝文件 方便的向minion发送配置文件 1salt-cp '*' /source/path /target/path 自定义函数 如果自带的函数模块和插件还不能满足你的需求的话,那你还以自己写sls文件,这里就不在叙述 参考 Salt in 10 Minutes]]></content>
      <categories>
        <category>tech</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux IO 模式]]></title>
    <url>%2F2016%2F08%2F13%2FTech%2FLinux%20IO%20%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[最近在研究Netty这个异步事件驱动网络应用框架，这自然得理解一些linux网络编程的故事。 本文讨论的背景是Linux环境下的network IO，大部分内容来自于这篇博客 概念说明 在进行解释之前，首先要说明几个概念： 用户空间和内核空间 进程切换 进程的阻塞 文件描述符 缓存 I/O 用户空间与内核空间 现在操作系统都是采用虚拟存储器，那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。针对linux操作系统而言，将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。 进程切换 为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的。 从一个进程的运行转到另一个进程上运行，这个过程中经过下面这些变化： 保存处理机上下文，包括程序计数器和其他寄存器。 更新PCB信息。 把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列。 选择另一个进程执行，并更新其PCB。 更新内存管理的数据结构。 恢复处理机上下文。 注：总而言之就是很耗资源，具体的可以参考这篇文章：进程切换 进程的阻塞 正在执行的进程，由于期待的默写时间未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作等，则有系统自动执行阻塞（Block），使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得CPU），才可能将其转为阻塞状态。当进程进入阻塞状态，是不占用CPU资源的。当进程进入阻塞状态，是不占用CPU资源的。 文件描述符fd 文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。 文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。 缓存IO 缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。 缓存 I/O 的缺点： 数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。 IO模式 刚才说了，对于一次IO访问（以read举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以说，当一个read操作发生时，它会经历两个阶段： 等待数据准备 (Waiting for the data to be ready) 将数据从内核拷贝到进程中 (Copying the data from the kernel to the process) 正式因为这两个阶段，linux系统产生了下面五种网络模式的方案。 阻塞 I/O（blocking IO） 非阻塞 I/O（nonblocking IO） I/O 多路复用（ IO multiplexing） 信号驱动 I/O（ signal driven IO） 异步 I/O（asynchronous IO） 注：由于signal driven IO在实际中并不常用，所以我这只提及剩下的四种IO Model 阻塞I/O（blocking IO） 在linux中，默认情况下所有的socket都是blocking，一个典型的读操作流程大概是这样 当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据（对于网络IO来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的UDP包。这个时候kernel就要等待足够的数据到来）。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞（当然，是进程自己选择的阻塞）。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。 所以，blocking IO的特点就是在IO执行的两个阶段都被block了。 非阻塞I/O（nonblocking IO） linux下，可以通过设置socket使其变为non-blocking。当对一个non-blocking socket 进行读操作时，流程是这个样子的： 当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度讲 ，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。 所以，nonblocking IO的特点是用户进程需要不断的主动询问kernel数据好了没有。 I/O 多路复用（ IO multiplexing） IO multiplexing就是我们说的select，poll，epoll，有些地方也称这种IO方式为event driven IO。select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。 当用户进程调用了select，那么整个进程会被block，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。 所以，I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select()函数就可以返回。 这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call (select 和 recvfrom)，而blocking IO只调用了一个system call (recvfrom)。但是，用select的优势在于它可以同时处理多个connection。 所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。） 在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。 异步 I/O（asynchronous IO） Linux下的asynchronous IO其实用得很少。先看一下它的流程： 用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。 总结 blocking和non-blocking的区别 调用blocking IO会一直block住对应的进程直到操作完成，而non-blocking IO在kernel还准备数据的情况下会立刻返回。 synchronous IO和asynchronous IO的区别 在说明synchronous IO和asynchronous IO的区别之前，需要先给出两者的定义。POSIX的定义是这样子的： A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes; An asynchronous I/O operation does not cause the requesting process to be blocked; 两者的区别就在于synchronous IO做”IO operation”的时候会将process阻塞。按照这个定义，之前所述的blocking IO，non-blocking IO，IO multiplexing都属于synchronous IO。 有人会说，non-blocking IO并没有被block啊。这里有个非常“狡猾”的地方，定义中所指的”IO operation”是指真实的IO操作，就是例子中的recvfrom这个system call。non-blocking IO在执行recvfrom这个system call的时候，如果kernel的数据没有准备好，这时候不会block进程。但是，当kernel中数据准备好的时候，recvfrom会将数据从kernel拷贝到用户内存中，这个时候进程是被block了，在这段时间内，进程是被block的。 而asynchronous IO则不一样，当进程发起IO 操作之后，就直接返回再也不理睬了，直到kernel发送一个信号，告诉进程说IO完成。在这整个过程中，进程完全没有被block。 各个IO Model的比较如图所示： 通过上面的图片，可以发现non-blocking IO和asynchronous IO的区别还是很明显的。在non-blocking IO中，虽然进程大部分时间都不会被block，但是它仍然要求进程去主动的check，并且当数据准备完成以后，也需要进程主动的再次调用recvfrom来将数据拷贝到用户内存。而asynchronous IO则完全不同。它就像是用户进程将整个IO操作交给了他人（kernel）完成，然后他人做完后发信号通知。在此期间，用户进程不需要去检查IO操作的状态，也不需要主动的去拷贝数据。]]></content>
      <categories>
        <category>tech</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[设计模式-策略模式(会飞的鸭子)]]></title>
    <url>%2F2016%2F07%2F17%2FTech%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[策略模式是一枚经典的设计模式，也非常实用。我们就用《Head First 设计模式》一书中的鸭子游戏的案例，来认识它吧。 先从简单的模拟鸭子应用说起 Joe上班的公司做了一套相当成功的模拟鸭子游戏：SimUDuck。游戏中会出现各种鸭子，一边游泳一边呱呱叫。作为一个OO（面向对象）的程序员，Joe在设计游戏的时候使用了相当标准的OO技术。设计了一个鸭子父类，并让各种鸭子集成这个超类。如下图所示： 然而互联网公司的节奏超快，上线一个月后，公司觉得需要在产品上加入真正创新的东西，来适应激烈的市场竞争。经过一周的头脑风暴，公司的董事会决定在下一次孤独会议上演示一些“真正”让人印象深刻的东西来振奋人心。 现在我们得让鸭子能飞 主管们确定，此模拟游戏需要会飞的鸭子来讲竞争者泡在后头，当然，在这个时候，Joe拍拍胸脯告诉主管们，他只要一个星期就可以搞定，毕竟Joe是一个OO的程序，这有何难。 Joe: 我只要在Duck类上加上fly（）方法，然后所有的鸭子都会继承fly()方法。这是我大显身手展示OO的优越的时候了。 但是可怕的问题发生了... Boss: Joe，我正在股东大会上，刚刚看了一下展示，有很多“橡皮小黄鸭”在屏幕上飞来飞去。你这是在逗我嘛，你可能要开始去逛逛拉勾网了啊。 Joe: 好吧，我好想设计上确实有一点小失误，老大请让我再想想。。。 Joe忽略了一件事，并非Duck的所哟䢮都会飞。joe在Duck超类上加上新的行为，会使得默写不合适该行为的子类也具有该行为。现在可好了SimDuck这个游戏中有了一个无生命的会飞的东西。 Joe 想到了继承 可以把橡皮鸭类中的fly()方法覆盖掉。就好像覆盖quack()的做法一样。因为橡皮鸭子不会呱呱叫，所以quack的定义覆盖成吱吱叫。 Joe:可是，如果以后我加入木头鸭（DecoyDuck）又会如何。木头呀不会飞也不会叫。 Joe认识到继承可能不是答案，因为他刚刚得知，接下来的几个月，游戏会有大的更新，至于怎么更新还没确定。鸭子的种类会越来越多。每当有新的鸭子子类出现，他就要被迫检查并可能需要覆盖fly()和quark()方法，看对不对，防止再闹笑话。 利用接口如何？ Joe: 我可以吧fly()从超类中取出来，拉进一个flyable接口中。这么一来只有会飞的鸭子才实现此接口。同样的方式，也可以用来设计一个Quackable接口，因为不是所有的鸭子都会叫。不叫的鸭子我们就不继承这个接口呢。 Boss: 这真是一个超笨的注意，你没发现这么一来重复的代码会变多嘛？好多鸭子的Quack和fly的形式都一样，你都要在子类中重复实现它一次吗？万一48个鸭子的之类要稍微修改一下quack或者fly的方法，你难道每个鸭子都去之类改一遍嘛？ 揭晓答案吧 这里我们用到的一个设计原则： 找出应用中可能需要变化之处，把它们独立出来，不要和那些不需要变化的代码混在一起。 换句话说，如果每次新的需求以来，都会使某方面的代码发生变化，那么你就可以确定，这部分的代码需要被抽出来，和其他稳定的代码有所区分。我们把这一过程称为分离变化。 这样做以后，flyBehavior以及quackBehavior就和鸭子类分离出来了。既可以重用这部分代码，也可以新增新的behavior也不会影响到原有的代码。 这里有一个原则： 针对接口编程，不针对实现编程 即，我们不关心flybehavior的实现，只关心它有flyBehavior以及quackBehavior。具体的实现用到了再说这样。]]></content>
      <categories>
        <category>tech</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式笔记-工厂模式(忙碌的果园)]]></title>
    <url>%2F2016%2F07%2F04%2FTech%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Java中的工厂设计模式是核心设计模式之一，工厂模式主要是为创建对象提供接口，以便将创建对象的具体过程屏蔽隔离起来，达到提高灵活性的目的。 工厂模式在业界主要有三种形式，分别是： 简单工厂（Simple Factory），又称静态工厂模式（static factory method） 工厂方法（Factory Method） 抽象工厂（Abstract Factory） 这三个模式都是创建型 这个这三个模式名字看着挺像，概念上容易让人比较混淆。本文就介绍一下这三种模式到底有什么区别。 简单工厂 场景 我们在新建一个对象的时候最常见的是去new一个对象，但有时候： 我们并不想知道这个这个对象是怎么构造； 我们只想使用这个对象的父类引用； 举个栗子，我们在果园（FruitFacotry）里，果园里有各种各样的水果。我们想吃水果，但我并不想知道这些水果是怎么长出来的。也就是说做为一个果园的客户以及饥渴的吃货，我不关心你的水果是怎么长得，我上来就要吃你一个苹果一个橘子。 这个场景的代码是这样的： 12345678public class ClientOfFruitFacotry &#123; public static void main(String[] args)&#123; Fruit apple = FruitFacory.create("APPLE"); apple.beEat(); Fruit orange = FruitFacory.create("ORANGE"); orange.beEat(); &#125; &#125; 实现 那他是怎么实现的呢？非常简单我们可以这样定义FruitFactory： 12345678public class FruitFacotry &#123; public static Fruit create (String fruitName) throws Exception &#123; if (fruitName.equalsIgnoreCase("APPLE")) return new Apple(); else if (fruitName.equalsIgnoreCase("ORANGE")) return new Orange(); else throw new Exception(); &#125; 物品间的关系也很简单： 123456789101112131415//抽象产品public interface Fruit&#123; public void beEat();&#125;//具体产品public class Apple implements Fruit&#123; public void beEat()&#123; System.out.println("eat apple"); &#125;&#125;public class Orange implements Fruit&#123; public void beEat()&#123; System.out.println("eat orange"); &#125;&#125; 小结 有人会说你的这个类中会有大量的if-else语句。我们可以通过反射，hashmap，枚举等方法对这进行优化。 如果你的代码中有许多对象有着相同的父类，同时你在使用这些对象时常常用到父类的方法，那么你需要使用这种方法。这样做的好处显而易见，果园负责生成果子，客户负责消费果子，客户不需要知道果子是怎么长的，这样就实现了职责的分离。 但是也会有坏处。假设你想新加一种水果，那么你必须修改FruitFacotry这个类，这样显然违反了开闭原则。同时这个类中集合了所有的水果的创造逻辑。可想而知对于新水果的加入，果园是很被动的，如果果园中种植的水果种类会越来越多。同时简单工程的静态方法也使得它不能够被子类继承，所以所有担子都得自己挑了。这可吧果园累坏了，也累坏了我们这些吃水果的人。 这个时候工厂模式就出现了，人们想了个办法，不把所有的水果都放到一个果园中。把它按种类分出来，苹果园种植苹果，橘子园种植橘子。让具体的种植方式分配的各自的果园中。如果将来想种梨，我们就直接在拓展出一个梨园，完全不会改变苹果园，橘子园的任何东西。 工厂方法 工厂方法有称虚构造器（Virtual Constructor，在工厂方法模式中，工厂父类负责定义创建产品对象的公共接口，而工厂子类则负责生成具体的产品对象，这样做的目的是将产品类的实例化操作延迟到工厂子类中完成，即通过工厂子类来确定究竟应该实例化哪一个具体产品类。 意图 定义一个创建对象的工厂父类公共接口，让工厂子类来决定具体怎样构造对象。 工厂方法模式去掉了简单工厂模式中工厂方法的静态属性，使得它可以被子类继承。这样在简单工厂模式里集中在工厂方法上的压力可以由工厂方法模式里不同的工厂子类来分担。 正如之前的类比，不把所有的水果都放到一个果园中。把它按种类分出来，苹果园种植苹果，橘子园种植橘子。让具体的种植方式分配的各自的果园中。如果将来想种梨，我们就直接在拓展出一个梨园，完全不会改变苹果园，橘子园的任何东西。各种水果的果园负责生产这类水果。代码如下： 果园的工厂类 12345678910111213public interface FruitFacotory &#123; Fruit create();&#125;public class AppleFacotry implements FruitFactory&#123; public Apple create()&#123; return new Apple(); &#125;&#125; public class OrangeFacotry implements FruitFactory&#123; public Orange create()&#123; return new Orange(); &#125;&#125; 优点 如之前所说，使用工厂方法模式在加入新产品时，无须修改抽象工厂和抽象产品提供的接口，无须修改客户端，也无须修改其他的具体工厂和具体产品，而只要添加一个具体工厂和具体产品就可以了。这样，系统的可扩展性也就变得非常好，完全符合开闭原则（对拓展开放，对修改关闭）。 在工厂方法模式中，工厂方法用来创建客户所需要的产品，同时还向客户隐藏了哪种具体产品类将被实例化这一细节，用户只需要关心所需产品对应的工厂，无须关心创建细节，甚至无须知道具体产品类的类名。--只需要知道工厂类的类名就行 基于工厂角色和产品角色的多态性设计是工厂方法模式的关键。它能够使工厂可以自主确定创建何种产品对象，而如何创建这个对象的细节则完全封装在具体工厂内部。工厂方法模式之所以又被称为多态工厂模式，是因为所有的具体工厂类都具有同一抽象父类。 缺点 在添加新产品时，需要编写新的具体产品类，而且还要提供与之对应的具体工厂类，系统中类的个数将成对增加，在一定程度上增加了系统的复杂度，有更多的类需要编译和运行，会给系统带来一些额外的开销。 由于考虑到系统的可扩展性，需要引入抽象层，在客户端代码中均使用抽象层进行定义，增加了系统的抽象性和理解难度，且在实现时可能需要用到DOM、反射等技术，增加了系统的实现难度。 拓展 使用多个工厂方法：在抽象工厂角色中可以定义多个工厂方法，从而使具体工厂角色实现这些不同的工厂方法，这些方法可以包含不同的业务逻辑，以满足对不同的产品对象的需求。比如： 12345678public class AppleFacotry implements FruitFactory&#123; public Apple createHongFuShi()&#123; return new Apple("红富士"); &#125; public Apple createGuoGuang()&#123; return new Apple("国光"); &#125;&#125; 产品对象的重复使用：工厂对象将已经创建过的产品保存到一个集合（如数组、List等）中，然后根据客户对产品的请求，对集合进行查询。如果有满足要求的产品对象，就直接将该产品返回客户端；如果集合中没有这样的产品对象，那么就创建一个新的满足要求的产品对象，然后将这个对象在增加到集合中，再返回给客户端。 多态性的丧失和模式的退化：如果工厂仅仅返回一个具体产品对象，便违背了工厂方法的用意，发生退化，此时就不再是工厂方法模式了。一般来说，工厂对象应当有一个抽象的父类型，如果工厂等级结构中只有一个具体工厂类的话，抽象工厂就可以省略，也将发生了退化。当只有一个具体工厂，在具体工厂中可以创建所有的产品对象，并且工厂方法设计为静态方法时，工厂方法模式就退化成简单工厂模式。 抽象工厂我们将在下一篇文章中介绍。 延伸阅读 简单工厂模式 工厂方法模式 JAVA设计模式之工厂模式(简单工厂模式+工厂方法模式) *Factory Method Pattern Design Patterns: Factory vs Factory method vs Abstract Factory]]></content>
      <categories>
        <category>tech</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式笔记-单例模式]]></title>
    <url>%2F2016%2F06%2F29%2FTech%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[好久没写博文，最近学习一些设计模式，顺便记录一下。 单实例Singleton设计模式可能是被讨论和使用的最广泛的一个设计模式了，这可能也是面试中问得最多的一个设计模式了。我们尝试从场景出发，来看看要怎么设计这个类。 场景 我们要得到一个类，整个系统中只能出现一个类的实例。这样的场景非常多，比如说一个国家，只有能有一个现任总统。仔细想想，要满足这一条件，我们觉得应该满足几个条件。 和大部分类不同，它的构造函数需要是私有的。否则，在任何地方大家都能够new这个实例，那么系统中就不能始终保持只存在一个实例的情况了。 既然没有公有构造函数，那么我如何实例化这个类呢？我们需要一个静态的方式让其形成实例，给个方法吧--getInstance()，这个方法要判断现在系统中有没有这个实例，如果有，则返回；如果没有则调用私有的构造方法来实例化这个类，并存好，等下次来getInstance()调用时返回。 所以我们还需要一个私有的变量来存储这个实例。 我们使用时就可以用**Singleton.getInstance()**得到它了。 根据这些条件我觉得我们可以得到朴素的教科书版本的代码： 基础版本（懒汉式，线程不安全） 1234567891011public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 看上去很美好，解决了我们上诉的要求，满足懒加载（只有用到的时候才会去创建这个实例）。然而该方法有一个致命的弱点，当系统中几个线程同时调用这个方法时，就很有可能会实例化出多个实例来，也就是说线程不安全。为了解决这个问题最简单的方法就是加Synchronize关键字。代码如下： 懒汉式，线程安全 1234567891011public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 好了，线程安全了，然而我们发现在每次调用getInstance()的方法时，我们都会上锁。但其实我们只需要在创建的时候上锁，而不创建的时候我们其实不需要上锁。如果在多线程的系统中有频繁的调用，那么这段代码的性能会比较低。那我们只在创建的时候加锁行不行？像这样： 懒汉式，线程不安全 12345678910111213public class Singleton&#123; private static Singleton singleton; private Singleton()&#123;&#125; public static Singleton getInstance() &#123; if (singleton== null) &#123; synchronized (Singleton.class) &#123; singleton= new Singleton(); &#125; &#125; return singleton; &#125;&#125; 看起来不错哦。应该没有问题了吧？！错！这还是有问题！为什么呢？前面已经说过，如果有多个线程同时通过(singleton== null)的条件检查（因为他们并行运行），虽然我们的synchronized方法会帮助我们同步所有的线程，让我们并行线程变成串行的一个一个去new，那不还是一样的吗？同样会出现很多实例。线程依然不安全，没有解决第一种方法的问题！！！！好了我知道了，这样，双重校验： 双重检验锁(double checked locking) 1234567891011121314public class Singleton &#123; private volatile static Singleton instance; //声明成 volatile private Singleton ()&#123;&#125; public static Singleton getSingleton() &#123; if (instance == null) &#123; //Single Checked synchronized (Singleton.class) &#123; if (instance == null) &#123; //Double Checked instance = new Singleton(); &#125; &#125; &#125; return instance ; &#125;&#125; 但是这种方法要对volatile关键字有想到深刻的理解，并且对Java的内存模型深度理解。同时这种方法在jdk1.5之前是不能有bug的。如果你在面试中使用了这种方法，但是又不能很好的解释这方法的话。面试官不会喜欢你。-,- 相信你不会喜欢这种复杂又隐含问题的方式，如果你仍有兴趣，请查看这里当然我们有更好的实现线程安全的单例模式的办法。 饿汉式 static final field法 这种方法非常简单，因为单例的实例被声明成 static 和 final 变量了，在第一次加载类到内存中时就会初始化，所以创建实例本身是线程安全的。 12345678910public class Singleton&#123; //类加载时就初始化 private static final Singleton instance = new Singleton(); private Singleton()&#123;&#125; public static Singleton getInstance()&#123; return instance; &#125;&#125; 也就是说这种方法巧妙的避开了新建的过程，所以也不存在多线程调用时会产生的问题。大部分的情况下，这种方法能满足要求。但是吹毛求疵一下，这种方法在没用到它的时候它就已经实例化好了，它不是懒加载的形式。 静态内部类 static nested class 123456789public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125; 老版《Effective Java》中推荐的方法，上面这种方式，仍然使用JVM本身机制保证了线程安全问题；由于 SingletonHolder 是私有的，除了 getInstance() 之外没有办法访问它，因此它只有在getInstance()被调用时才会真正创建；同时读取实例的时候不会进行同步，没有性能缺陷；也不依赖 JDK 版本。 最优雅版本--枚举 Enum 用枚举写单例实在太简单了！这也是它最大的优点。下面这段代码就是声明枚举实例的通常做法。 123public enum EasySingleton&#123; INSTANCE;&#125; 居然用枚举！！看上去好牛逼，通过EasySingleton.INSTANCE来访问，这比调用getInstance()方法简单多了。 默认枚举实例的创建是线程安全的，所以不需要担心线程安全的问题。但是在枚举中的其他任何方法的线程安全由程序员自己负责。还有防止上面的通过反射机制调用私用构造器。 这个版本基本上消除了绝大多数的问题。代码也非常简单，实在无法不用。这也是新版的《Effective Java》中推荐的模式。 总结 小小的一个场景演化出了这么多方法。在一般的情况下饿汉式的方法用的比较多，在有懒加载要求时，静态内部类方法不错。 延伸阅读 如何正确地写出单例模式 深入浅出单实例Singleton设计模式 10-interview-questions-on-singleton Double Checked Locking on Singleton Class in Java Why Enum Singleton are better in Java]]></content>
      <categories>
        <category>tech</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2015年终总结]]></title>
    <url>%2F2016%2F02%2F15%2Flife%2F2015%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93-2016-02-15%2F</url>
    <content type="text"><![CDATA[懒癌大概已经进入晚期了吧，翻了翻15年的博客，大概才写了5~6篇博文，之前一直想要写的年终总结也拖到过完春节回到了上海才开始写。尽管如此，2015年相对于我来说确实是非常重要的一年。之前在朋友圈看到过这样一段话： 当你老了，回顾一生，就会发觉：什么时候出国读书，什么时候决定做第一份职业，何时选定了对象而恋爱，什么时候结婚，其实都是命运的巨变。只是当时站在三岔路口，眼见风云千樯，你做出选择的那一日，在日记上，相当沉闷和平凡，当时还以为是生命中普通的一天 -- 陶杰 《杀鹌鹑的少女》 是的，2015年我从学校毕业，由一名学生转换成了一名码农。结束了长达8年的帝都求学生活，回到了离家乡不远的上海，开始了从事自己比较喜欢的软件开发工作。 求职历程 15年的上半年，大部分时间处在准备毕业的状态。因为有了几个相对满意的offer，年后的春招基本也就随意投了几个想去的单位，依然无缘bat等第一梯队的互联网公司，不过ctrip的实习经历和offer确实给了我不少底气，简历也更加容易通过了。后来机缘巧合拿到了招商银行信用卡中心的offer，一方面当时看评论业界对招行的技术评价确实不错，另一方面招行给出的offer也确实比较给力。最后选择了来到这边工作，原本以为能够参与卡中心拳头产品掌上生活APP的移动端开发。没想到后来被分配到数据室，专门负责数据营销的团队。在学校期间还经历了令人头疼的毁三方的流程，以及复杂的上海落户材料整理流程。好在后来成功和杭州的海康毁约（3000大洋违约金），也成功的评上市优，为上海落户加了5分。到现在也以及成功的拿到了上海户口，户口这个东西，就是那种有了感觉没什么，用到的时候才发现不能缺，不过这么已落户估计自己这些年也就被栓在上海了，而在此之前我毕业想去的城市一直是杭州。 毕业旅程 可爱的母校的毕业流程真是一点不可爱，以至于经过这么半年，我对它（毕业流程）心里还有怨言，也终于明白原本2年半足够毕业的为啥要硬生生拖到三年。基本上3月初回到学校后，就开始着力写毕业论文，准备毕业答辩了，没想到这个时间足足持续到6月份，整整三个月的战线谁都受不了，期间还有导师的攻心计，分分钟恐吓你今年你可能毕不了业哦（毕竟当时投的两篇sci都还没有中，后来毕业后才中了一篇，还有就是带坏了实验室的学术氛围，师弟们纷纷要去找实习，转互联网）。期间那种想突破牢笼的欲望和写论文整理实验，收集材料数据，调论文格式等等等等的焦虑无奈繁琐烦躁简直要把人逼疯。好在一切都过去了。但是遗憾的是没有去毕业旅行，单身狗是一方面，另一方面是之前攒的钱买了一台macbook，基本上没有什么经费去旅行了。没能和好基友们出去走走确实遗憾。不过离京前，该聚的老朋友都聚了聚，帝都八年还是非常喜欢帝都的朋友们的，也交到了不少知心朋友，希望以后大家有缘再见吧。 新生活新工作 回到上海，除了冬天还是觉得有一些冷外，其他的生活是不能再适应更多了。刚来开始上班那一阵，下班背着包走在路上，不由的想起了小时候放学回家的场景，路边的小吃车，人流不多，车流不多的路，这一切我都很熟悉。之前一直在想2015年如果非要加一个主题的话，我会选“回归”吧，从北方回归到南方，回归到家庭，回归到生活。上海郊区的生活还是比较安静简单的，新同事朋友也都很奈斯，自己的厨艺也开始得到朋友们的认可，对做菜产生了比较浓厚的兴趣。就是依旧单身，依旧热泪盈眶。。。 工作上，同事们都很nice，虽然没有能从事移动端开发，而是偏向于java web的数据营销平台开发，但是我还是学到了很多东西的。熟悉了java web 开发的一些框架，spring structs hibernate，还有前端的js，jquery，bootstrap 等等，这些对我来说都是新的。身在数据室sql知识也得到了恶补，以后应该也会偏向于接触大数据处理的一些相关技术栈吧。期间还对老的营销平台代码做了一些重构，看了些cleancode的东西。后来索性新小组开始写新的平台替换原有的平台。新小组团队很不错，组长是同龄的复旦大牛，科室重点培养的对象，刚开始可能还有些不同意见，后来还是从他身上学到不少东西，也觉得自己必须保持谦虚的太多，毕竟不了解的东西太多，千万不可井底之蛙。组长休假期间还担任了2个迭代的scrum master，深感带队和闷头干确实有不少差别，自己还有许多地方可能还是需要提高。 期望与计划 新的一年，期望的事情很多，首先是期望家里人身体健康平平安安，今年回家明显感觉到父母的身体都不同程度的出现问题。希望明年父母能够健康吧，也一直提醒父母这个年纪要注意保养自己的身体，养成良好的生活习惯。另外就是自己的体重已经超出了巅峰时期，体重控制以及必须提严格执行了（虽然一直在议程），希望新的一年能够多去健身房，把体重减下来，保持在140的样子。也借此希望能够在16年结束单身生活，遇到一个不错的伴侣。也期望买房能够提上议程。 工作上希望能够撸实java基础，对一些框架做更深入一点的理解，后端技术的熟悉，比如redis、MQ、数据库调优等等。另外像大数据的一些技术栈去靠近，比如kafka，spark ，storm等等。多了解一些敏捷的东西，玩玩node.js，ruby之类能够业余撸一点自己的小项目。 愿2016一切都好！]]></content>
      <categories>
        <category>life</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Clean Code 笔记（3）Function Structure]]></title>
    <url>%2F2015%2F12%2F27%2FTech%2FClean%20Code%20%E7%AC%94%E8%AE%B0%EF%BC%883%EF%BC%89Function%20Structure%2F</url>
    <content type="text"><![CDATA[Introduction Do you know why swith statement casuse such harm to software structure,did you know if statement have same problem.In this episode we are going to talk about how to get rid of most of them, and how to deal with the rest of them.Did you know assagnment staement also harmful.Have you ever seen a function full of error checks ,null checks that you couldn't tell what it was doing.Or how about try catch. Fusion sun metaphor. Arguments How many arguments should a function have.They are hard to read,hard to understand.we should treat them as a liabilty not a asset Three arguments Max. as fewer as prossiable. one is ok, two is ok, three is usaully too many. remenber order.why aren't they a object No boolean arguments write two function, one for ture , other for false.passing two boolean is even worse,do four things.]]></content>
      <categories>
        <category>tech</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Clean Code 笔记 (2) Function]]></title>
    <url>%2F2015%2F12%2F27%2FTech%2FClean%20Code%20%E7%AC%94%E8%AE%B0%20(2)%20Function%2F</url>
    <content type="text"><![CDATA[Introduction function is what we do.fisrt container put codes in. how long ?20,30lines? there is a simple rule to follow. how to find classes by getting your function at right size. do one thing , do it well, do it only. but what does onething mean? what makes sun shine the first rule of funtions They should be smaller.The second rule of functions that should small than that. How big should a funtion be? Back in the 1990s, we have a simple rule, a function should be a screen full.(about 20 lines.) original the screen full is out of date,doesn't apply anymore. &quot;Four lines is ok, maybe five, six? ok,ten is way too big.&quot; sound insame. In a four line function ,how many indent how many if else statement,try catch blocks your can see.there is not enough room for indenting. small functions have nice long descriptive names, what's more, you will extrat the pridicts of your if statements and while loop into even more nice small named boolean functions. example: before: 123456789public String invoke() throws Exception&#123; if(pageData.hasAttribute("Test"))&#123; content+= includeSetups(); content+=page.getContent()+"\n"; content+=includeTeardowns(); pageData.setContent(content.toString()); &#125; return page.getHtml();&#125; after 123456789101112public String invoke() throws Exception&#123; if(isTestPage())&#123; content+= includeSetups(); content+=page.getContent()+"\n"; content+=includeTeardowns(); pageData.setContent(content.toString()); &#125; return page.getHtml(); &#125; private boolean isTestPage() throw Exception&#123; return pageData.hasAttribute("Test"); &#125; Are you out of mind you are thinking,you will lost in a sea of funtions where do classes go to hide a long function is where the classes go to hide long funtion could always refactor to a class one thing The old rule that &quot;the function should do one thing, do it well and do it only.&quot; But what does onething mean? it could be any thing. at same level extract untill your drop extract till you can not extract conculsion the fisrt rule of funtion is small; making functions smaller will save your and everybody time. the class hide in long functions. funtions do one thing. extract till you drop.]]></content>
      <categories>
        <category>tech</category>
      </categories>
      <tags>
        <tag>cleancode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Clean Code 笔记（1）Name]]></title>
    <url>%2F2015%2F12%2F21%2FTech%2FClean%20Code%20%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89Name%2F</url>
    <content type="text"><![CDATA[Reveal Your Intent use intent-revealing name, if your need a comment ,that's not a good name. bad: 1int d; // elapsed time in days good 12345678910111213 int elapsedTimeInDays;``` # Descpribe Your Problem* bad:```java/***Useful range constant*/public static final int INLCUDE_NONE=0;/***Useful range constant*/public static final int INLCUDE_NONE=1;/***Useful range constant*/public static final int INLCUDE_NONE=2;/***Useful range constant*/public static final int INLCUDE_NONE=3; can you tell me what they mean,did the comment helpful? 'userful?' are their useless code?'range?'range of what?'Constant' ofco use its a constant whenever you have to read the code in order to understand the name, the name has pretty much failed to communicate the intent.It's a bad name. Remember names are not for your convienient, it's your primariy tools for communicate intent.Commuicate your intent is always your first priority,it's even more important than the code work. Avoid Disinformation disinformation: name in code does not mean what it's said.(worsest sins) a misleading name can cous Pronounceable Names bad 123456789int PC_GWDA;public int getYYYY()&#123; return this.year; &#125; int qty_tests =0; int qty_pass_m=0; int qty_pass_s =0; int qty_skip = 0; int qty_fails=0; Avoid Encoding prifix: p mean point,c mean char, b mean boolean.C for Class,I for Interface. C: Class CAccount I：Interface IAccount p: pointer pAccount s: String sName st: Null Terminated StName now days,IDE can tell all this informations,i won't be using them.That' 1990s legency,silly prifixs.Just use names! Let the IDE,compiller and test do the rest. 'Account is better than IAccount' Part of Speech class and variable are norns or norn pharse,methods are verbs. ignore silly nosie words. good ACOUUNT,MESSAGEPARSER silly nosie words MANAGER,DATA,INFO,PROSSOR boolean variabels should been write like predicats,they read well,like isEmpty,isTerminated methods should been verbs or verbs pharese, they read well,like getPirce,postPayment if a methods returns a boolean,then should be predicats,beacuse they likely be used in if statement and they read well,likeisPostable 12345678boolean isEmpty;boolean isTerminated;---if(isEmpty)&#123; if (payment.isPostable())&#123; postPayment(payment); &#125;&#125; Enums tends to be states or object descriptors,so they often adj. 123enum Color&#123;RED,GREEN,BLUE&#125;;enum Status&#123;PENDING,CLOSED,CANCELLED&#125;;enum Size&#123;SMALL,MEDIUM,LARGE&#125;; remmeber: it's a lot easier to read code if the statements form sentences that read like well written poems. 1if(employee.isLate() employee.reprimand(); The Scope of Length Rule for varibales: the longger the scope of variable,the longger veriable names,but the variable with short scope should have short names; 1234for (ITestResult tr: m_configIssues)&#123;Elemtent element = createElement(d,tr);rootElement.appendChild(element);&#125; here tr is a good name,but d is bad.element should be shorten to e. for funtions(oppsite): the longer the scope,the shorter the function or name should be; the shortter the scope,the longer the function should be. 12345678910public void server(Socket s)&#123; try&#123; tryProcessInstructions(s); &#125;catch(Throwable e)&#123; slimFactory.stop(); close(); closeEnclosingServiceInSeperateThread(); &#125; &#125; &#125; we like long socpe public function names short like serve,convinient to use,long name hard to tell apart;we like short scope private function names long like tryProcessInstructions,closeEnclosingServiceInSeperateThread,only called from here ,it's kind of a comment, explain themselves. The same arguement can be applied to Classes.]]></content>
      <categories>
        <category>tech</category>
      </categories>
      <tags>
        <tag>cleancode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jira Basic User Guide]]></title>
    <url>%2F2015%2F10%2F10%2FTools%2FJira%20Basic%20User%20Guide%2F</url>
    <content type="text"><![CDATA[Jira Basic User guide 创建时间:2015-09-28 修订时间:2015-09-28 编写：shengjunhui What is jira ? JIRA是Atlassian公司出品的** Issue &amp; Project Tracking(项目与事务跟踪)工具**，被广泛应用于缺陷跟踪、客户服务、需求收集、流程审批、任务跟踪、项目跟踪和敏捷管理等工作领域。 JIRA中配置灵活、功能全面、部署简单、扩展丰富，其超过150项特性得到了全球115个国家超过19,000家客户的认可。 对于Jira中的一些概念，我整理了一个对应的脑图**Jira简介**，以便于我们能够简洁明了理解这些词在Jira中的含义。 What is project? 项目的基本单位,有名字，标识(key)，模块，版本等组成。每一个项目都会有一个key（例如，“BD”）,所有的该项目下的issue都会被分配一个issue key(例如&quot;BD-1&quot;)。 What is issue? Issue(问题)为一切可以追踪的议题，包括史诗，用户故事，任务，子任务，缺陷等等。一般的项目中的惯例是一个项目有多个史诗级故事Epics。对于每一个Epic，又可以由多个用户故事组成。对于每一个用户故事我们可以将其分解成几个任务来进行。当任务还是比较难以执行时，我们还可以将任务分解为多个可以执行的子任务。 任务和用户故事的区别： 任务一般是开发人员拿过来直接可以执行的单元。 用户故事是用户对于需求场景的简短的描述。 任务是对用户故事的分解，是一些为了完成用户故事需要做的一些工作。 issue的属性 在创建一个issue时，我们需要填写它的一些属性。目前一个issue所含的属性有： [img5]:img/createIssue.bmp ![Original page][img5] 我们可以选择填写所有的属性，也可以自定义选择要填的属性。其中值得注意的是： Issue 详细 对于每一个issue，我们都可以通过点击它的issue key（例如“BD-1”）或者右键选择“在Jira中浏览”，查看它的详细信息 What is workflow? Scrum 面板 Scrum面板是JIRA目前主推的敏捷插件中的重要功能，是团队中最常用的一个面板。用来记录开发活动中的issue。 面板的右上角为三个不同的菜单：Plan， Work， Report 分别用来记录项目在不同阶段的内容。 Plan 在项目计划阶段，项目需求管理人员将需求转换成对应的大的Epic Stroy 录入到Product backlog中，并将这些Epic Stroy 分解成小故事以及任务。 该面板中，可以通过设置不同的filter来搜索得到不同的结果。比如“Only my Issues”,“Recently update”，还可以根据版本，epic，来筛选查找对应的issue。 Work 当我们在Plan面板录完了对于的用户故事后，可以将这个迭代需要做的issue拖入到Sprint框中，点击Strat Sprint开始这个sprint。 该面板展示了当前sprint的所有issue，并在不同的状态的栏目中能够看这些issue。 Report Report 面板是对项目的sprint中的各种指标进行监控，包括Burndown Chart(燃尽图),Sprint Report(Sprint报告，在项目的回顾中分析使用)，Epic Report(针对每一个Epic的进度分析图)，Version Report, Control Chart,Velocity Chart,Cumulative Flow Diagram 等等 [img9]:img/workreport.bmp ![Original page][img9] 项目的概览 另外一个项目中常用的页面时项目概览页面，我们通过点击**项目**菜单下对应的项目，便可以查看项目的概览。包括项目的简介 如果你的角色是管理员，还可以管理对应栏目下的内容。 一些插件 SVN插件 在提交代码时，在commit message中输入对应的issue key，则在对应的issue下，能够看到相应的svn记录。 例如：commit message: BD-1: 添加了xxxx功能 [img11]:img/svn.bmp ![Original page][img11] Draw.io 插件 在对某一个任务做设计时，可以方便的使用draw.io插件对它做流程图的设计 Jira的官方用户手册6.3.6很详细的描述了更多的关于Jira的使用说明，请参考！(PS. 如果你们是敏捷的小团队，我会更推荐teambition,tower,trello等更加轻的小工具。)]]></content>
      <categories>
        <category>tools</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[程序员的自我修养]]></title>
    <url>%2F2015%2F08%2F12%2Flife%2F%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%2F</url>
    <content type="text"><![CDATA[最近看了几句话非常有感觉，觉得它是属于一个合格专业的程序员应该具备的修养，写出来分享给大家，同时主要是勉励自己。 &lt;!-- more --&gt; 第一句 Amateurs practice until they get it right, professionals practice until they can’t get it wrong. 来自O神的qq签名，大意是说业余选手会不断的练习直到他们作对了，专业的选手也会不断练习知道他们不会犯错。这段话大概是o神入职阿里2~3个月后修改的，想必是他工作几个月后的感悟。虽然他现在已经不再阿里，但是这条qq签名一直留在那里，想必这还是他以后工作人生中的一个感悟吧。虽然作为一个程序员不犯错，不出bug基本是一个不可能的问题，但这句话要传达的意思想要成为一个专业的程序员，离不开的是不断的学习，不断训练，不断的严格要求自己。 第二句 才华撑不起野心的时候，你要静下心来学习 来自wuchong同学博客的签名，一度我还把它设成了微信的签名。这句话就更直白了，告诫我们这些明明可以靠脸吃饭的同学，但是又对才华有不懈追求的同学，要充满野心并且不断学习。IT行业是一个高速奔跑的行业，想要成为professional，**不断学习，做一个lifelong learner**是非常有必要的。 第三句 一天天消失 // Stay Focused, Keep Shipping. Build Early, Build Always. Improve yourself, Write Solid/Simple/Stupid code. 这是阮一峰老师的微博签名（擦，是有多闲，一直在看别人签名，也是醉了 - -！）。看吧，阮大神也是这么说的，而且阮大神境界更高，毕竟是独立思考，有思想的（参见阮老师大作《如何变的有思想》）经历也丰富的大神。Improve yourself，Write Solid/Simple/Stupid code是督促我们要不断学习提高自己，写出3个S的代码。三个S开头的单词说起来简简单单，但不经过长年累月的积累和学习应该是很难做到的。同时阮老师用一个一天天消失,简单有力的说出了时间流逝之快，看明天又是周四了，一周又要立马的过去了。Stay Focused, Keep Shipping告诫我们要保持专注，保持源源不断的向用户输送自己的代码，自己的产品。这句话也是FB的zucker对自己的告诫。 至于Bulid Early, Build Always我也不大理解，大概是说要尽早的把产品传递给用户，这个变化快速的市场，精雕细琢产品错过最佳的推向市场的时机是非常可悲的。一直保持行动，一直保持将产品输送给用户。 总结 最后，说了这么多，这篇文章主要是告诫我这个非科班出身的，刚毕业进入职场的码农，要严格要求自己，不断学习进步，学做lifelong learner，早日成为一个professional的码农。 Happy Coding,Happy Hacking. Eric Sheng]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>闲言碎语</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ctrip Day 1]]></title>
    <url>%2F2015%2F01%2F12%2Flife%2FCtripDay1-2015-1-12%2F</url>
    <content type="text"><![CDATA[Balabala 好久没更新日志了，现在人已在魔都，刚刚开始第一天的实习，刚刚下班，闲得无聊，坐下来写写我的近况。之前辗转的求职路都不想提，因为受不了帝都的空气和压力，帝都的许多互联网我也没有多去尝试。一心想回杭州阿里的我，在阿里的内推电面以及校招笔试挂掉之后，就无缘和阿里的面试官当面聊一聊了。直到10月份才拿到2个自己比较满意的offer，一个在杭州离家超近，一个在上海离家也不远；一个是传统IT行业正起步转型互联网，一个是国内最大的在线旅游网站，携程。懒惰的我，自此求职战斗力就直线下降。沉迷于offer比较，研究两家公司的背景，财报，股票，口碑等等，甚至其行业内的竞争对手的材料，简直调研了在线旅游和安防两个行业（-_-）。这时处女座的选择困难症上身，两边还是分不出胜负，最后决定先来携程实习一下，感受一下大互联网的氛围。 互联网深度用户的魔都新生活 由于携程一直有入职前先实习的惯例，正好导师这边活也都忙完了，便约定了来上海实习。我一直是一个非常会做计划，喜欢提前把事安排的妥妥当当的人。 行 于是我便早早的在12306上订好了票，非常熟练的抢到票，并且是一辆经过上海但是终点站杭州的车，刚好符合我的学生票优惠区间。（后来发现携程的APP顶高铁票十分方便） 住 最令人头疼的是租房问题，只能实习一个月，所以是一个月的短租，非常不方便，而且最好是希望那边直接能提供被子，枕头等等一些大件的生活用品，省的自己买，用完丢了也不合适，还得搬来搬去。最先想到的解决方案当然是上58，赶集，安居客，搜房等等一些信息网上去找，找了一阵发现这边已经被中介占领的死死的。后来来到了豆瓣小组，不可否认，豆瓣现在是为数不多的个人房源阵地了，不过对与我们这种短租一月的还是困难。再后来想到了鼎鼎大名的Airbnb，这个靠谱，门槛高，还看不到中介的身影，好多房子都是英文介绍。不过门槛高了价格也高，一些装修不错的房子都超过了200yuan/night。便宜的也有，不过大多数是沙发或者气垫床。最后总算发现了一个做短租的国内网站，小猪短租，价格合适，与房东商量后，以一个略高于市场价的价格租到了一个不错的次卧。 吃 刚来第一天，叫了份饿了么，不过尽管魔都是饿了么的老巢，但是覆盖情况还是一般 在爱博家园这边还是只有零星几家有送外卖服务，满减活动基本上么有。不过楼下有个小购物广场，上面有不少吃的，打开大众点评搜罗了附近的美食与娱乐场所。找了家老鸭粉丝汤的店试了试。 陆续体验其他中。。]]></content>
      <categories>
        <category>life</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习实战之k近邻算法]]></title>
    <url>%2F2014%2F11%2F03%2FData%20structure%20and%20Algorithms%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E4%B9%8Bk%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[{}]]></content>
      <categories>
        <category>Data Struture and algorithm</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu下 sublime无法输入中文解决办法]]></title>
    <url>%2F2014%2F11%2F03%2FTools%2Fubuntu%E4%B8%8B%20sublime%E6%97%A0%E6%B3%95%E8%BE%93%E5%85%A5%E4%B8%AD%E6%96%87%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"><![CDATA[sublime是一款非常令人爱不释手的编译器，功能强大，界面美观，跨平台。但是最近在ubuntu下发现他没法输入中文，本来写代码可能需要的中文输入情况比较少，但是奈何有时候写写markdown，注释之类的可能还是需要用到中文，所以今天索性找方法解决他。 这种问题力求快速解决,不废话. ##安装搜狗输入法 搜狗输入法最近适配了linux系统,简直是linux粉们的福音,安装可以参见Sougoupinyin官网,安装完成后,需要进行重启并且配置输入法(按win键,然后输入keyboard),选择Fcitx. ##安装补丁 装完搜狗貌似还是不能进行中文输入,网上找到一个解决方案: 新建文件sub-fcitx.c,建议放到Subime Text所在的目录下,将下面的代码复制: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/*sublime-imfix.cUse LD_PRELOAD to interpose some function to fix sublime input method support for linux.By Cjacker Huang gcc -shared -o libsublime-imfix.so sublime-imfix.c `pkg-config --libs --cflags gtk+-2.0` -fPICLD_PRELOAD=./libsublime-imfix.so subl*/#include &lt;gtk/gtk.h&gt;#include &lt;gdk/gdkx.h&gt;typedef GdkSegment GdkRegionBox; struct _GdkRegion&#123; long size; long numRects; GdkRegionBox *rects; GdkRegionBox extents;&#125;; GtkIMContext *local_context; voidgdk_region_get_clipbox (const GdkRegion *region, GdkRectangle *rectangle)&#123; g_return_if_fail (region != NULL); g_return_if_fail (rectangle != NULL); rectangle-&gt;x = region-&gt;extents.x1; rectangle-&gt;y = region-&gt;extents.y1; rectangle-&gt;width = region-&gt;extents.x2 - region-&gt;extents.x1; rectangle-&gt;height = region-&gt;extents.y2 - region-&gt;extents.y1; GdkRectangle rect; rect.x = rectangle-&gt;x; rect.y = rectangle-&gt;y; rect.width = 0; rect.height = rectangle-&gt;height; //The caret width is 2; //Maybe sometimes we will make a mistake, but for most of the time, it should be the caret. if(rectangle-&gt;width == 2 &amp;&amp; GTK_IS_IM_CONTEXT(local_context)) &#123; gtk_im_context_set_cursor_location(local_context, rectangle); &#125;&#125; //this is needed, for example, if you input something in file dialog and return back the edit area//context will lost, so here we set it again. static GdkFilterReturn event_filter (GdkXEvent *xevent, GdkEvent *event, gpointer im_context)&#123; XEvent *xev = (XEvent *)xevent; if(xev-&gt;type == KeyRelease &amp;&amp; GTK_IS_IM_CONTEXT(im_context)) &#123; GdkWindow * win = g_object_get_data(G_OBJECT(im_context),"window"); if(GDK_IS_WINDOW(win)) gtk_im_context_set_client_window(im_context, win); &#125; return GDK_FILTER_CONTINUE;&#125; void gtk_im_context_set_client_window (GtkIMContext *context, GdkWindow *window)&#123; GtkIMContextClass *klass; g_return_if_fail (GTK_IS_IM_CONTEXT (context)); klass = GTK_IM_CONTEXT_GET_CLASS (context); if (klass-&gt;set_client_window) klass-&gt;set_client_window (context, window); if(!GDK_IS_WINDOW (window)) return; g_object_set_data(G_OBJECT(context),"window",window); int width = gdk_window_get_width(window); int height = gdk_window_get_height(window); if(width != 0 &amp;&amp; height !=0) &#123; gtk_im_context_focus_in(context); local_context = context; &#125; gdk_window_add_filter (window, event_filter, context);&#125; 安装变异环境 12sudo apt-get install build-essentialsudo apt-get install libgtk2.0-dev 切换到sub-fcitx.c，所在目录，编译生成so文件. 正常的话，目录下会多一个libsublime-imfix.so文件 1gcc -shared -o libsublime-imfix.so sub-fcitx.c `pkg-config --libs --cflags gtk+-2.0` -fPIC 切换到Sublime Text目录下，通过LD_PRELOAD=./libsublime-imfix.so ./sublime_text命令启动Sublime Text之后，就会发现可以输入中文了，但是这样太麻烦了，通过添加自己的启动脚本可以简化这一过程。 ##添加启动脚本 在～/bin目录下新建文件sub3，把下面的的代码复制到文件里，其中SUB3_HOME是Sublime Text的根目录，根据自己放的目录更改。另外,Ubuntu13.04下~/bin是默认在用户PATH里的，其它系统可能要自己在~/.bash_profile或者~/.profile中添加环境变量。 12345678910#!/bin/bashSUB3_HOME=$HOME/Software/sublime_text_3CMD="LD_PRELOAD=./libsublime-imfix.so ./sublime_text"FILENAME=$1if [ -n "$1" ]then CMD=$&#123;CMD&#125;" "`pwd`/$FILENAMEficd "$SUB3_HOME"eval $CMD 修改文件权限 1chmod 777 sub3 测试,可以带参数表示文件名也可以不带参数 1234#打开Sublime Text,对文件a.sh进行编辑sub3 a.sh#打开Sublime Text,回到之前关闭时的工作状态sub3]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>sublime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[压栈思想计算Java运算表达式]]></title>
    <url>%2F2014%2F09%2F25%2FData%20structure%20and%20Algorithms%2F%E5%8E%8B%E6%A0%88%E6%80%9D%E6%83%B3%E8%AE%A1%E7%AE%97Java%E8%BF%90%E7%AE%97%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[#压栈思想计算Java运算表达式 栈的规则是先进后出。利用压栈的思想来计算四则运算表达式是这样的：我们给定两个栈，一个用来存放数字、一个用来存放对应的操作符。假定我们有一个给定的四则运算表达式a+b+c/d*(e+f)-da，那我们先把这个表达式拆分成一个个的数字或者是运算符、或者就是括号了。然后我们从左至右遍历每一个元素，遍历过程中遵循步骤和原则如下： （1）遇到数字则直接压到数字栈顶。 （2）遇到运算符（+-/）时，若操作符栈为空，则直接放到操作符栈顶，否则，见（3）。 （3）若操作符栈顶元素的优先级比当前运算符的优先级小，则直接压入栈顶，否则执行步骤（4）。 （4）弹出数字栈顶的两个数字并弹出操作符栈顶的运算符进行运算，把运算结果压入数字栈顶，重复（2）和（3）直到当前运算符被压入操作符栈顶。 （5）遇到左括号“（”时则直接压入操作符栈顶。 （6）遇到右括号“）”时则依次弹出操作符栈顶的运算符运算数字栈的最顶上两个数字，直到弹出的操作符为左括号。 下面的例子中分别使用java.util.Vector和java.util.Stack基于上述原则实现了这一运算过程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254import java.util.HashMap;import java.util.Map;import java.util.Stack;import java.util.StringTokenizer;import java.util.Vector;import java.util.regex.Pattern;public class Test &#123; public static void main(String args[]) &#123; String computeExpr = "1 + 5 * 6 + 3 * (2 + 3*2+2-1+3*3) + 10/5 - 6*1"; Test test = new Test(); double result1 = test.computeWithVector(computeExpr); double result2 = test.computeWithStack(computeExpr); System.out.println(result1 + "=======" + result2); &#125; /** * 利用java.util.Vector计算四则运算字符串表达式的值，如果抛出异常，则说明表达式有误，这里就没有控制 * @param computeExpr 四则运算字符串表达式 * @return 计算结果 */ public double computeWithVector(String computeExpr) &#123; StringTokenizer tokenizer = new StringTokenizer(computeExpr, "+-*/()", true); Vector&lt;Double&gt; nums = new Vector&lt;Double&gt;(); Vector&lt;Operator&gt; operators = new Vector&lt;Operator&gt;(); Map&lt;String, Operator&gt; computeOper = this.getComputeOper(); Operator curOper; String currentEle; while (tokenizer.hasMoreTokens()) &#123; currentEle = tokenizer.nextToken().trim(); if (!"".equals(currentEle)) &#123;//只处理非空字符 if (this.isNum(currentEle)) &#123; // 数字 nums.add(Double.valueOf(currentEle)); &#125; else &#123; // 非数字，即括号或者操作符 curOper = computeOper.get(currentEle); if (curOper != null) &#123; // 是运算符 // 运算列表不为空且之前的运算符优先级较高则先计算之前的优先级 while (!operators.isEmpty() &amp;&amp; operators.lastElement().priority() &gt;= curOper .priority()) &#123; compute(nums, operators); &#125; // 把当前运算符放在运算符队列的末端 operators.add(curOper); &#125; else &#123; // 括号 if ("(".equals(currentEle)) &#123; // 左括号时直接放入操作列表中 operators.add(Operator.BRACKETS); &#125; else &#123;// 当是右括号的时候就把括号里面的内容执行了。 // 循环执行括号里面的内容直到遇到左括号为止。试想这种情况(2+5*2) while (!operators.lastElement().equals(Operator.BRACKETS)) &#123; compute(nums, operators); &#125; //移除左括号 operators.remove(operators.size()-1); &#125; &#125; &#125; &#125; &#125; // 经过上面代码的遍历后最后的应该是nums里面剩两个数或三个数，operators里面剩一个或两个运算操作符 while (!operators.isEmpty()) &#123; compute(nums, operators); &#125; return nums.firstElement(); &#125; /** * 利用java.util.Stack计算四则运算字符串表达式的值，如果抛出异常，则说明表达式有误，这里就没有控制 * java.util.Stack其实也是继承自java.util.Vector的。 * @param computeExpr 四则运算字符串表达式 * @return 计算结果 */ public double computeWithStack(String computeExpr) &#123; //把表达式用运算符、括号分割成一段一段的，并且分割后的结果包含分隔符 StringTokenizer tokenizer = new StringTokenizer(computeExpr, "+-*/()", true); Stack&lt;Double&gt; numStack = new Stack&lt;Double&gt;(); //用来存放数字的栈 Stack&lt;Operator&gt; operStack = new Stack&lt;Operator&gt;(); //存放操作符的栈 Map&lt;String, Operator&gt; computeOper = this.getComputeOper(); //获取运算操作符 String currentEle; //当前元素 while (tokenizer.hasMoreTokens()) &#123; currentEle = tokenizer.nextToken().trim(); //去掉前后的空格 if (!"".equals(currentEle)) &#123; //只处理非空字符 if (this.isNum(currentEle)) &#123; //为数字时则加入到数字栈中 numStack.push(Double.valueOf(currentEle)); &#125; else &#123; //操作符 Operator currentOper = computeOper.get(currentEle);//获取当前运算操作符 if (currentOper != null) &#123; //不为空时则为运算操作符 while (!operStack.empty() &amp;&amp; operStack.peek().priority() &gt;= currentOper.priority()) &#123; compute(numStack, operStack); &#125; //计算完后把当前操作符加入到操作栈中 operStack.push(currentOper); &#125; else &#123;//括号 if ("(".equals(currentEle)) &#123; //左括号时加入括号操作符到栈顶 operStack.push(Operator.BRACKETS); &#125; else &#123; //右括号时, 把左括号跟右括号之间剩余的运算符都执行了。 while (!operStack.peek().equals(Operator.BRACKETS)) &#123; compute(numStack, operStack); &#125; operStack.pop();//移除栈顶的左括号 &#125; &#125; &#125; &#125; &#125; // 经过上面代码的遍历后最后的应该是nums里面剩两个数或三个数，operators里面剩一个或两个运算操作符 while (!operStack.empty()) &#123; compute(numStack, operStack); &#125; return numStack.pop(); &#125; /** * 判断一个字符串是否是数字类型 * @param str * @return */ private boolean isNum(String str) &#123; String numRegex = "^\\d+(\\.\\d+)?$"; //数字的正则表达式 return Pattern.matches(numRegex, str); &#125; /** * 获取运算操作符 * @return */ private Map&lt;String, Operator&gt; getComputeOper() &#123; return new HashMap&lt;String, Operator&gt;() &#123; // 运算符 private static final long serialVersionUID = 7706718608122369958L; &#123; put("+", Operator.PLUS); put("-", Operator.MINUS); put("*", Operator.MULTIPLY); put("/", Operator.DIVIDE); &#125; &#125;; &#125; /** * 取nums的最后两个数字，operators的最后一个运算符进行运算，然后把运算结果再放到nums列表的末端 * @param nums * @param operators */ private void compute(Vector&lt;Double&gt; nums, Vector&lt;Operator&gt; operators) &#123; Double num2 = nums.remove(nums.size() - 1); // 第二个数字，当前队列的最后一个数字 Double num1 = nums.remove(nums.size() - 1); // 第一个数字，当前队列的最后一个数字 Double computeResult = operators.remove(operators.size() - 1).compute( num1, num2); // 取最后一个运算符进行计算 nums.add(computeResult); // 把计算结果重新放到队列的末端 &#125; /** * 取numStack的最顶上两个数字，operStack的最顶上一个运算符进行运算，然后把运算结果再放到numStack的最顶端 * @param numStack 数字栈 * @param operStack 操作栈 */ private void compute(Stack&lt;Double&gt; numStack, Stack&lt;Operator&gt; operStack) &#123; Double num2 = numStack.pop(); // 弹出数字栈最顶上的数字作为运算的第二个数字 Double num1 = numStack.pop(); // 弹出数字栈最顶上的数字作为运算的第一个数字 Double computeResult = operStack.pop().compute( num1, num2); // 弹出操作栈最顶上的运算符进行计算 numStack.push(computeResult); // 把计算结果重新放到队列的末端 &#125; /** * 运算符 */ private enum Operator &#123; /** * 加 */ PLUS &#123; @Override public int priority() &#123; return 1; &#125; @Override public double compute(double num1, double num2) &#123; return num1 + num2; &#125; &#125;, /** * 减 */ MINUS &#123; @Override public int priority() &#123; return 1; &#125; @Override public double compute(double num1, double num2) &#123; return num1 - num2; &#125; &#125;, /** * 乘 */ MULTIPLY &#123; @Override public int priority() &#123; return 2; &#125; @Override public double compute(double num1, double num2) &#123; return num1 * num2; &#125; &#125;, /** * 除 */ DIVIDE &#123; @Override public int priority() &#123; return 2; &#125; @Override public double compute(double num1, double num2) &#123; return num1 / num2; &#125; &#125;, /** * 括号 */ BRACKETS &#123; @Override public int priority() &#123; return 0; &#125; @Override public double compute(double num1, double num2) &#123; return 0; &#125; &#125;; /** * 对应的优先级 * @return */ public abstract int priority(); /** * 计算两个数对应的运算结果 * @param num1 第一个运算数 * @param num2 第二个运算数 * @return */ public abstract double compute(double num1, double num2); &#125;&#125;]]></content>
      <categories>
        <category>Data Struture and algorithm</category>
      </categories>
      <tags>
        <tag>alogrithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[漫谈“推荐系统”]]></title>
    <url>%2F2014%2F09%2F05%2FRecSys%2F%E6%BC%AB%E8%B0%88%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[漫谈“推荐系统” 本文转自@复旦李斌的博客，非常白话的review了推荐系统领域的一些研究内容以及存在的挑战，mark一下。 由于需要准备一月底与三月中两个关于“推荐系统”的短期课程（前者在阿卜杜拉国王科技大学，没错就是那个传说中沙特的土豪大学！后者在悉尼科技大学），期间二月份还夹带了一个推荐系统相关的讨论班，所以从去年十二月开始我几乎每个周末至少得抽出一天的时间来做幻灯片。推荐技术是我个人研究兴趣并不是我日常工作内容，无法利用上班时间准备，再加上我有强迫症（必须要让累加起来多达二百多页的幻灯片风格色调字体公式图例都保持视觉上的审美愉悦与逻辑上的高度统一），所以在画图敲公式上花了很长时间。幻灯片做烦了，突然很想写点东西聊聊我对推荐系统的见解。此文不会出现公式，尽量用白话说清楚目前主流推荐技术的直观原理。 先谈谈问题背景，故事是这样的：互联网出现后，随着网上内容的增加，好学的小伙伴们发现很多他们不懂的姿势网上都有，可互联网不像图书馆搞个书目索引就行，于是出现了搜索引擎帮助小伙伴们在茫茫互联网上找到他们感兴趣的东西，但条件是你必须知道你想要什么，然后提取成关键字去搜，所谓信息检索（Information Retrieval）。十年过去了，信息爆炸了，问题出现了，搜索引擎动辄返回几十万个结果，或者有些想要的信息却根本不知道它的存在，甚至根本不知道如何用关键词描述你想要的东西，这时推荐系统应运而生——小伙伴们不用自己去找，推荐系统就会根据小伙伴们的个人资料历史纪录从海量信息中自动筛选符合小伙伴们口味的内容进行推荐，所谓推荐系统（Recommender Systems）。如今，推荐系统已经无处不在，几乎所有的网络服务都集成了推荐系统。在此我就不提那些学术圈老掉牙的例子什么Netflix电影推荐啊Google新闻推荐啊Yahoo!广告推荐啊什么的了；我瞄了下自己的手机举几个例子吧：微博朋友推荐、虾米音乐推荐、LinkedIn工作推荐、YouTube视频推荐、大众点评餐馆推荐、等等。 既然要谈的是推荐“技术”，那么我得先把推荐问题用数学语言形式化了。淡定淡定，推荐问题形式化后非常简单干净——就三个矩阵（从这里往下得有一丢丢想像力，能在脑海里想象简单的矩阵操作）。最重要的一个矩阵是评分或偏好矩阵（Rating/Preference Matrix），其每一行对应一个用户，每一列对应一件物品，矩阵中的任一元素就是某用户对某物品的感兴趣程度（评分可以用正整数表示，点赞神马的可以用0/1表示），不失一般性，下面我们仅基于评分矩阵讨论。这个评分矩阵是极其稀疏的，因为每个用户只可能对很少一部分物品打分。第二个矩阵是用户信息矩阵，每一行对应一个用户，每一列对应一个用户属性（如年龄、职业、地区、标签等）。第三个矩阵是物品信息矩阵，每一行对应一件物品，每一列对应一个物品属性（如电影的流派、导演、演员等）。推荐问题的目标就是：基于给定的三个矩阵，把评分矩阵中缺失元素的评分预测出来，并基于预测出来的评分把得分高的物品推荐给相应用户。这里值得注意的是，只有评分矩阵是所有推荐技术所必需的，用户信息矩阵与物品信息矩阵这两者是可选的。真实推荐系统面临最大的挑战是评分矩阵的大规模与稀疏性。 接下来我把一些当前常用的推荐技术分门别类。推荐技术可先分为三大类：基于人口统计的推荐技术（Demography-based）、基于物品内容的推荐技术（Content-based）、以及基于协同过滤的推荐技术（Collaborative Filtering，简称CF）。基于人口的又包括基于用户资料的（User Profile）和基于信任关系的（社交网络上的好友关系）等。基于物品内容的又可细分为基于元数据的（即Metadata，比如电影的流派、导演、演员等）和基于内容数据的（比如视频数据、音频数据）等——真实应用大多是基于元数据的，基于内容数据的推荐系统由于语义鸿沟（Semantic Gap）和效率问题，做了几十年，一直未突破（深度学习能突破么，拭目以待呵呵）。虽然基于人口和基于内容的两大类推荐技术在实际中的应用极广而且效果在某些应用场景下不比第三类技术协同过滤差，那为什么协同过滤技术一跃成为当今主流的推荐技术了呢？有以下几方面原因：1）协同过滤问题相当干净，只需要一个评分矩阵，不需要用户信息与物品信息，这解决了用户物品信息缺失场景下的推荐问题。2）协同过滤问题的本质是矩阵补全问题（Matrix Completion），也就是把一个稀疏矩阵的缺失元素给估计出来，这是机器学习中一个经典问题，除了推荐之外还有无数的应用都可归结为矩阵补全问题，所以机器学习的高速发展也促进了协同过滤技术。3）2006年Netflix发起的那个百万美元大奖功不可没，直接上演了持续多年相关研究领域全民做推荐的激情岁月，虽然吧这个竞赛使用了一个完全误导的评价指标来判断推荐算法的优劣（使用的是RMSE指标，这是一个评价回归的指标，而推荐问题事实上是一个排序问题）。跑题了，接着分类。协同过滤技术可以继续分为基于记忆的（Memory-based）和基于模型的（Model-based）。基于记忆的继续可分为基于用户的（User-based）和基于物品的（Item-based）；而基于模型的可以继续分为基于矩阵分解的（Matrix Factorization）和基于联合聚类的（Co-clustering）。基于记忆的协同过滤技术使用的是K-近邻（K-Nearest Neighbors）的思想，而基于模型的协同过滤技术使用的是机器学习方法。分类结束。 真实系统都是使用的混合策略（Hybrid Strategy），多为基于人口、基于元数据、以及基于用户或物品的协同过滤推荐技术的各种组合。基于模型的协同过滤虽然使用了高端大气上档次的机器学习方法，但做过真实应用的同学都懂的，简单粗暴才是王道，提出并改进一个模型连发三篇顶级机器学习会议论文提高了一个百分点，往往不如真实系统中屌丝程序员在哪疙瘩加个莫名的阈值来得有效。那为什么顶尖互联网企业都在搞机器学习呢？这么说吧，五百的衣服和五万的衣服功能都是一样的，但是地位高到一定程度，除了衣服的基本功能外我们还会追求一些其它的东西。但是如果只是想基于推荐技术做一个网络服务神马的，就没必要搞那么玄的机器学习花样了，反而大规模计算的效率问题和推荐应用本身是否有市场前景是更应该考虑的，有了这些，最基本的基于人口统计与基于记忆的推荐技术就能搞定大多数应用。貌似跑题了，接着说混合策略。有些混合策略是对不同推荐技术的结果加权相加（Weighting）；有些是根据场景不同在不同技术间跳转（Switching），比如新用户基于人口统计老用户基于协同过滤；有些是一个网页上不同区域同时显示不同推荐技术的结果（Mixing）；有些是用一个推荐技术对另一个推荐技术输出的结果进行提升（Cascading）。 除了基于模型的协同过滤技术外，其它的推荐技术在原理上都相对简单，使用一些相关查询和启发式算法就能解决。这段就把除基于模型的协同过滤以外的推荐技术都简单介绍下。首先是基于人口统计学的，该类推荐技术需要基于用户信息矩阵和评分矩阵。原理很简单，就是查找用户信息矩阵中背景类似的用户，然后把对应评分矩阵中打高分的物品推荐给背景类似的用户。举个例子，用户信息上显示两个人年龄相仿居于湾区互联网从业者，于是系统就会认为这两人相关性强会有共同爱好，把其中一人打高分的电影推荐给另一个。这种推荐技术的优点是简单，一些相关性查询操作就能搞定，而且没有“冷启动（Cold-start）”问题（即用户缺失历史评分纪录）；缺点是无法个性化推荐，基于人口统计相似度的假设太强，比如同为IT男，一个技术宅，一个伪文青，你把技术宅喜欢的东西推荐给伪文青肯定是不靠谱的，比如我排斥一切XX侠的电影。接下来是基于物品内容的推荐技术，该类推荐技术需要基于物品信息矩阵和评分矩阵（这里只讨论基于元数据的，基于真实内容的开门课都讲不完）。该类推荐技术的原理也很简单，基于元数据计算物品之间的相关性，然后把与该用户以前打高分的物品最相关的物品推荐给他。这类推荐技术比前一种靠谱，因为用户在同类物品上一般会表现出相同的兴趣程度。举个例子，我如果对《巴黎我爱你》打了个高分，那么推荐系统就会向我推荐强关联的《纽约我爱你》，而我也会对同一血统的电影很感兴趣。因此，该类技术的优点就是对偏好的建模较为精细与准确；缺点是依赖于物品元数据包含的信息量，以及存在冷启动问题（需要用户的历史评分）。接下来介绍基于记忆的协同过滤技术，该类推荐技术的标准问题设置仅需要评分矩阵，当然近年来学术界有些关于迁移学习（Transfer Learning）在推荐系统中的研究会使用到用户与物品的信息矩阵、甚至使用另一个域的评分矩阵（我以前在推荐系统中的工作主要在这块，感兴趣的同学可以用谷歌百度下一个二页纸的小文“Cross-Domain Collaborative Filtering: A Brief Survey”），但这里我们只讨论标准的协同过滤问题设置。基于用户的协同过滤分为两步：第一步是计算用户之间的相关度，这里的相关度是评分矩阵行向量间的相关度，其直观意义就是如果两个用户在相同物品上打的分越接近，那么这两个用户的偏好也越接近。如果评分矩阵是一个没有缺失项的满矩阵，那么行向量之间的相似度直接可以用欧式距离或者夹角余弦计算；由于评分矩阵是稀疏矩阵，因此计算相关性首先要把两个行向量之间的交集（打过分的物品）找出来，并只在该交集上计算一个类似夹角余弦的值，叫作皮尔森相关系数（Pearson Correlation Coefficients）。在取得了与所有用户两两之间的相关性后，第二步就是预测该用户的缺失评分。给定一个待预测用户，找到他的K-近邻用户集合，他的缺失评分就是用其K-近邻用户对应物品上的历史评分用相关性加权平均得到。基于物品的协同过滤和基于用户的是对称的，一个是对行操作，一个是对列操作，方法和原理都是一样的。 从这里往后的内容主要将介绍基于模型的协同过滤技术。在大多数推荐系统的介绍中一般直接就把基于模型与矩阵分解等同起来了，因为应用到实际推荐系统中的基于模型的推荐技术一般都是基于矩阵分解的，比如Netflix百万大奖得主提出的(Time)SVD++方法（但事实上前几名所用的方法都是很多种算法集成的结果，所以说研究归研究，在实际应用中干净优美的模型很难超越东拼西凑再加点人脑规则的四不像，这个道理我早在八年前做TRECvid的时候就总结出来了，当时直接导致我放弃多媒体研究直到现在一直对计算机视觉抱有悲观态度）。又跑题了，继续说基于模型的推荐技术。除了矩阵分解，这里我还要额外介绍一种基于联合聚类的技术，所谓联合聚类，就是对用户与物品（即评分矩阵的行与列）同时聚类，聚类的方法可以是简单的K-Means，但更优美的建模方法是双向混合模型——我个人非常喜欢这种建模方式，虽然对于评分预测的性能没有基于矩阵分解的好（因为矩阵分解的目标就是拟合评分而混合模型的目标是估计用户与物品在潜在类型上的分布）。 先说基于矩阵分解的方法吧。给定一个评分矩阵（大小为NM），把该矩阵分解为两个矩阵的乘积，一个是用户特征矩阵（大小为NK），一个是物品特征矩阵（大小为M*K），其中潜在特征（latent features）的维度远小于用户数与物品数；目标函数就是两个特征矩阵的重构与给定评分矩阵在那些可见评分上的值尽可能接近，一般使用矩阵范数（Frobenius norm），即两个矩阵相减所有元素上残差平方和；再加上对两个特征矩阵的矩阵范数作为正则化项。改优化问题常用两种方法解决：一种是交替最小二乘（Alternative Least Squares），交替优化用户特征矩阵与物品特征矩阵，在优化其一的时候固定另一个的值视其为已知，这样就相当于每轮解决一个标准的最小二乘问题，最后收敛到局部最优解。该方法的优点是收敛速度快，缺点是需要对用户数与物品数大小的方正求逆，难以规模化。另一种是随机梯度下降（Stochastic Gradient Descent），对每个用户与每个物品（评分矩阵的行与列）分别求偏导建立牛顿迭代公式，然后用可见评分顺序对这些迭代公式进行更新。该方法的优点是可以并行化、效率高，目前大规模矩阵分解都是用的这种优化算法；缺点可能是收敛速度没有第一种快（这点我不是很确定）。最后说说这种形式矩阵分解的物理含义。这样分解成两部分后，就相当于用户和物品都被放置到一个潜在的K维特征空间，只要拥有相似潜在特征的用户与物品，他们的夹角就小乘积就大得到的预测评分也就相应更高。那么凭什么我们能指定一个“潜在的K维特征空间”呢？拿Pandora的音乐推荐举例子，每个音乐有几百条“音乐基因”就是音乐的显式特征（不知道音乐基因的可以去古歌百度一下Music Gene Project）。如果不降维的话，那么音乐特征矩阵和用户特征矩阵的纬度就是其真实的特征纬度。假设我们基于主成分分析（PCA）用相同的一套基分别对这两个矩阵进行线性变换，那么得到的两个矩阵就可以认为是投影到潜在特征空间的两个矩阵，而这两个矩阵的乘积和原来的两个矩阵是一样的（因为当中两个投影矩阵的乘积是单位矩阵）。那么假如我们只用前K个基投影呢？那我们就得到了只有K维潜在特征空间的低秩矩阵。所以在实际问题中，我们都不需要知道真实的特征空间，只需要人为指定一个K维潜在特征空间就可以了，得出的结果可以认为是真实特征经过某个线性变化后投影到一个低维潜在特征空间。 最后介绍基于联合聚类的方法。这类方法的物理意义更直观，其实也能表示成为矩阵分解的形式，但不同的是联合聚类把评分矩阵（大小为NM）分解为三部分，一个是用户隶属度矩阵（NK），表示每个用户在K个潜在用户组上的分布情况，所有元素为正每行加起来为1；一个是物品隶属度矩阵（ML），表示每个物品在L个潜在物品组上的分布情况，所有元素为正每行加起来为1；还有一个是压缩评分矩阵（KL），表示某个用户组对某个物品组给的评分。使用这三个矩阵的乘积重构评分矩阵可以对缺失评分进行预测。解决该问题最简单的方法是分别对行与列进行K-Means聚类，然后用户与物品隶属度矩阵就根据聚类结果把对应的组设为1其它为0（硬聚类），而压缩评分矩阵是每个联合聚类中评分的平均值。更一般性的建模方法是令两个隶属度矩阵为在潜在组别上的分布（软聚类），这需要使用期望最大（Expectation-Maximization）算法解决；进一步地考虑贝叶斯，由于隶属度就是Dirichlet分布，那么其实该联合聚类问题可以使用Latent Dirichlet Allocation的变种建模，叫作Bi-LDA，使用吉布斯采样解决。这类方法的具体细节就不介绍了。 至此为止，基本的推荐技术大体都过了一遍了。剩下的就是解决协同过滤技术中的各种挑战，比如兴趣随时间与环境变化问题、矩阵稀疏问题、冷启动问题、噪声问题、大规模矩阵分解问题等等，这些“挑战”也是近年来学术界写论文的切入点。但其实在工业界，这些所谓的挑战大多都不是问题，或是可以用替代方案解决、或是对结果真正的影响不大。我个人觉得无论是学术界还是工业界，当前最重要的问题还是大规模矩阵分解问题（我也无法免俗大数据啊），各路神仙也从不同的突破点去解决这个问题，有使用分布式计算的、有提出加速优化算法的、有使用近似哈希算法的等等。在我的短期课程中，针对这些挑战的一些解决方案也占了很大的比重，但是这里就不一一累述了，用纯文字描述个问题都得花半页纸。其实我本来还想谈一下在线推荐系统，也是如今精准广告投放背后的核心技术，但是也因为问题的设置和协同过滤有很大的不同，技术上也几乎没有什么交集，就不展开了。在线推荐系统的主要技术是一大类被称为Multi-Armed Bandits（MAB）的方法——没错就是老虎机！广告投放就像赌博，你选哪个广告投放出去都会有不同的回报，随着一次又一次的尝试，从失败中吸取教训，慢慢学习到背后隐含的规律，之后就可以保持大概率的赢面。MAB的在线学习策略遵循的是“开采（Exploitation）”与“探索（Exploration）”，一边尽量投注之前赢面较大的广告，一边又不停尝试其它未知底细的广告以发现更高的赢面——这不就像是人生么？有些人觉得现状不错就一直保持着开采状态，而有些人则时不时探索一下，也许会走一些弯路，可或许在一段弯路过后会发现比以前更好的一条路；更何况，人生并不只是用最后累积到的财富来论成败，沿途的风景，妙不可言的或是痛彻心扉的，都是人生独一无二的财富。]]></content>
      <categories>
        <category>RecSys</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Apache Mahout 入门篇]]></title>
    <url>%2F2014%2F08%2F27%2FRecSys%2FApache%20Mahout%20%E5%85%A5%E9%97%A8%E7%AF%87%2F</url>
    <content type="text"><![CDATA[author: sheng junhui weibo: @armysheng blog: youngfor.me email: armysheng@Gmail.com 转载请注明出处 0. 基本概念的介绍 在搭建开发环境know how的步骤前，最好是先做一些konw what的学习，这里先简单介绍一下一些基本的概念。 0.1 什么是Mahout？ Apache Mahout 翻译成中文是骑大象的人，或驯象师。这里是指Apache开源社区维护的一个可伸缩(Scable)的机器学习库。Mahout机器学习库*目前实现的算法*包括协同滤波，分类，聚类，特征降维等等。我们知道Hadoop的logo是一头大象，而Mahout要做的就是驯服这头大象。Mahout致力于提供可支持超大数据集，支持商业使用的开源机器学习库，并且构建充满活力的开源社区。---Mahout 主页 0.2 Mahout又是一个Maven项目,Maven又是什么呢？ Apache Maven，是一个软件（特别是Java软件）项目管理及自动构建工具，由Apache软件基金会所提供。基于项目对象模型（缩写：POM）概念，Maven利用一个中央信息片断能管理一个项目的构建、报告和文档等步骤。项目对象模型存储在命名为 pom.xml 的文件中。POM 文件中包含了它的配置信息，依赖信息等等。---Maven wiki 所以，Mahout是一个用java语言开发的，Maven管理工具管理的Maven project，因此整个过程也需要安装Maven。事实上，安装完java, Maven, Mahout 以后，不需要eclipse，整个Mahout机器学习库已经可以被完整的调用，也可以在此基础上进行开发。而eclipse是一个业界广泛使用的IDE，在eclipse上让人得心应手，而且eclipse对Maven项目也有很好的支持，只需要装上M2E插件便可以非常方便的使用。 1. 安装 首先介绍一下开发环境：Win7 32位系统 需要安装的软件: jdk1.7.0_60 +apache-maven-3.2.1+TortoiseSVN 1.8.7+mahout-distribution-0.9(源码&amp;&amp;包) 由于我们公司内部走的是代理网络，所以部分工具安装完成使用前，都会介绍如何配置代理信息，如果你使用的是直连网络，请忽视这些段落。 1.1 Java环境的安装 我们选择的java版本是jdk1.7.0_60，可以在oracle主页下载，这里不建议使用java8，在编译过程中可能会出现一些不必要的问题。java安装比较简单，直接下载二进制文件进行安装。我们安装在D:\Program Files\Java\jdk1.7.0_60,安装完成后在控制命令行中输入: 1234C:\Users\shengjh&gt;java -versionjava version "1.7.0_60"Java(TM) SE Runtime Environment (build 1.7.0_60-b19)Java HotSpot(TM) Client VM (build 24.60-b09, mixed mode, sharing) 看似java已经可以运行，其实java的环境变量尚未加入，而是在安装过程中，将java.exe,javaw.exe,javawc.exe 三个文件copy到了system32目录下。所以我们需要手动添加环境变量: 右键计算机-&gt;属性-&gt;高级系统设置-&gt;环境变量-&gt;系统变量-&gt;新建 变量名:JAVA_HOME 变量值:D:\Program Files\Java\jdk1.7.0_60 这一步非常重要，不做的话再Maven的安装上,会出现JAVA_HOME找不到的错误，至此我们完成了JAVA环境的配置。 1.2 Maven环境搭建 Maven 是一个项目管理和构建自动化工具。但是对于我们程序员来说，我们最关心的是它的项目构建功能。所以这里我们介绍的就是怎样用 maven 来满足我们项目的日常需要。 Maven 使用惯例优于配置的原则 。它要求在没有定制之前，所有的项目都有如下的结构： 目录 目的 ${basedir} 存放 pom.xml和所有的子目录 ${basedir}/src/main/java 项目的 java源代码 ${basedir}/src/main/resources 项目的资源，比如说 property文件 ${basedir}/src/test/java 项目的测试类，比如说 JUnit代码 ${basedir}/src/test/resources 测试使用的资源 一个 maven 项目在默认情况下会产生 JAR 文件，另外 ，编译后 的 classes 会放在 ${basedir}/target/classes 下面， JAR 文件会放在 ${basedir}/target 下面。 在确保JAVA环境已经搭建好的情况下我们开始安装Maven，可以从Maven的主页下载二进制文件，这里我们选择的是Maven 3.2.1，并解压到你常用的软件路径，这里我们安装在D:\Program Files\apache-maven-3.2.1\。该页的最后也该处了Maven的安装方法。这里我们使用最简单的将Maven的bin路径D:\Program Files\apache-maven-3.2.1\bin加入到系统变量Path中: 完成后在命令行中输入mvn --version 查看maven的版本: 代理配置 之前介绍了代理的问题，因为后续的操作，maven会自动联网搜索依赖包，这里在继续操作之前先介绍Maven的代理配置。 在Maven的配置文件都在它的安装目录下的\config\setting.xml文件中&lt;proxies&gt;字段定义了代理的设置。根据代理情况修改Host，port，username，password等等。这里我们只修改了host和port。 接下来我们用 maven 来建立最著名的“Hello World!”程序，来更好的了解Maven。 **注意：**如果你是第一次运行 maven，你需要 Internet 连接，因为 maven 需要从网上下载需要的插件。 我们要做的第一步是建立一个 maven 项目。在 maven 中，我们是执行 maven 目标 (goal) 来做事情的。 maven 目标和 ant 的 target 差不多。在命令行中执行下面的命令来建立我们的 hello world 项目 1mvn archetype:create -DgroupId=com.`mycompany.app -DartifactId=my-app 命令执行完后你将看到maven生成了一个名为my-app的目录，这个名字就是你在命令中指定的artifactId,进入该目录，你将发现以下标准的项目结构： 其中:src/main/java 目录包含了项目的源代码，src/test/java 目录包含了项目的测试代码，pom.xml是项目的项目对象模型（Project Object Model or POM）。 之后我们输入cd my-app进入该项目 再输入mvn package 回车来build这个项目，得到，折后target目录下回生成该项目的类文件和包： 接着你可以使用以下的命令来测试新编译和打包出来的jar包， 1java -cp target/my-app-1.0-SNAPSHOT.jar com.mycompany.app.App 至此Maven的安装和介绍就到这里，这一章节主要参考了Apache Maven 入门篇 ( 上 )和maven 教程一 入门，以及Maven主页，阅读原文获得更多的了解。 1.3 Mahout源码的编译 Mahout的源码和编译后的包可以从Mahout官网上获得,然而在使用这里下载的0.9发行版源码时，somehow遇到了一些错误。 这里我们选择用SVN获得它最新的源码。Tortoise SVN是大家常用的代码托管工具，可以从它的主页上下载到。 安装完成后，在使用之前，我们还要对他做一下代理配置。 代理配置 SVN的代理配置只需要打开，在开始按钮-&gt;程序-&gt;Tortoise SVN-&gt;settings,找到Network项进行如下配置： 接着我们就可以开始编译源码了 首先得到源码，在命令行输入： 1svn co http://svn.apache.org/repos/asf/mahout/trunk cd trunk并编译： 12345# With hadoop-1.2.1 dependencymvn clean install# With hadoop-2.2.0 dependencymvn -Dhadoop2.version=2.2.0 clean install 这里可以选择Hadoop依赖版本，在没有hadoop的情况下，也可以编译。 编译时经常会遇见Test failure的error而卡住，关于这个问题，是由于mahout中有大量的测试例子，test failure是遇到了broken unit test.遇到这种问题要么需要自己去定位修复这个broken test，要么可以等待官方发行并update svn 进行修复。关于这个问题参加这里，一般的解决方法是使用： 12345# With hadoop-1.2.1 dependencymvn -DskipTests clean install # With hadoop-2.2.0 dependencymvn -Dhadoop2.version=2.2.0 -DskipTests clean install 这里我们选的是 1mvn -DskipTests clean install 最后得到结果： 至此，Mahout的源码就编译完毕，我们可以看到每个Mahout源码的目录下都生成了相应的target文件,包含对应的类和.jar包。 1.4 如何在Eclipse上使用Mahout？ 编译完后的mahout还是不能在eclipse上使用，下面我们介绍一下如何在eclipse下开发Maven project。 首先Eclipse版本选择，关于eclipse的版本代号问题请参考这里。 我们选择的是Eclipse IDE for Java EE Developers(JUNO)，它包含了Maven所需的一些依赖包。Eclipse standard (Indigo)由于缺乏依赖包，在安装上Maven插件上会有问题。eclipse安装包可以在官网下载。 代理配置 eclipse安装完成后，启动会自动检测本机的代理设置，将本机的代理设置填到软件设置界面。但是如果你想修改的话可以在下面找到: eclipse-&gt;windows-&gt;perferences-&gt;general-&gt;Network Connection 接着我们介绍一下m2eclipse插件和它的安装方法。m2eclipse插件是一款一流的支持Apache Maven的eclipse插件。用户可以用它更方便的编辑Maven的pom.xml文件，可以在IDE上build一个Maven工程。 启动eclipse定位到 Help -&gt; Install New Software… 点击Add，分别填入: Name:m2e Location:http://download.eclipse.org/technology/m2e/releases 点击next，同意协议，确认warning后安装。安装完成后，eclipse会提示你重启。 安装完成后我们还需要做一些设置 定位到 eclipse -&gt; windows -&gt; preference -&gt; Maven -&gt;Installations 将本地的Maven路径Add进来。 将User-setting 定位到Maven安装路径下的.\conf\setting.xml文件。 这样我们就完成了插件的安装，我们便可以Import我们之前已经编译好的Mahout源码，在eclipse下方便的查看Mahout的类文件。 导入时可能会遇到Maven-scala-plugin缺失的情况，点击finish 会自动找到缺失的plugin并且安装（需重启eclipse）。 导入buliding workspace 后，提示了有错误： error：Plugin execution not covered by lifecycle configuration: org.scala-tools:maven-scala-plugin:2.15.2:testCompile (execution: default, phase: test-compile) 这是由于m2eclipse-scala插件的缺失，The default maven plugin does not support Scala out of the box, so you need to install the m2eclipse-scala connector. 即m2eclipse-scala connector 的缺失。所以我们需要再次安装这个插件,方法和上面的类似。 Add: Name: Maven for Scala Location:http://alchim31.free.fr/m2e-scala/update-site/ (若错误持续，则右键改project-&gt; Maven -&gt; Update Project) 最后我们成功的导入Mahout源码到了eclipse中。 2. 简单推荐算法的实现 接着我们介绍如何用Mahout来建立简单的推荐算法，大部分过程我们follow了官方的这个quick guide Step 0:创建数据集，命名为Dataset.csv，内容如下： 1,10,1.0 1,11,2.0 1,12,5.0 1,13,5.0 1,14,5.0 1,15,4.0 1,16,5.0 1,17,1.0 1,18,5.0 2,10,1.0 2,11,2.0 2,15,5.0 2,16,4.5 2,17,1.0 2,18,5.0 3,11,2.5 3,12,4.5 3,13,4.0 3,14,3.0 3,15,3.5 3,16,4.5 3,17,4.0 3,18,5.0 4,10,5.0 4,11,5.0 4,12,5.0 4,13,0.0 4,14,2.0 4,15,3.0 4,16,1.0 4,17,4.0 4,18,1.0 Step 1: 新建一个Maven Project，选择simple project Step 2: 命名为UBrecommender Step 3:打开pom.xml文件，加入Mahout的依赖配置,并将Dataset.csv放到工程目录下 12345678&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.mahout&lt;/groupId&gt; &lt;artifactId&gt;mahout-core&lt;/artifactId&gt; &lt;version&gt;0.9&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; Step 4: 在src\main\java\下新建SampleRecommender.java 12345678910111213141516171819202122232425262728293031323334353637import java.io.File;import java.io.IOException;import org.apache.mahout.cf.taste.impl.model.file.FileDataModel;import org.apache.mahout.cf.taste.impl.neighborhood.ThresholdUserNeighborhood;import org.apache.mahout.cf.taste.impl.recommender.GenericUserBasedRecommender;import org.apache.mahout.cf.taste.impl.similarity.PearsonCorrelationSimilarity;import org.apache.mahout.cf.taste.model.DataModel;import org.apache.mahout.cf.taste.neighborhood.UserNeighborhood;import org.apache.mahout.cf.taste.recommender.RecommendedItem;import org.apache.mahout.cf.taste.recommender.UserBasedRecommender;import org.apache.mahout.cf.taste.similarity.UserSimilarity;import java.util.List;public class SampleRecommender&#123; public static void main(String[] args) throws IOException, Exception&#123; DataModel model = new FileDataModel (new File("dataset.csv")); UserSimilarity similarity = new PearsonCorrelationSimilarity(model); UserNeighborhood neighborhood = new ThresholdUserNeighborhood(0.1, similarity, model); UserBasedRecommender recommender = new GenericUserBasedRecommender(model, neighborhood, similarity); List&lt;RecommendedItem&gt; recommendations = recommender.recommend(2,3); for (RecommendedItem recommendation : recommendations) &#123; System.out.println(recommendation); &#125; &#125; &#125; Step 5: 运行项目，run as java application 我们便得到推荐结果： RecommendedItem[item:12, value:4.8328104] RecommendedItem[item:13, value:4.6656213] RecommendedItem[item:14, value:4.331242] 至此我们便完成了简单的一个推荐算法。 总结 总结来说我们安装了jdk，安装了Maven，安装了Eclipse以及在eclipse上使用Maven所需的一些插件，最后我们能够顺利地查看和修改Mahout的源码。 **Tips：**环境配置中会遇到许多问题，请多谷歌，国内如果访问不够顺畅，请直接使用一些谷歌的服务器IP来流畅访问，你也可以将浏览器的默认搜索改为这个地址。]]></content>
      <categories>
        <category>RecSys</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oryx 推荐系统初体验]]></title>
    <url>%2F2014%2F08%2F25%2FRecSys%2FOryx%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%88%9D%E4%BD%93%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[Oryx 推荐系统初体验 Summary Oryx 的前身叫 Myrrix，后来被 Cloudera 收购改了这个名字。值得一提的是 Oryx 的维护者就是 Mahout 的主要贡献者——Sean Owen Oryx开源项目旨在提供实时的大规模机器学习/预测分析基础框架目前它实现了一列工程应用常用到的机器学习算法：协同过滤，分类/回归以及聚类。Oryx 通过Apache hadoop来实现对大规模的数据流进行建模。同时可以通过实时的REST API来对建好的模型进行查询，并且有也可以通过API将新数据输入到模型中进行训练。 Oryx 具有以下特点： Hadoop 版本跟着 CDH 升级 将推荐引擎分为 Serving Layer 和 Computing Layer，隔离出 Serving Layer 让扩展变得很容易 同时支持推荐、聚类、分类的机器学习 数据引入了 Generation 的概念，提供了很好的增量补充数据的支持 目前Oryx release的版本是Oryx1.0，而Oryx2.0也正在孵化当中。相较于1.0版本，Oryx2.0更好的实现了lambda架构，使得各个层之间可复用性更强。实现了比1更多的机器学习算法（1.0只实现了，ALS协同过滤，随机森林，以及K-means++算法）同时2.0版本还引入更新的Spark和Streaming技术。 安装Oryx Oryx 的安装需要运行环境，以及Hadoop2.3或以后的运行环境，这里我们悬着Cloudera公司的CDH5发行的hadoop版本，也是官方推荐的版本。CDH5的安装，请参见上一篇文章。 Oryx的Server层和Computation层都是独立的Jar包oryx-serving-x.y.z.jar和oryx-computation-x.y.z.jar，只需要用java命令单独的运行这两个Jar包即可。需要注意的是这里的两个jar包不是在hadoop jar命令中运行，只是在长期跑的服务器程序。 跑Oryx例子stepBystep Orxy官方的项目提供了3个机器学习的例子，分别是用协同过滤做推荐，用随机森林做分类，用K-means++做聚类。在运行这些例子之前我们先要做一下准备工作。 已经建立了CDH5 或者Hadoop2.0+的环境，如果没有请参考这里 运行git -clone https://github.com/cloudera/oryx.git，将项目下载到本地的机子上并用unzip命令解压，并cd到该目录下。 这样我们就准备好了，开始跑第一个例子吧。 例1.建立一个简单的推荐系统 第一个例子采用了交替最小二乘的算法ALS (alternating least squares)是对audioscrobbler数据库（last.fm的音乐数据）的一些样本建立推荐系统。为了建立这个系统，ALS-Modle要求的输入的原始数据格式必须是user,item或者是user,item,strength。每列的数据必须用,隔开，前两列可以是任何数据类型（数字和非数字都可以），最后一列是可选的，但必须是数字类型。 我们看一眼audioscrobbler数据库的数据。 1000002,&quot;A Perfect Circle&quot;,144 1000002,&quot;Aerosmith&quot;,314 1000002,&quot;Metallica&quot;,329 1000002,&quot;Counting Crows&quot;,157 1000002,&quot;Dire Straits&quot;,125 1000002,&quot;Free&quot;,155 1000002,&quot;Guns N' Roses&quot;,236 1000002,&quot;Goo Goo Dolls&quot;,119 1000002,&quot;Michael Jackson&quot;,104 1000002,&quot;Barenaked Ladies&quot;,115 第一列为用户ID，第二列为歌手的名字，第三列为喜爱度。 正常情况下会把数据放在hadoop下跑，我们可以通过下面的指令把下载下来的数据库存放到hdfs上： hadoop fs -mkdir -p /user/oryx/example/00000/inbound hadoop fs -copyFromLocal [data file] /user/oryx/example/00000/inbound/ 这里我们只在本地做一下实验，将数据拷贝到/tmp/oryx/example: mkdir -p /tmp/oryx/example1/00000/inbound cp audioscrobbler-sample.csv.gz /tmp/oryx/example/00000/inbound/ 为了运行Serving层和computation层，我们必须定义一个配置文件，来告诉程序如何运行。配置文件详细的说明可以在common/src/main/resources/reference.conf下查看。我们这的配置例子如下： model=${als-model} model.instance-dir=/tmp/oryx/example1 model.local-computation=true model.local-data=true model.features=25 model.lambda=0.065 我们可以用vim oryx-example1.conf新建一个配置文件将内容填入。 接着我们便可以运行计算层和服务层了。可以看到model类型为ALS-model，instance-dir，设定了存放数据的地址。 java -Dconfig.file=oryx-example1.conf -jar computation/target/oryx-computation-1.0.1-SNAPSHOT.jar 我们可以看到程序的运行日志，看到从读取数据到不停跌代进行矩阵分解的整个过程 接着我们运行serving层： sudo java -Dconfig.file=oryx-example1.conf -jar serving/target/oryx-serving-1.0.1-SNAPSHOT.jar serving 层开启了Tomcat网络服务，并且加载了模型中的元素。日志如下： 我们还可以在浏览器中查看运行的结果，在浏览器上输入http:\\localhost，我们便可以显示的看到所有oryx提供的API。 包括对某一用户做推荐，物品间的相似度，用户对某个item的偏好，最受欢迎Item等等，甚至可以在原来数据的基础上添加数据再对这些数据进行推荐，非常方便。 在浏览器中输入http:\\localhost:8080可以查看计算时候的资源消耗等等一系列属性，由于我们是在本地跑，所以上面没有数据显示。 至此我们的第一个实验就做完了。 例2.利用随机森林算法做分类 这里随机森林算法是机器学习中常用的算法，有关它的介绍可以看这里。例子二对某一地区的森林覆盖类型做分类的例子。数据集采用了UCL repository。该数据集记录了581012个样本地区的54个特征，包括海拔，坡度，到最近水域的垂直距离和水平距离，土壤类型等等。更详细的描述可以参见covtype.info文件。 它的一条样本是这个样子的： 2596,51,3,258,0,510,221,232,148,6279,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5 跟例子1类似，我们同意需要配置文件orxy-example2.conf，在里面输入： model=${rdf-model} model.instance-dir=/tmp/oryx/example2 model.local-computation=true model.local-data=true inbound.numeric-columns=[0,1,2,3,4,5,6,7,8,9] inbound.target-column=54 inbound.column-names=[&quot;Elevation&quot;, &quot;Aspect&quot;, &quot;Slope&quot;, &quot;Horizontal_Distance_To_Hydrology&quot;, &quot;Vertical_Distance_To_Hydrology&quot;, &quot;Horizontal_Distance_To_Roadways&quot;, &quot;Hillshade_9am&quot;, &quot;Hillshade_Noon&quot;, &quot;Hillshade_3pm&quot;, &quot;Horizontal_Distance_To_Fire_Points&quot;, &quot;Wilderness_Area1&quot;, &quot;Wilderness_Area2&quot;, &quot;Wilderness_Area3&quot;, &quot;Wilderness_Area4&quot;, &quot;Soil_Type1&quot;, &quot;Soil_Type2&quot;, &quot;Soil_Type3&quot;, &quot;Soil_Type4&quot;, &quot;Soil_Type5&quot;, &quot;Soil_Type6&quot;, &quot;Soil_Type7&quot;, &quot;Soil_Type8&quot;, &quot;Soil_Type9&quot;, &quot;Soil_Type10&quot;, &quot;Soil_Type11&quot;, &quot;Soil_Type12&quot;, &quot;Soil_Type13&quot;, &quot;Soil_Type14&quot;, &quot;Soil_Type15&quot;, &quot;Soil_Type16&quot;, &quot;Soil_Type17&quot;, &quot;Soil_Type18&quot;, &quot;Soil_Type19&quot;, &quot;Soil_Type20&quot;, &quot;Soil_Type21&quot;, &quot;Soil_Type22&quot;, &quot;Soil_Type23&quot;, &quot;Soil_Type24&quot;, &quot;Soil_Type25&quot;, &quot;Soil_Type26&quot;, &quot;Soil_Type27&quot;, &quot;Soil_Type28&quot;, &quot;Soil_Type29&quot;, &quot;Soil_Type30&quot;, &quot;Soil_Type31&quot;, &quot;Soil_Type32&quot;, &quot;Soil_Type33&quot;, &quot;Soil_Type34&quot;, &quot;Soil_Type35&quot;, &quot;Soil_Type36&quot;, &quot;Soil_Type37&quot;, &quot;Soil_Type38&quot;, &quot;Soil_Type39&quot;, &quot;Soil_Type40&quot;, &quot;Cover_Type&quot;] 这里模型选择是rdf-model，同意定义了数据的一些格式0~9为数字，label列是滴54列，以及各个列的名名称。 然后我们把下载到的文件拷贝的/tmp/oryx/example2/00000/inbound下，按上例的方法运行serving层和computation层。只是配置文件做改变。 java -Dconfig.file=oryx-example2.conf -jar computation/target/oryx-computation-1.0.1-SNAPSHOT.jar sudo java -Dconfig.file=oryx-example2.conf -jar serving/target/oryx-serving-1.0.1-SNAPSHOT.jar 计算层的时间会比较长，需要较长时间等待。相信在集群上跑回好很多。 我们可以在/classify栏里手动的输入一个样本： 2500,51,3,258,0,510,221,232,148,6279,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0, 进行判别，得到分类结果。 例3.使用K-means++算法聚类 oryx的聚类算法采用了Scalable K-Means++算法。数据集使用KDD Cup 1999的数据，kddcup.data_10_percent.gz A 10% subset. (2.1M; 75M Uncompressed)。 由于内存较小，在用-Xmx 1024m分配了1G内存后，运行过程还是出现了OutofMemoryError。所以在本地单机没法进行，serve层开放的API如下： 可以使用/assign对新样本进行聚类。 0,tcp,http,SF,259,14420,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0.00,0.00,0.00,0.00,1.00,0.00,0.00,11,97,1.00,0.00,0.09,0.08,0.00,0.00,0.00,0.00 总结 至此我们就初步体验了oryx这个开源的大规模机器学习框架，虽然目前oryx实现的算法比较少，但是可以看到他的lamda架构，提供Rest API，generation概念，以及对Hadoop乃至Spark的适配，必将使得它在未来推荐系统领域占领一席之地。 参考 oryx官方github build-a-scalable-recommendation-system]]></content>
      <categories>
        <category>RecSys</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[安装CDH5 step by step]]></title>
    <url>%2F2014%2F08%2F14%2FRecSys%2F%E5%AE%89%E8%A3%85CDH5%20step%20by%20step%2F</url>
    <content type="text"><![CDATA[安装CDH5 step by step Cdh5安装基本上按照官网的手册进行安装。我们所用的系统环境是win7+VirtualBox-4.3.12+Centos6.5,下面让我们开始吧： 首先我们如果你装了老版本的Hadoop我们先把他移除，没装就可以无视： 1.暂停hadoop服务： $ for x in `cd /etc/init.d ; ls hadoop-hdfs-*` ; do sudo service $x stop ; done $ for x in 'cd /etc/init.d ; ls hadoop-0.20-mapreduce-*' ; do sudo service $x stop ; done 2.移除hadoop-0.20-conf-pseudo: $ sudo yum remove hadoop-0.20-conf-pseudo hadoop-0.20-mapreduce-* 安装Java： 从oracle上现在下载适合CHD5'.tar.gz'的JDK文件，目前支持的版本是java1.7.0_55. Extract the JDK to /usr/java/jdk-version; for example /usr/java/jdk.1.7.0_nn, where is a nn is a supported version. 3.In /etc/default/bigtop-utils, set JAVA_HOME to the directory where the JDK is installed; for example: 1export JAVA_HOME=/usr/java/default Symbolically link the directory where the JDK is installed to /usr/java/default; for example: ln -s /usr/java/jdk.1.7.0_nn /usr/java/default 下载CDH5包文件 下载支持CentOS的CDH5版本，然后使用yum命令在本地安装： $ sudo yum --nogpgcheck localinstall cloudera-cdh-5-0.x86_64.rpm 开始安装 (Optionally) add a repository key: 1$ sudo rpm --import http://archive.cloudera.com/cdh5/redhat/5/x86_64/cdh/RPM-GPG-KEY-cloudera 2.安装Hadoop伪节点模式 $ sudo yum install hadoop-conf-pseudo 启动Hadoop并验证环境 至此，Hadoop的伪节点安装已经完毕，下面我们就开始做一些配置，并启动Hadoop。 格式化NameNode 首先切换输入su切换到root用户，接着输入命令: $ sudo -u hdfs hdfs namenode -format 第一次使用必须格式化NameNode 启动HDFS for x in cd /etc/init.d ; ls hadoop-hdfs-* ; do sudo service $x start ; done 为了验证是否启动成功，可以在浏览器里输入地址：http:\\localhost:50070 进行查看,可以看到分布式文件系统的熔炼，数据节点个数，以及日志，在伪分布节点配置下，你只能够看到一个活动的节点localhost。 3.创建/tmp，Staging 以及Log的目录： $ sudo -u hdfs hadoop fs -mkdir -p /tmp/hadoop-yarn/staging/history/done_intermediate $ sudo -u hdfs hadoop fs -chown -R mapred:mapred /tmp/hadoop-yarn/staging $ sudo -u hdfs hadoop fs -chmod -R 1777 /tmp $ sudo -u hdfs hadoop fs -mkdir -p /var/log/hadoop-yarn $ sudo -u hdfs hadoop fs -chown yarn:mapred /var/log/hadoop-yarn 运行下面的指令，来查看文件是否建立: $ sudo -u hdfs hadoop fs -ls -R / 我们可以看到刚刚在HDFS上建立的目录结构： drwxrwxrwt - hdfs supergroup 0 2012-05-31 15:31 /tmp drwxr-xr-x - hdfs supergroup 0 2012-05-31 15:31 /tmp/hadoop-yarn drwxrwxrwt - mapred mapred 0 2012-05-31 15:31 /tmp/hadoop-yarn/staging drwxr-xr-x - mapred mapred 0 2012-05-31 15:31 /tmp/hadoop-yarn/staging/history drwxrwxrwt - mapred mapred 0 2012-05-31 15:31 /tmp/hadoop-yarn/staging/history/done_intermediate drwxr-xr-x - hdfs supergroup 0 2012-05-31 15:31 /var drwxr-xr-x - hdfs supergroup 0 2012-05-31 15:31 /var/log drwxr-xr-x - yarn mapred 0 2012-05-31 15:31 /var/log/hadoop-yarn 启动YARN(YARN是MapReduce的升级版) $ sudo service hadoop-yarn-resourcemanager start $ sudo service hadoop-yarn-nodemanager start $ sudo service hadoop-mapreduce-historyserver start 创建用户目录 为每个MapReduce 用户创建home目录，例如： $ sudo -u hdfs hadoop fs -mkdir -p /user/&lt;user&gt; $ sudo -u hdfs hadoop fs -chown &lt;user&gt; /user/&lt;user&gt; 这里我们的用户名是Cdh5，拿他可以替换掉&lt;User&gt;便可以。 至此，我们的环境配置便已经完成，下面我们跑个例子来检验一下。 跑一个YARN的例子 首先我们根据上面的例子创建一个Hadoop用户： $ sudo -u hdfs hadoop fs -mkdir -p /user/joe $ sudo -u hdfs hadoop fs -chown joe /user/joe 2.然后我们通过su joe切换到用户joe,创建input目录，并且将几个xml文件复制到该目录下： $ hadoop fs -mkdir input $ hadoop fs -put /etc/hadoop/conf/*.xml input $ hadoop fs -ls input Found 3 items: -rw-r--r-- 1 joe supergroup 1348 2012-02-13 12:21 input/core-site.xml -rw-r--r-- 1 joe supergroup 1913 2012-02-13 12:21 input/hdfs-site.xml -rw-r--r-- 1 joe supergroup 1001 2012-02-13 12:21 input/mapred-site.xml 设置用户joe的环境变量： $ export HADOOP_MAPRED_HOME=/usr/lib/hadoop-mapreduce 运行mapred的例子。这个例子是从input目录中查找dfs[a-z.]+这一正则表达式的匹配字段，命令如下： $ hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar grep input output23 'dfs[a-z.]+' 这之后便可以查看output23目录， $ hadoop fs -ls output23 Found 2 items drwxr-xr-x - joe supergroup 0 2009-02-25 10:33 /user/joe/output23/_SUCCESS -rw-r--r-- 1 joe supergroup 1068 2009-02-25 10:33 /user/joe/output23/part-r-00000 我们的运行结果就在part-r-00000文件里，我们可以查看： $ hadoop fs -cat output23/part-r-00000 | head 1 dfs.safemode.min.datanodes 1 dfs.safemode.extension 1 dfs.replication 1 dfs.permissions.enabled 1 dfs.namenode.name.dir 1 dfs.namenode.checkpoint.dir 1 dfs.datanode.data.dir 至此，我们的CDH 5运行环境便搭配完毕，我们可以在该环境下跑hadoop程序了。]]></content>
      <categories>
        <category>RecSys</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[用Scrapy爬网易新闻简易教程]]></title>
    <url>%2F2014%2F08%2F13%2FSpider%2F%E7%94%A8Scrapy%E7%88%AC%E7%BD%91%E6%98%93%E6%96%B0%E9%97%BB%E7%AE%80%E6%98%93%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[用Scrapy爬网易新闻简易教程 最近在研究爬虫相关的东东，经过上一篇文章的一系列调研比较后，收敛到scarpy这个Python的开源框架。它的好处和简单介绍可以参见上一篇文章。 Scrapy的安装和最简单的入门也可以参见官方文档tutorial,同时readthedocs上也挂有完整翻译的Scrapy中文文档,也有几篇中文的博客将该文档翻译了过来，这些都是非常好的入门材料。如果想完全弄懂爬虫的一些更加高阶的功能，比如分布式，缓存技术，并发等等问题，那么就必须好好研究下文档了。 这里我们直接举一个简单的用Scrapy来爬去网易科技新闻的例子。爬虫之前我们先应该把我们的大体思路理清楚，我们爬虫的对象是网易科技新闻。这个网页上会有很多信息，有导航，有专栏，有新闻，还有一些广告，我们的目的则很明确——爬新闻，细分一下主要可以包括新闻的标题，正文，来源，时间等等。同时为了简单起见，我们对于新闻里的图片，视频等等一系列多媒体元素也不做处理。同时我们还要考虑到如何把爬到内容用数据库存储起来。好，那我们开始吧。 创建项目 使用命令创建项目： 1scrapy startproject tech163 我们可以看到新建的项目目录结构如下： tech163/ ├── scrapy.cfg └── tech163 ├── __init__.py ├── items.py ├── pipelines.py ├── settings.py └── spiders └── __init__.py 其中scrapy.cfg定义的是项目的配置文件，一般里面的内容只是做了一个指向，将配置指向到了settings.py这个文件，这才是我们对项目进行配置的地方，包括对pipeline，download middleware，useragent。 定义要爬的内容 在scrapy框架中 要爬的内容都被定义在items.py文件中，定义也非常简单，代码如下： 12345678910111213141516171819# -*- coding: utf-8 -*-# Define here the models for your scraped items## See documentation in:# http://doc.scrapy.org/en/latest/topics/items.htmlimport scrapyclass Tech163Item(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() news_thread=scrapy.Field() news_title = scrapy.Field() news_url = scrapy.Field() news_time=scrapy.Field() news_from=scrapy.Field() from_url=scrapy.Field() news_body=scrapy.Field() 其中，news_thread是从每条新闻的url中提取特有的一个字符串，比如这条新闻,它的地址是：http://tech.163.com/14/0813/10/A3H72TD4000915BF.html， 那么它的thread就是：A3H72TD4000915BF，个人感觉它应该是随机产生的新闻的识别号，14和0813是年份份和日期，10是一个两位的数字。news_title，news_url，news_time，news_from，from_url，news_body这些定义的是之前提到新闻的属性。 开始写爬虫蜘蛛 定义完要爬的内容，我们再开始来写我们的爬虫蜘蛛——spider，我们在目录tech163/spiders/下，创建我们的spider文件叫做news_spider.py。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#encoding: utf-8import scrapyimport refrom scrapy.selector import Selectorfrom tech163.items import Tech163Itemfrom scrapy.contrib.linkextractors import LinkExtractorfrom scrapy.contrib.spiders import CrawlSpider,Ruleclass ExampleSpider(CrawlSpider): name = "news" allowed_domains = ["tech.163.com"] start_urls = ['http://tech.163.com/'] rules=( Rule(LinkExtractor(allow=r"/14/08\d+/\d+/*"), callback="parse_news",follow=True), ) def printcn(suni): for i in uni: print uni.encode('utf-8') def parse_news(self,response): item = Tech163Item() item['news_thread']=response.url.strip().split('/')[-1][:-5] # self.get_thread(response,item) self.get_title(response,item) self.get_source(response,item) self.get_url(response,item) self.get_news_from(response,item) self.get_from_url(response,item) self.get_text(response,item) #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!remenber to Retrun Item after parse return item def get_title(self,response,item): title=response.xpath("/html/head/title/text()").extract() if title: # print 'title:'+title[0][:-5].encode('utf-8') item['news_title']=title[0][:-5] def get_source(self,response,item): source=response.xpath("//div[@class='left']/text()").extract() if source: # print 'source'+source[0][:-5].encode('utf-8') item['news_time']=source[0][:-5] def get_news_from(self,response,item): news_from=response.xpath("//div[@class='left']/a/text()").extract() if news_from: # print 'from'+news_from[0].encode('utf-8') item['news_from']=news_from[0] def get_from_url(self,response,item): from_url=response.xpath("//div[@class='left']/a/@href").extract() if from_url: # print 'url'+from_url[0].encode('utf-8') item['from_url']=from_url[0] def get_text(self,response,item): news_body=response.xpath("//div[@id='endText']/p/text()").extract() if news_body: # for entry in news_body: # print entry.encode('utf-8') item['news_body']=news_body def get_url(self,response,item): news_url=response.url if news_url: #print news_url item['news_url']=news_url 代码非常简单,大部分都是对网页做解析，解析的地方，大部分是用xpath对内容进行提取，这部分还是得参考文档。这些解析只实现了对单张新闻网页的解析，那么是什么能够让它不停的爬取科技频道的新闻呢？scrapy的方便之处就是。这个机制只用一行代码就实现了 1234rules=(Rule(LinkExtractor(allow=r"/14/08\d+/\d+/*"),callback="parse_news",follow=True),) 这行代码，设置了整个爬虫的规则，通过LinkExtractor这个元件从response提取到所有的链接，再通过设置allow来设置需要再递归往下爬的新闻。这里使用正则表达式，将新闻的url做了约束。我们可以回顾下，正常的url的格式是这样的http://tech.163.com/14/0813/10/A3H72TD4000915BF.html ，代码中的正则/14/08\d+/\d+/*的含义是大概是爬去/14/08开头并且后面是数字/数字/任何格式/的新闻，可以说是14年8月份的新闻。通过这个正则我们便可以很好的对递归爬去做出筛选。follow=ture定义了是否再爬到的结果上继续往后爬。 定义数据的处理管道 scrapy通过在pipeline来对每次爬去到的items值进行处理，这里我们使用mongodb数据库来存储我们爬到的数据，新建的数据库为NewsDB。我们先在tech163目下单独使用文件，新建一个文件store.py来配置数据库存储，这里需要mongodb的依赖，以及pymongo依赖。 1234567#encoding: utf-8import pymongoimport randomHOST = "127.0.0.1"PORT = 27017client = pymongo.MongoClient(HOST, PORT)NewsDB = client.NewsDB 然后我们的pipeline文件这么写： 12345678910111213141516# -*- coding: utf-8 -*-# Define your item pipelines here## Don't forget to add your pipeline to the ITEM_PIPELINES setting# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html#encoding: utf-8from store import NewsDBclass Tech163Pipeline(object): def process_item(self, item, spider): if spider.name != "news": return item if item.get("news_thread", None) is None: return item spec = &#123; "news_thread": item["news_thread"] &#125; NewsDB.news.update(spec, &#123;'$set': dict(item)&#125;, upsert=True) return None 最后我们在setting.py 文件中做一些设置，主要是定义pipeline： 1234567891011121314151617181920212223242526# -*- coding: utf-8 -*-# Scrapy settings for tech163 project## For simplicity, this file contains only the most important settings by# default. All the other settings are documented here:## http://doc.scrapy.org/en/latest/topics/settings.html#BOT_NAME = 'tech163'SPIDER_MODULES = ['tech163.spiders']NEWSPIDER_MODULE = 'tech163.spiders'ITEM_PIPELINES = ['tech163.pipelines.Tech163Pipeline', ]# Crawl responsibly by identifying yourself (and your website) on the user-agent#USER_AGENT = 'tech163 (+http://www.yourdomain.com)'USER_AGENT = 'Mozilla/5.0 (X11; Linux x86_64; rv:7.0.1) Gecko/20100101 Firefox/7.7'DOWNLOAD_TIMEOUT = 15# DOWNLOAD_DELAY = 0.1# LOG_LEVEL = "INFO"# LOG_STDOUT = True# LOG_FILE = "log/newsSpider.log" 这样我们就完成了整个项目的配置，我们便可以在命令行中敲入： scrapy crawl news 来开始爬去新闻。整个项目的源代码传到了github上供大家参考，完成这个以后可以学习图片爬取，分布式处理，以及缓存技术等等问题。]]></content>
      <categories>
        <category>RecSys</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Data Structure--Binary search trees]]></title>
    <url>%2F2014%2F08%2F12%2FData%20structure%20and%20Algorithms%2FData%20Structure--Binary%20search%20trees%2F</url>
    <content type="text"><![CDATA[Data Structure--Binary search trees Definition. A BST is a binary tree in symmetric order A binary tree is either: ・Empty. ・Two disjoint binary trees (left and right). Symmetric order. Each node has a key, and every node’s key is: ・Larger than all keys in its left subtree. ・Smaller than all keys in its right subtree. Java definition A BST is a reference to a root Node. A Node is comprised of four fields: A key and A vaule. A reference to the left and right subtrees 1234567891011public class Node&#123; private Key key; private Vaule val； private Node left,right; private int count;//for size() private Node(Key key,Vaule val)&#123; this.key=key; this.vaule=val; &#125;&#125; BST implementation(Skeleton) 123456789101112public class BST&lt;Key extends Comparable&lt;Key&gt;,Vaule&gt;&#123; private Node root; private class Node &#123;/* see above*/&#125; public Vaule get(Key,key) &#123;/*see next*/ &#125; public void put(Key key,Vaule val) &#123;/*see next*/ &#125; public void delete(Key key)&#123;&#125; public Iterable&lt;Key&gt; iterator()&#123;&#125;&#125; Get. Return Vaule corresponding to given key, or null if no such key. 12345678910111213141516public Vaule get(Key key)&#123; Node cur=root; int cmp; while(cur!=null)&#123; cmp=cur.key.CompareTo(Key)); if(cmp&gt;0) cur=cur.left; if (cmp&lt;0) &#123; cur=cur.right; &#125; else return cur.Vaule; &#125; return null;&#125; Put .Associate value with key.(Tricky and recursive) Search for key, then two cases: ・Key in tree ⇒ reset value. ・Key not in tree ⇒ add new node 1234567891011121314151617181920public void put(Key key,Vaule val)&#123; root=put(root,key,val);&#125;private Node put(Node node,Key key,Vaule val)&#123; if (node==null) &#123; //如果没有节点，则新建节点，如果原来不含该节点，最终都到这步 return new Node(key,val); &#125; else&#123; cmp=key.CompareTo(node.key); if (cmp&gt;0) node.right=put(node.right,key,val); else if (cmp &lt; 0) node.left=put(node.left,key,value); else node.value=val; &#125; return node;&#125; Ordered Operations 如何求最大/最小值 ？ 很简单，我们可以根据二叉查找树的定义，判定最左的子节点即为最小值，最右的为最大值。 如何写Floor和Ceiling 函数？(Floor. Largest key ≤ a given key.Ceiling. Smallest key ≥ a given key.) Q. How to find the floor / ceiling? Case 1. [k equals the key at root] The floor of k is k. Case 2. [k is less than the key at root] The floor of k is in the left subtree. Case 3. [k is greater than the key at root] The floor of k is in the right subtree (if there is any key ≤ k in right subtree); otherwise it is the key in the root. 12345678910111213141516171819public Key floor(Key k)&#123; Node X=floor(root,k); if (root == null) return null; return X.Key;&#125;private Node floor(Node X,Key k)&#123; Node Y; if(X=null) return null; int cmp=k.compareTo(X.key); if(cmp&lt;0) retrun Y=floor(X.left,k); else if(cmp==0) return X; else &#123; if (floor(X.right,k)!=null) return floor(X.right,k); else return X; &#125;&#125; 求数的大小： 方法：在每个节点存储字数节点个数的值，使用size函数，使得它返回该节点的count值，这样的话，节点必须新加一个变量count，size函数这么写： 1234567public class Node&#123; private Key key; private Value val; private Node left; private Node right; private int count;&#125; 1234567public int size()&#123; return size(root);&#125;private int size(Node X)&#123; if(X==null) return null; return X.count;&#125; 这样put函数里应该多加这么一句： 12345678910111213141516171819202122232425262728293031323334353637 public void put(Key key,Vaule val)&#123; root=put(root,key,val); &#125; private Node put(Node node,Key key,Vaule val)&#123; if (node==null) &#123; //如果没有节点，则新建节点，如果原来不含该节点，最终都到这步 return new Node(key,val); &#125; else&#123; cmp=key.CompareTo(node.key); if (cmp&gt;0) node.right=put(node.right,key,val); else if (cmp &lt; 0) node.left=put(node.left,key,value); else node.value=val; &#125; //return 前添加一句count值 node.count=1+size(node.left)+size(node.right); return node; &#125;java 4. 排序统计问题（Rank） Q. How many keys &lt; k ? ```java public int rank(Key k)&#123; int num=rank(root,k); return num; &#125; private int rank(Node X,Key k)&#123; if (X==null) return 0; int cmp =k.compareTo(X.key); if (cmp==0) return size(X.left)//注意，size函数包括自己 else if (cmp&lt;0) return rank(X.left,k); else return 1+rank(X.left,k)+rank(X.right,k); &#125; 迭代写法 方法，用队列实现，先存左边的node，再存root及节点，再存右边的节点。 1234567891011public Iterable&lt;Key&gt;Keys()&#123; Queue&lt;Key&gt; q = new Queue&lt;Key&gt;(); inorder(root,q); return q;&#125;private void inorder(Node X,Queue&lt;Key&gt;,q)&#123; if(X==null) return; inorder(X.left,q); enquue(X.Key); inorder(X.right,q);&#125; 算法比较 可见二叉查找树和二分查找的比较，二叉查找树算法复杂度与树的高度成正比，而二叉查找树树的高度又和二叉查找树的动态存储的数据结构，在插入方面更有优势（红线划到insert处）。]]></content>
      <categories>
        <category>Data Struture and algorithm</category>
      </categories>
      <tags>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫 工具——Scrapy 介绍]]></title>
    <url>%2F2014%2F08%2F07%2FSpider%2FSpider%2F</url>
    <content type="text"><![CDATA[基于Python的crawler 考虑到垂直爬虫及站内搜索的重要性，重新思考一下项目爬虫的技术架构及实现方案 考察垂直爬虫的几个原则： 性能较高：较好支持多线程并发处理；支持异步、非阻塞socket；支持分布式爬取；爬取调度算法性能较高；内存使用效率较高，不要老是出现out of memory问题； 架构优美：组件式设计式架构，扩展方便；架构设计精巧。至少值得花时间去学习架构设计思想。 扩展方便：能够与现有框架较好集成；由于是垂直爬虫，需要针对不同的网页定制爬取规则集逻辑，需要能够方便测试，不要老是重新编译，因此最好支持python等脚本语言 功能全面：内置支持ajax/javascript爬取、登录认证、深度爬取设置、类似heritrix的爬取过滤器（filter）、页面压缩处理等 管理功能：提供爬虫管理接口，能够实时监控和管理爬取 厌烦了基于java的爬虫方案，尤其是考虑到python在网络编程上的易用性，因此打算考察基于python做新版本爬虫的可行性，刚好把久不使用的python捡起来。 整理了一下目前基于python的crawler，大致有如下一些现成的项目方案可供参考： Mechanize：http://wwwsearch.sourceforge.net/mechanize/ Twill：http://twill.idyll.org/ Scrapy：http://scrapy.org HarvestMan：http://www.harvestmanontheweb.com/ Ruya：http://ruya.sourceforge.net/ psilib：http://pypi.python.org/pypi/spider.py/0.5 BeautifulSoup + urllib2：http://www.crummy.com/software/BeautifulSoup/ 比较之后，选择Scrapy作为重点考察学习对象，尽管没有Mechanize及Harvestman成熟，但从其架构来看，还是很有前途的，尤其是基于twisted高性能框架的架构，很有吸引力。 scrapy的架构图： scrapy 包含的component如下： Components Scrapy Engine The engine is responsible for controlling the data flow between all components of the system, and triggering events when certain actions occur. See the Data Flow section below for more details. Scheduler The Scheduler receives requests from the engine and enqueues them for feeding them later (also to the engine) when the engine requests them. Downloader The Downloader is responsible for fetching web pages and feeding them to the engine which, in turns, feeds them to the spiders. Spiders Spiders are custom classes written by Scrapy users to parse response and extract items (aka scraped items) from them or additional URLs (requests) to follow. Each spider is able to handle a specific domain (or group of domains). For more information see Spiders. Item Pipeline The Item Pipeline is responsible for processing the items once they have been extracted (or scraped) by the spiders. Typical tasks include cleansing, validation and persistence (like storing the item in a database). For more information see Item Pipeline. Downloader middlewares Downloader middlewares are specific hooks that sit between the Engine and the Downloader and process requests when they pass from the Engine to the downloader, and responses that pass from Downloader to the Engine. They provide a convenient mechanism for extending Scrapy functionality by plugging custom code. For more information see Downloader Middleware. Spider middlewares Spider middlewares are specific hooks that sit between the Engine and the Spiders and are able to process spider input (responses) and output (items and requests). They provide a convenient mechanism for extending Scrapy functionality by plugging custom code. For more information see Spider Middleware. Scheduler middlewares Spider middlewares are specific hooks that sit between the Engine and the Scheduler and process requests when they pass from the Engine to the Scheduler and vice-versa. They provide a convenient mechanism for extending Scrapy functionality by plugging custom code. Frequently Asked Questions How does Scrapy compare to BeautifulSoup or lxml? BeautifulSoup and lxml are libraries for parsing HTML and XML. Scrapy is an application framework for writing web spiders that crawl web sites and extract data from them. Scrapy provides a built-in mechanism for extracting data (called selectors) but you can easily use BeautifulSoup (or lxml) instead, if you feel more comfortable working with them. After all, they’re just parsing libraries which can be imported and used from any Python code. In other words, comparing BeautifulSoup (or lxml) to Scrapy is like comparing jinja2 to Django. How is BeautifulSoup different from Scrapy?[3] Scrapy is a Web-spider or web scraper framework, You give Scrapy a root URL to start crawling, then you can specify constraints on how many number of Urls you want to crawl and fetch,etc., It is a complete framework for Web-scrapping or crawling. While Beautiful Soup is a parsing library which also does pretty job of fetching contents from Url and allows you to parse certain parts of them without any hassle. It only fetches the contents of the URL that you give and stops. It does not crawl unless you manually put it inside a infinite loop with certain criteria. In simple words, with Beautiful Soup you can build something similar to Scrapy. Beautiful Soup is a library while Scrapy is a complete framework. References [1]基于python的crwaler [2]doc.scarpy.org [3]How-is-BeautifulSoup-different-from-Scrapy]]></content>
      <categories>
        <category>RecSys</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Mahout source code modification and re-compile]]></title>
    <url>%2F2014%2F08%2F05%2FRecSys%2FMahout%20source%20code%20modification%20and%20re-compile%2F</url>
    <content type="text"><![CDATA[Mahout source code modification and re-compile author: sheng junhui weibo: @armysheng blog: youngfor.me email: armysheng@gmail.com 1.unzip your downloaded mahout src file mahout-distribution-0.9-src, import this maven project to eclipse; 2.new a test class at project mahout-core with package name org.apache.mahout.test,name modifytest.java; 3.implement this modifytest class with code as follows: package org.apache.mahout.test; public class modifytest { public static void hello(){ System.out.println(&quot;modifytest success !&quot;); } } 4.re-compile the project by right click the pom.xml of mahout-core project, choose Run as + Maven build and fill the goal option with -DskipTests clean install, then the compile starts and it will take few minutes. After successful compliation you should see: [INFO] Building jar: D:\workspace\Mahout\mahout-distribution-0.9\core\target\mahout-core-0.9-sources.jar [INFO] [INFO] --- maven-install-plugin:2.5.1:install (default-install) @ mahout-core --- [INFO] Installing D:\workspace\Mahout\mahout-distribution-0.9\core\target\mahout-core-0.9.jar to C:\Users\shengjh\.m2\repository\org\apache\mahout\mahout-core\0.9\mahout-core-0.9.jar [INFO] Installing D:\workspace\Mahout\mahout-distribution-0.9\core\pom.xml to C:\Users\shengjh\.m2\repository\org\apache\mahout\mahout-core\0.9\mahout-core-0.9.pom [INFO] Installing D:\workspace\Mahout\mahout-distribution-0.9\core\target\mahout-core-0.9-tests.jar to C:\Users\shengjh\.m2\repository\org\apache\mahout\mahout-core\0.9\mahout-core-0.9-tests.jar [INFO] Installing D:\workspace\Mahout\mahout-distribution-0.9\core\target\mahout-core-0.9-job.jar to C:\Users\shengjh\.m2\repository\org\apache\mahout\mahout-core\0.9\mahout-core-0.9-job.jar [INFO] Installing D:\workspace\Mahout\mahout-distribution-0.9\core\target\mahout-core-0.9-sources.jar to C:\Users\shengjh\.m2\repository\org\apache\mahout\mahout-core\0.9\mahout-core-0.9-sources.jar [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 45.205 s [INFO] Finished at: 2014-06-11T13:57:27+08:00 [INFO] Final Memory: 25M/124M [INFO] ------------------------------------------------------------------------ After this step, we have added a new class in mahout source code at mahout-core. Now let's see if it works. 5.build a new simple maven project with eclipse named modifytest or others, add the mahout dependency into the pom.xml file. &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.mahout&lt;/groupId&gt; &lt;artifactId&gt;mahout-core&lt;/artifactId&gt; &lt;version&gt;0.9&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; add a new class named test.java to this project and implement with code: package com.docomo.muc.mahoutlearning; import org.apache.mahout.test.modifytest; public class test { public static void main(String[] args){ modifytest tt= new modifytest(); tt.hello(); } } Hit crtl+F11 to run this project, and we can see the print output: modifytest success !]]></content>
      <categories>
        <category>RecSys</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Little trick--Sort two ordered array]]></title>
    <url>%2F2014%2F08%2F01%2FData%20structure%20and%20Algorithms%2FLittle%20trick--Sort%20two%20ordered%20array%2F</url>
    <content type="text"><![CDATA[Problem: Given two sorted integer arrays A and B, merge B into A as one sorted array. Note: You may assume that A has enough space to hold additional elements from B. The number of elements initialized in A and B are m and n respectively. 对于两个排好序的数组，如何进行合并呢？ 主要思想是定义两个指针，数组从尾到头比较，直到有一个数组到头，如果B到头，则返回，如果A到头，继续将B内的元素放到A的前面部分 1234567891011121314151617181920212223242526public class solution &#123; public static void merge(int A[], int m, int B[], int n) &#123; int i=m-1; int j=n-1; int newlen=m+n-1; while(i&gt;=0&amp;&amp;j&gt;=0)&#123; //loop until one array get to the head if(A[i]&lt;B[j]) A[newlen--]=B[j--]; else A[newlen--]=A[i--]; &#125; while(j&gt;=0) //if A reaches its head, copy the remained entrys of B to newA; A[newlen--]=B[j--]; //if B reaches its head, there' s no need to copy the remained entrys of A to newA; &#125; public static void main(String[] args) &#123; int[] A =new int[10]; A[0]=7; A[1]=9; A[2]=10; int[] B = &#123;2,5,6&#125;; merge(A,3,B,3); for(int entry :A) System.out.print(entry+" "); &#125;&#125;]]></content>
      <categories>
        <category>Data Struture and algorithm</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mahout推荐系统源码分析]]></title>
    <url>%2F2014%2F08%2F01%2FRecSys%2FMahout%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Mahout推荐系统源码分析 author: sheng junhui weibo: @armysheng blog: youngfor.me email: armysheng@gmail.com Mahout下个性化推荐引擎Taste介绍 以下讨论基于Mahout 版本 version0.9 本文关心的是如何利用Mahout在非分布式，非Hadoop based 情况下（即单机模式下）建立一个协同滤波的推荐系统。 Taste曾经是一个独立的项目，现在在Mahout和其他基于Hadoop的项目中发展。相对与目前的研究重点*--如何将推荐算法在Hadoop等分布式计算上实现--*Taste项目可以是一个更加独立，更加综合，更加稳定的项目。也被认为是学习和理解Mahout推荐引擎的基本入门点。 基于系协同滤波的Mahout推荐引擎可以通过用户对某些物品的喜好的计算，来得到用户对另一些物品的喜好。比如说一个卖书或者CD的网站，可以个根据用户以往的购买经历和喜好等数据，轻松的利用Mahout来得到用户对其他的那些书或者CD感兴趣。 Mahout提供了一组丰富的组件,您可以选择特定的算法构建一个定制的推荐系统。并且Mahout设计时考虑到了企业级的应用，高性能、可伸缩性和灵活性。 在构建推荐系统是，Mahout的顶层的packages中用到的接口有这些： DataModel UserSimilarity ItemSimilarity UserNeighborhood Recommender 在org.apache.mahout.cf.taste.impl这个包中有对这些接口的实现。所有这些包就是你在搭建自己的推荐系统时需要用到的。 架构图： 改图显示了在一个Userbased 推荐器中，各个Mahout组件之间的关系。Item-base的推荐系统也与之类似，只是不包含Neighborhood算法。 标准化的开发过程 以UserCF的推荐算法为例，官方建议我们的开发过程： 图片摘自Mahout in Action 从上图中我们可以看到，算法是被模块化的，通过1,2,3,4的过程进行方法调用。我们可以使用如下的简单代码来实现一个User-based CF算法。 123456789101112131415161718192021public class SampleRecommender &#123; public static void main(String[] args) throws IOException, Exception&#123; //读取数据，利用DataModel类建立数学模型 DataModel model = new FileDataModel (new File("dataset.csv")); //利用PearsonCorrelationSimilarity()函数建立用户的相似度 UserSimilarity similarity = new PearsonCorrelationSimilarity(model); //利用ThresholdUserNeighborhood()函数得到和用户最相似的其他用户 UserNeighborhood neighborhood = new ThresholdUserNeighborhood(0.1, similarity, model); //利用GenericUserBasedRecommender()函数构造推荐器recommeder,至此推荐器构造完成 UserBasedRecommender recommender = new GenericUserBasedRecommender(model, neighborhood, similarity); //检验构造器与显示结果，222和5参数代表推荐给UID为222的用户前5个偏好度最大的商品 List&lt;RecommendedItem&gt; recommendations = recommender.recommend(222,5); for (RecommendedItem recommendation : recommendations) &#123; System.out.println(recommendation); &#125; &#125; &#125; 我们用到的4个类有DataModel, UserSimilarity, NearestNUserNeighborhood, Recommender。下面我们来逐个介绍一下这些类。 DataModel DataModel是关于用户偏好信息的一个借口。实现这个接口的数据可以是任何来源（单独的条目，文件，数据库等等），但是大多数情况下数据都来源于数据库。使用时应当与ReloadFromJDBCDataModel类结合来获得更好的性能。举例来说，Mahout提供了MySQLJDBCDataModel来获得JDBC和MySQL数据库中的用户偏好信息。同时还有对PostgreSQL的支持。Mahout 同样提供了一个FileDataModel类，可以非常方便的应用于规模较小的项目中。 在该模型中，用户和物品都是用一个唯一的ID号来辨识，而且这个ID还必须是在Java中long类型的数字。而其中的Preference或者PreferenceArray对象者封装了用户和物品直接的关系（即物品，和用户对物品的喜爱度）。 最后，Mahout还有对布尔量数据模型的支持，该模型用户没有对某一物品进行特定的评分，而只是单纯的判断他们之间有没有建立某种联系。举例来说，某用户在一些电影推荐的网站上，可能可以对电影进行1~5星的评分。但是对于某个网站(比如说亚马逊)的推荐页，用户可能没有办法对这个推荐页的好坏进行评分，但是我们可以通过或缺用户是否访问了推荐的产品，即用户与推荐页是否建立了联系来建立DataModel。 Mahout0.9版本中Datamodel的类图（缩放页面查看大图）如下： UserSimilarity&amp;&amp;ItemSimilarity UserSimilarity定义了两个用户之间的相似度。只是推荐引擎的关键部分，也是目前推荐系统研究性能提高的关键点。这也是Neighborhood类寻找近邻的基础。Neighborhood也类似，是用来寻找物品之间的相似度。 Mahout0.9版本中相关的类图（缩放页面查看大图）如下： UserNeighborhood 在一个user-based 推荐系统中，相关的推荐是通过找到某一用户的相似用户，即“近邻”。 一个UserNeighborhood通过某种特定的手段得到一些邻近的用户，比如说最近的10用户、相似度大于0.8的用户。 通常UserNeighborhood的实现需要UserSimilarity作为入口。 Mahout0.9版本中相关的类图（缩放页面查看大图）如下： Recommender Recommender是Mahout核心的抽象类。对于一个DataModel,该类能够构造一个推荐器。项目中我们大部分情况下会用到 GenericUserBasedRecommender or GenericItemBasedRecommender来产生userbased以及Itembased 协同滤波推荐器，同时可能结合了 CachingRecommender。 Mahout0.9版本中相关的类图（缩放页面查看大图）如下： 结尾 至此我们就介绍完了构造一个推荐系统所需要的一些类和接口，我们可以通过分析它更底层的代码，来进一步了解推荐系统实现的细节。]]></content>
      <categories>
        <category>RecSys</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Data Structure--Priority Queue]]></title>
    <url>%2F2014%2F07%2F18%2FData%20structure%20and%20Algorithms%2FData%20Structure--Priority%20Queue%2F</url>
    <content type="text"><![CDATA[Data Structure--Priority Queue Collection : Insert and delete items. But Which item to delete? Stack. Remove the item most recently added Queue. Remove the imte least recently added Randomized queue. Remove a random item Priority queue. Remove the largest (or smallest) item. APIs: public class MaxPQ&lt;Key extends Comparable &lt;Key&gt;&gt; Challenge. Find the lagest M items in a stream of N items 优先队列的无序数组实现形式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169 import static java.lang.System.out; import java.util.NoSuchElementException; /** * Implements a &lt;i&gt;Priority Queue&lt;/i&gt; unordered that iterates over the largest key. * * &lt;h5&gt;Lecture: APIs and Elementary Implementations (Week 4)&lt;/h5&gt; * * &lt;p&gt; * Keep the entries unordered in an resizing array, when method &lt;code&gt;delMax&lt;code&gt; * is called it remove and return the largest key. * &lt;/p&gt; * * @see MaxPQ.java * @author eder.magalhaes * @param &lt;Key&gt; parameterized type for key. */ public class UnorderedMaxPQ&lt;Key extends Comparable&lt;Key&gt;&gt; &#123; private Key[] pq; private int N; public UnorderedMaxPQ() &#123; this(1); &#125; @SuppressWarnings("unchecked") public UnorderedMaxPQ(int capacity) &#123; pq = (Key[]) new Comparable[capacity]; &#125; public boolean isEmpty() &#123; return N == 0; &#125; public void insert(Key k) &#123; if (N == pq.length - 1) //resize if outbound resize(2 * pq.length); pq[N++] = k; &#125; //linear (N) running time public Key delMax() &#123; if (isEmpty()) throw new NoSuchElementException(); int max = 0; for (int i = 1; i &lt; N; i++) if (less(max, i)) max = i; //遍历取Max exch(max, N-1); //把最大值放在队尾，并返回 Key k = pq[--N]; pq[N] = null; if ((N &gt; 0) &amp;&amp; (N == (pq.length - 1) / 4)) //小于队列4分之一是进行resize resize(pq.length / 2); return k; &#125; private boolean less(int i, int j) &#123; return pq[i].compareTo(pq[j]) &lt; 0; &#125; private void exch(int i, int j) &#123; Key swap = pq[i]; pq[i] = pq[j]; pq[j] = swap; &#125; private void resize(int capacity) &#123; @SuppressWarnings("unchecked") Key[] copy = (Key[]) new Comparable[capacity]; for (int i = 0; i &lt;= N; i++) copy[i] = pq[i]; pq = copy; &#125; public static void main(String[] args) &#123; UnorderedMaxPQ&lt;String&gt; queue = new UnorderedMaxPQ&lt;String&gt;(15); queue.insert("P"); queue.insert("R"); queue.insert("I"); queue.insert("O"); queue.insert("R"); queue.insert("I"); queue.insert("T"); queue.insert("Y"); queue.insert("Q"); queue.insert("U"); queue.insert("E"); queue.insert("U"); queue.insert("E"); while (!queue.isEmpty()) &#123; out.printf("%s ", queue.delMax()); &#125; &#125; &#125;``` 优先队列数组的实现形式的算法复杂度是这样的： ![Image Title](/_image/2014-07-19/4.png) 无序数组的insert 和delmax的复杂度分别为1和N ，有没有可能实现logN的复杂度呢？有，那就是binary Heap！ ##binary Heap 1. Binary heap . Array representation of a heap-ordered complete binary tree.堆序排列的完全二叉树 * 所谓堆序排列的完整二叉树： * Keys in nodes * 父节点不比子节点小 * 所谓数组呈现： *从1开始索引 *水平顺序上实现节点 *不需要显示的链表 * 几点假设：Key a[1] 是最大的元素，也是二叉树的根节点 可以使用数组下标来遍历二叉树 对于节点K的父节点为K/2 对于节点K的子节点为2K和2K+1* 场景1：子节点出现比父节点要大的情况* 改变方法： 子节点和父节点交换位置，在和该父节点的父节点比较，直到到根节点```java private void swim(int k)&#123; while (k&gt;1&amp;&amp;less(k/2,k))&#123; exch(k/2,k); k=k/2; &#125; &#125;``` * 场景2：插入一个数 * 方法：在数组尾部加一个数，然后swim up,比较次书为1+lgN```java private void insert(int k)&#123; pq[N++]=k; swim(N); &#125; ``` * 场景3：父节点比自己点要小 * 方法： 交换父节点与子节点中较大的一个，迭代直到队尾或者不出现这种现象为止 ```java private void sink(int k)&#123; while (2*k&lt;N)&#123; //注意越界条件 int j=2*k; //j =(less(j,j+1)?j+1:j);//litter trick 与大的值换 if(j&lt;N&amp;&amp;less(j,j+1)) j++; if (!less(k,j)) break; exch(k,j); k=j; &#125; &#125;``` * 场景4：删除最大节点，即父节点 * 方法：父节点与最末节点换位置，然后吧最末节点从最高层sink下来```java public Key delMax()&#123; Key max = pq[1]; exch(1,N--); sink(1); pq[N+1]=null; return Max; &#125; 最后我们可以写出整个PQ类的程序： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127import static java.lang.System.out;import java.util.NoSuchElementException;/** * Implements a &lt;i&gt;Priority Queue&lt;/i&gt; ordered that iterates over the largest key. * * &lt;h5&gt;Lecture: APIs and Elementary Implementations (Week 4)&lt;/h5&gt; * * &lt;p&gt; * Keep the entries ordered in an resizing array. * &lt;/p&gt; * * &lt;p&gt;This colecttion uses &lt;i&gt;Binary heap&lt;/i&gt; algorithm.&lt;/p&gt; * * @see UnorderedMaxPQ.java * @author eder.magalhaes * @param &lt;Key&gt; parameterized type for key. */public class MaxPQ&lt;Key extends Comparable&lt;Key&gt;&gt; &#123; private Key[] pq; private int N; public MaxPQ() &#123; this(1); &#125; @SuppressWarnings("unchecked") public MaxPQ(int capacity) &#123; pq = (Key[]) new Comparable[capacity+1]; &#125; public boolean isEmpty() &#123; return N == 0; &#125; public void insert(Key x) &#123; if (N == pq.length - 1) resize(2 * pq.length); pq[++N] = x; swim(N); &#125; public Key delMax() &#123; if (isEmpty()) throw new NoSuchElementException(); Key max = pq[1]; exch(1, N--); sink(1); pq[N+1] = null; if ((N &gt; 0) &amp;&amp; (N == (pq.length - 1) / 4)) resize(pq.length / 2); return max; &#125; //node promoted to level of incompetence (Binary heap); private void swim(int k) &#123; while (k &gt; 1 &amp;&amp; less(k / 2, k)) &#123; exch(k, k / 2); k = k / 2; &#125; &#125; //better subordinate (child) promoted (Binary heap); private void sink(int k) &#123; while (2 * k &lt;= N) &#123; int j = 2 * k; if (j &lt; N &amp;&amp; less(j, j+ 1)) j++; if (!less(k, j)) break; exch(k, j); k = j; &#125; &#125; private boolean less(int i, int j) &#123; return pq[i].compareTo(pq[j]) &lt; 0; &#125; private void exch(int i, int j) &#123; Key swap = pq[i]; pq[i] = pq[j]; pq[j] = swap; &#125; private void resize(int capacity) &#123; @SuppressWarnings("unchecked") Key[] copy = (Key[]) new Comparable[capacity]; for (int i = 1; i &lt;= N; i++) copy[i] = pq[i]; pq = copy; &#125; public static void main(String[] args) &#123; MaxPQ&lt;String&gt; queue = new MaxPQ&lt;String&gt;(); queue.insert("P"); queue.insert("R"); queue.insert("I"); queue.insert("O"); queue.insert("R"); queue.insert("I"); queue.insert("T"); queue.insert("Y"); queue.insert("Q"); queue.insert("U"); queue.insert("E"); queue.insert("U"); queue.insert("E"); while (!queue.isEmpty()) &#123; out.printf("%s ", queue.delMax()); &#125; &#125;&#125; Heapsort 最后介绍基于binary heap的heapsort： basic plan： 先把二叉树变成heapsort，即父节点大于子节点 repeatly delete the max key 代码入下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import static java.lang.System.out;/** * Implements a &lt;i&gt;Heapsort&lt;/i&gt;. * * &lt;h5&gt;Lecture: Heapsort (Week 4)&lt;/h5&gt; * * &lt;p&gt; * Worst case this algorithms works in 2 N lg N running time. * Best case works in N lg N running time. * &lt;/p&gt; * * &lt;p&gt; * Faster than Selection, Insertion, Shell, Quick and 3-way quick. * But slow than Mergesort. * &lt;/p&gt; * * @author eder.magalhaes */public class Heap &#123; private static void sink (Comparable[] pq, int k, int N) &#123; while (2 * k &lt;= N) &#123; int j = 2 * k; if (j &lt; N &amp;&amp; less(pq, j, j+1)) j++; if (!less(pq, k, j)) break; exch(pq, k, j); k = j; &#125; &#125; private static void exch(Object[] pq, int i, int j) &#123; Object swap = pq[i-1]; pq[i-1] = pq[j-1]; pq[j-1] = swap; &#125; private static boolean less(Comparable[] pq, int i, int j) &#123; return pq[i-1].compareTo(pq[j-1]) &lt; 0; &#125; public static void sort(Comparable[] pq) &#123; int N = pq.length; for (int k = N/2; k &gt;= 1; k--) //此处从数组最后一个父节点，即最后一个节点 // 的父节点开始，遍历节点，使得整个数组为heap-sorted sink(pq, k, N); while (N &gt; 1) &#123; exch(pq, 1, N--);//把最大的放到末尾，把N-1的元素放到树顶去sink sink(pq, 1, N);//重复做 &#125; &#125; public static void main(String[] args) &#123; Integer[] data = new Integer[] &#123;95, 29, 57, 82, 41, 83, 52, 21, 94, 79&#125;; sort(data); out.print("Array sorted by Heapsort: "); for (Integer x: data) &#123; out.printf("%s ",x); &#125; &#125;&#125; 复杂度比较 Significance. In-place sorting algorithm with** N log N worst-case.** Mergesort: no, linear extra space. Quicksort : no, quadratic time in worst case. Heapsort: yes! Bottom line. Heapsort is optimal for both time and space, but: Inner loop longer than quicksort’s. Makes poor use of cache memory. Not stable. summary]]></content>
      <categories>
        <category>Data Struture and algorithm</category>
      </categories>
      <tags>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里大数据竞赛season1 总结]]></title>
    <url>%2F2014%2F07%2F12%2Falibaba%2F%E9%98%BF%E9%87%8C%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9Bseason1%20%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[关于样本测试集和训练集数量上，一般是选择训练集数量不小于测试集，也就是说训练集选取6k可能还不够，大家可以多尝试得到更好的效果； 2. 有人提出归一化方面可能有问题，大家可以查查其他的归一化方法，但是归一化环境是不可少的； 3. 将部分代码传到了**github** 4. 听说阿里又改赛制了，哈哈。 最近好累啊，简单总结一下吧。 碎碎念 这个比赛自己真的是花时间花精力去做了，虽然在s1止步，但是可以说对自己的数据分析入门算是蛮有意义的。收获的东西也蛮多，学了下python，真是一门灵活的语言（感谢o神的入门指南和规范易懂的代码，回头自己也得整理下代码，放到github上，其实之前在github上搜过，目测有10个左右的repository，==）；试着学习用了下git，每天提交的版本，不好好管理真的是会分不清最好成绩是那一版，git果然也是码农的神器；还接触了正则表达式（皮毛之皮毛），熟悉了sublime这款精美的编辑器，配上python简直高大上+爱不释手。算法方面，一开始面对这个题目，直接用了经验参数，8号的时候就到了6.8%，排名也蛮靠前，于是乎安安然，一直没去搞LR，cf等等算法，后来发现小伙伴们简直凶残，baseline 蹭蹭涨，排名蹭蹭往下掉。所以立马开始考虑转到LR，虽然之前也搞过一些分类问题，但是回头来看当时做的时候理解还是不够深的，这次索性从线性回归开始重新看了一遍Ng的视频，又加深了理解（但是还是喜欢直接调用，自己编的话太痛苦了吧），建立了逻辑回归模型，最后用纯LR线上到了6.5%吧，感觉还有提高的空间，但是调试的次数太少了。 关于比赛 大赛的赛题和数据在**这里** 关于比赛入门什么的就不应该提了，毕竟能进入s2的大牛应该占大多数。但是目测s2应该还有一些规则用户。我稍微结合自己对模式识别的理解，讲讲一些思路吧，希望能对小白有一点帮助，大牛可以无视了。 整个问题其实可以抽象成一个模式识别问题，对于任意的模式识别系统都可以套用如下的几个步骤： 数据采集-&gt;预处理-&gt;特征提取-&gt;特征选择-&gt;分类器选择-&gt;分类器评价-&gt;再改进分类器 数据采集 这一步当然就不必说，阿里已经给了我们所有的数据 预处理 o神的第一发**指南已经给出了明确的说明，如何数据处理成uid，bid，action_type,date** 其中date是离起始日4月15日的时差。然而模式识别中广义的预处理，除了将原始数据转换成可用的格式外，还包括数据清理，数据集成变换等等。比如说对于从未买过东西的Uid，从来没被买过的Bid直接删除（这对后面要说的正负样本采样很有意义），以及一些数据平滑，比如同一天点击数超过15就平滑为15，超过10次的购买平滑为10次。当然还有比较专业的平滑算法，如移动平滑，指数平滑。 特征提取 特征提取一般是整个系统的核心部分，好的特征是整个判别的关键，在图像处理领域，特征的提取一直是研究的热门。特征的提取一般是依靠个人的经验，这个例子中原始的点击数，购买数，搜藏数，以及加入购物车数，时间，很明显可以作为特征，但是单用这几个特征效果是不大好的。大家讨论的品牌热度，用户购买力,访问天数等等，都应该是不错的特征。这个阶段就需要靠你自己的购物体验，去提取出最好的特征。这样我们就可以得到一个如下的特征矩阵： ---uid bid F1....Fn Label---- 12000 911 5 ... 0.8 0 同时，为了使各个特征的本身属性对分类的影响，比如说点击数一般都是一个很大的数，而某某率则是一个0~1之间的数，这样点击数肯定会对分类有更显著的影响，所以我们必须对特征进行归一化，我这里采用的是列模归一化，具体的做法就是每个值除以它所在列的平均值。 特征选择 特征是不是越多越好呢，这可不一定。特征数量较多，其中可能存在不相关的特征，特征之间也可能存在相互依赖，容易导致如下的后果： Ø 特征个数越多，分析特征、训练模型所需的时间就越长。 Ø 特征个数越多，容易引起“维度灾难”，模型也会越复杂，其推广能力会下降。 S2里数据库更大，可玩性更高，估计大家要提前的特征也会更多，特征多的情况下，我们就要坐特征选择了。特征选择的意思就是通过算法能够得到前n个特征组合，使得分类器的error rate 最小，即这样组合最具有判别力。 一般特征选择是利用相关系数，好的特征子集所包含的特征应该是与分类的相关度较高（相关度高），而特征之间相关度较低的（亢余度低）。 可以使用线性相关系数(correlation coefficient) 来衡量向量之间线性相关度。 还有之前论文中用到过的mutual information (MI) based method mRMR (minimal-Redundancy-Maximal-Relevance)，这个请参考【1】 分类器构造 首先在构建样本时，有个问题就是正负样本不平衡，常规的方法是重采用正样本，欠采样负样本。Bootstrap 采样是常用的重采样方法，简单的说就是又放回的抽样正样本。数据集第3月份的购买 数（即正样本）为215，我们可以通过放回的随机抽样出1k的正样本，未购买数由20000+条，我们可以无放回的随机抽样出5k条，当然正负样本比例1：5，1：10都可以构造。 构造完样本集，可以选择分类器了，貌似大部分选手都是选择的逻辑回归。逻辑回归的介绍请参见o神的**指南3**，实现起来也是比较方便的，当然由于之前选择样本时随机性较大，一般会取1000次LR的结果取平均的参数来减少随机性的影响。 最后就是分类器的评价了，我的训练集选择前2月的行为特征和第三个月的购买(不是所有购买，前两个月有记录且购买)作为label，这样测试集就是前三个月的行为，以及第四个月的购买行为作为label。这样就可以不停的测试本地的分类效果。 最后说几点： 模型融合会得到较好的效果，不要单纯的靠一种方法黑到底； 多种模型融合的时候，要注意这两种模型尽量不要在同一维度，这样会得到较好的效果 以上谈的数据集基本只能对出现过的历史行为做预测，对于没有出现的组合就无力了。如果要预测没有的组合就可以这样建矩阵：样本为所有 uid×bid,即摆出所有出现过的组合，特征还可以那样提，这样样本总数会变多，正样本数也会变多，对于未曾出现过的（uid，bid）组合其一些历史行为相关的特征可能为0，但是用户购买力，品牌热度的特征都是有的。所以也可以两类情况分开建模，没有试过，应该会有效果。当然cf之类的算法在预测未出现的组合上应该也会有不错的效果。排名靠前的港科同学们肯定也是发现了新组合这篇未开垦的处女地，预测数飙到270+。 LR的结果作为特征再次进行迭代LR会怎么样，没有试过。 [1] H. Peng, F. Long, and C. Ding, “Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 27, no. 8, pp. 1226-1238, 2005.]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[链表问题学习大总结]]></title>
    <url>%2F2014%2F07%2F10%2FData%20structure%20and%20Algorithms%2FDataStructure--%E9%93%BE%E8%A1%A8%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[链表问题学习大总结 链表问题各种被虐，基础非常不牢靠，所以要多花时间补基础，索性搜索一下链表问题做一个大总结。 0. 链表的定义 1234567private static class Node()&#123; int value; Node next; public Node(int value)&#123; this.value=value; &#125;&#125; 1. 求链表的长度 123456789101112131415//判断链表是否为空，如果空则没法赋值,同时不应该改动原来链表的头指针的指向,//复杂度为O(n)private int getListLength(Node head)&#123; if (head == null) return 0; int len=0; Node cur = head; while(cur!==null)&#123; len++; cur=cur.next; &#125; return len;&#125; 2. 求单链表翻转 12345678910111213141516171819202122232425262728293031323334// 翻转链表（遍历） // 从头到尾遍历原链表，每遍历一个结点， // 将其摘下放在新链表的最前端。 // 注意链表为空和只有一个结点的情况。时间复杂度为O（n）private Node reverse (Node head)&#123; Node cur=head; Node rehead=null; if (head == null || head.next==null) &#123; return head; &#125; while(cur!=null)&#123;//较难理解 Node temp = cur; cur=cur.next; temp.next = rehead; rehead= temp; &#125; return rehead;&#125;//翻转链表（递归）private Node reverse(Node head)&#123; Node rehead=null; if (head==null||head.next==null) &#123; return head; &#125; Node rehead=reverse(head.next) head.next.next=head; head.next=null; return rehead;&#125; 3. 求倒数第K个点 题目描述：输入一个单向链表，输出该链表中倒数第k个节点，链表的倒数第0个节点为链表的尾指针。 分析：设置两个指针 p1、p2，首先 p1 和 p2 都指向 head，然后 p2 向前走 k 步，这样 p1 和 p2 之间就间隔 k 个节点，最后 p1 和 p2 同时向前移动，直至 p2 走到链表末尾。very tricky 代码如下： 12345678910111213141516private Node findBottomKth(Node head,int k)&#123; Node slow = null; Node fast = null; slow=head; fast=head; int i=k; for (;i&gt;0&amp;&amp;fast!=null;i--) &#123; fast=fast.next; &#125; while(fast!=null)&#123; fast=fast.next; slow=slow.next; &#125; return slow;&#125; 4. 查找链表中间的值 这题的方法思路和上一题一样，采用快慢两个指针，步长分别为1和2，遍历链表，直到快指针为null停止。 123456789101112131415161718private Node getListMid(Node head)&#123; if (head==null) &#123; return 0; &#125; Node slow,fast; slow=head; fast=head; while(fast!=null&amp;&amp;fast.next!=null)&#123; fast=fast.next(); if (fast.next!=null) &#123; fast=fast.next &#125; else return slow; slow=slow.next; &#125;&#125; 5. 从尾到头打印链表 123456789101112131415161718192021222324252627282930/** * 从尾到头打印单链表 * 对于这种颠倒顺序的问题，我们应该就会想到栈，后进先出。所以，这一题要么自己使用栈，要么让系统使用栈，也就是递归。注意链表为空的情况 * 。时间复杂度为O（n） */ public void printReverseLink(Node head)&#123; Stack&lt;Node&gt; st =new Stack&lt;Node&gt;(); if (head==null) return; Node cur = head; while (cur!=null) &#123; st.push(cur); cur=cur.next; &#125; while (!s.empty()) &#123; System.out.print(st.pop().vaule+" "); &#125;&#125;/** * 使用递归 */public void printReverseLink(Node head)&#123; if (head==null) return; else&#123; Node cur = head; printReverseLink(cur.next); System.out.print(cur.vaule+" "); &#125;&#125; 6. 判断链表是否有环，并找到环的入口 题目描述：输入一个单向链表，判断链表是否有环。如果链表存在环，如何找到环的入口点？ 解题思路： 由上题可知，按照 p2 每次两步，p1 每次一步的方式走，发现 p2 和 p1 重合，确定了单向链表有环路了。接下来，让p2回到链表的头部，重新走，每次步长不是走2了，而是走1，那么当 p1 和 p2 再次相遇的时候，就是环路的入口了。 为什么？：假定起点到环入口点的距离为 a，p1 和 p2 的相交点M与环入口点的距离为b，环路的周长为L，当 p1 和 p2 第一次相遇的时候，假定 p1 走了 n 步。那么有： p1走的路径： a+b ＝ n； p2走的路径： a+b+k*L = 2*n； p2 比 p1 多走了k圈环路，总路程是p1的2倍 根据上述公式可以得到 k*L=a+b=n显然，如果从相遇点M开始，p1 再走 n 步的话，还可以再回到相遇点，同时p2从头开始走的话，经过n步，也会达到相遇点M。 显然在这个步骤当中 p1 和 p2 只有前 a 步走的路径不同，所以当 p1 和 p2 再次重合的时候，必然是在链表的环路入口点上。 代码如下： 12345678910111213141516171819202122232425public Node findLoopPort(head)&#123; Node slow; Node fast; slow=head; fast=head; while(fast!=null&amp;&amp;fast.next!=null)&#123; slow=slow.next; fast=fast.next.next; if (slow==fast) break; &#125; if (slow!=fast) &#123; return null; &#125; else&#123; fast=head; while(fast!=slow) &#123; fast=fast.next; slow=slow.next; &#125; return slow; &#125;&#125; 7.判断链表是否相交 **题目描述：**给出两个单向链表的头指针（如下图所示）， 比如h1、h2，判断这两个链表是否相交。这里为了简化问题，我们假设两个链表均不带环。 解题思路： 直接循环判断第一个链表的每个节点是否在第二个链表中。但，这种方法的时间复杂度为O(Length(h1) * Length(h2))。显然，我们得找到一种更为有效的方法，至少不能是O（N^2）的复杂度。 针对第一个链表直接构造hash表，然后查询hash表，判断第二个链表的每个节点是否在hash表出现，如果所有的第二个链表的节点都能在hash表中找到，即说明第二个链表与第一个链表有相同的节点。时间复杂度为为线性：O(Length(h1) + Length(h2))，同时为了存储第一个链表的所有节点，空间复杂度为O(Length(h1))。是否还有更好的方法呢，既能够以线性时间复杂度解决问题，又能减少存储空间？ 转换为环的问题。把第二个链表接在第一个链表后面，如果得到的链表有环，则说明两个链表相交。如何判断有环的问题上面已经讨论过了，但这里有更简单的方法。因为如果有环，则第二个链表的表头一定也在环上，即第二个链表会构成一个循环链表，我们只需要遍历第二个链表，看是否会回到起始点就可以判断出来。这个方法的时间复杂度是线性的，空间是常熟。 进一步考虑“如果两个没有环的链表相交于某一节点，那么在这个节点之后的所有节点都是两个链表共有的”这个特点，我们可以知道，如果它们相交，则最后一个节点一定是共有的。而我们很容易能得到链表的最后一个节点，所以这成了我们简化解法的一个主要突破口。那么，我们只要判断两个链表的尾指针是否相等。相等，则链表相交；否则，链表不相交。 所以，先遍历第一个链表，记住最后一个节点。然后遍历第二个链表，到最后一个节点时和第一个链表的最后一个节点做比较，如果相同，则相交，否则，不相交。这样我们就得到了一个时间复杂度，它为O((Length(h1) + Length(h2))，而且只用了一个额外的指针来存储最后一个节点。这个方法时间复杂度为线性O(N)，空间复杂度为O(1)，显然比解法三更胜一筹。 12345678910public boolean isConnected(Node head1, Node head2)&#123; Node tail1 = head1; Node tail2 =head2; while(tail1!=null) tail1=tail1.next; while(tail2!=null) tail2=tail2.next; return tail2==tail1;&#125; 8.求两个单链表相交的第一个节点 题目描述：如果两个无环单链表相交，怎么求出他们相交的第一个节点呢？ 分析：采用对齐的思想。计算两个链表的长度 L1 , L2，分别用两个指针 p1 , p2 指向两个链表的头，然后将较长链表的 p1（假设为 p1）向后移动L2 - L1个节点，然后再同时向后移动p1 , p2，直到 p1 = p2。相遇的点就是相交的第一个节点。 1234567891011121314151617181920212223242526272829303132333435363738public Node getFirstCommonNode(Node head1, Node head2)&#123; Node tail1 = head1; Node tail2 =head2; int l1=0; while(tail1!=null)//list1 长度l1 &#123; tail1=tail1.next; l1++; &#125; int l2=0; while(tail2!=null)//list2 长度l2 &#123; tail2=tail2.next; l2++; &#125; if (tail1!=tail2) &#123; //如果不相等直接返回空，结束 return null; &#125; tail1=head1;//再回到头，长的链表先走|l1-l2|，再同步走直到相交 tail2=head2; if (l1&gt;l2) &#123; int k=l1-k2; for(;k!=0;k--) tail1=tail1.next; &#125; else&#123; int k=l2-l1; for (;k!=0;k--) &#123; tail2=tail2.next; &#125; &#125; while(tail1!=tail2)&#123; tail1=tail1.next; tail2=tail2.next; &#125; return tail1;&#125; 9. 在O(1)时间删除链表节点 **题目描述：**给定链表的头指针和一个节点指针，在O(1)时间删除该节点。[Google面试题] 分析：本题与《编程之美》上的「从无头单链表中删除节点」类似。主要思想都是「狸猫换太子」，即用下一个节点数据覆盖要删除的节点，然后删除下一个节点。但是如果节点是尾节点时，该方法就行不通了。 1234567891011121314151617181920212223242526272829 /** * 给出一单链表头指针head和一节点指针toBeDeleted，O(1)时间复杂度删除节点tBeDeleted * 对于删除节点，我们普通的思路就是让该节点的前一个节点指向该节点的下一个节点 * ，这种情况需要遍历找到该节点的前一个节点，时间复杂度为O(n)。对于链表， * 链表中的每个节点结构都是一样的，所以我们可以把该节点的下一个节点的数据复制到该节点 * ，然后删除下一个节点即可。要注意最后一个节点的情况，这个时候只能用常见的方法来操作，先找到前一个节点，但总体的平均时间复杂度还是O(1) */ public void delete(Node head, Node toDelete)&#123; if(toDelete == null)&#123; return; &#125; if(toDelete.next != null)&#123; // 要删除的是一个中间节点 toDelete.val = toDelete.next.val; // 将下一个节点的数据复制到本节点! toDelete.next = toDelete.next.next; &#125; else&#123; // 要删除的是最后一个节点！ if(head == toDelete)&#123; // 链表中只有一个节点的情况 head = null; &#125;else&#123; Node node = head; while(node.next != toDelete)&#123; // 找到倒数第二个节点 node = node.next; &#125; node.next = null; &#125; &#125; &#125; &#125; 10. 参考： 面试精选：链表问题集锦 面试大总结之一：Java搞定面试中的链表题目]]></content>
      <categories>
        <category>Data Struture and algorithm</category>
      </categories>
      <tags>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode刷题记--字符串包含]]></title>
    <url>%2F2014%2F06%2F05%2FData%20structure%20and%20Algorithms%2FLittle%20trick--%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%85%E5%90%AB%2F</url>
    <content type="text"><![CDATA[Little trick--字符串包含 题目描述 给定两个分别由字母组成的字符串A和字符串B，字符串B的长度比字符串A短。请问，如何最快地判断字符串B中所有字母是否都在字符串A里？ 为了简单起见，我们规定输入的字符串只包含大写英文字母，请实现函数bool StringContains(string &amp;A, string &amp;B) 比如，如果是下面两个字符串： String 1：ABCD String 2：BAD 答案是true，即String2里的字母在String1里也都有，或者说String2是String1的真子集。 如果是下面两个字符串： String 1：ABCD String 2：BCE 答案是false，因为字符串String2里的E字母不在字符串String1里。 同时，如果string1：ABCD，string 2：AA，同样返回true。 分析与解法 题目描述虽长，但题意很明了，就是给定一长一短的两个字符串A，B，假设A长B短，要求判断B是否包含在字符串A中。 初看似乎简单，但实现起来并不轻松，且如果面试官步步紧逼，一个一个否决你能想到的方法，要你给出更好、最好的方案时，恐怕就要伤不少脑筋了。 解法一 判断string2中的字符是否在string1中?最直观也是最简单的思路是，针对string2中每一个字符，逐个与string1中每个字符比较，看它是否在String1中。 代码可如下编写： 1234567891011121314151617public static boolean stringContain(String S1,String S2)&#123; char[] S1Array=S1.toCharArray(); char[] S2Array=S2.toCharArray(); for(int i=0;i&lt;S2Array.length;i++)&#123; int j; for ( j=0;j&lt;S1Array.length;j++)&#123; if(S2Array[i]==S1Array[j])&#123; break; &#125; &#125; //如果内圈循环结束，判断是否遍历完了还是没有找到相等的 if (j&gt;=S1Array.length)&#123; return false; &#125; &#125; return true;&#125; 假设n是字符串String1的长度，m是字符串String2的长度，那么此算法，需要O(n*m)次操作。显然，时间开销太大，应该找到一种更好的办法。 解法二 如果允许排序的话，我们可以考虑下排序。比如可先对这两个字符串的字母进行排序，然后再同时对两个字串依次轮询。两个字串的排序需要(常规情况)O(m log m) + O(n log n)次操作，之后的线性扫描需要O(m+n)次操作。 关于排序方法，可采用最常用的快速排序，参考代码如下： 12345678910111213141516171819//Version 2 排序后，使用连个指针指向长短两个string，进行遍历public static boolean stringContain2(String longstr, String shortstr)&#123; char[] longstrArray=longstr.toCharArray(); char[] shortstrArray=shortstr.toCharArray(); Arrays.sort(longstrArray); Arrays.sort(shortstrArray); int i=0;//point to longstr int j=0;//point ot shortstr while(j&lt;shortstrArray.length)&#123; while(i&lt;longstrArray.length&amp;&amp;shortstrArray[j]&gt;longstrArray[i]) i++; if(i&gt;=longstrArray.length)&#123; return false; &#125; j++; &#125; return true;&#125; 解法三(tricky) 有没有比快速排序更好的方法呢？ 我们换一种角度思考本问题： 假设有一个仅由字母组成字串，让每个字母与一个素数对应，从2开始，往后类推，A对应2，B对应3，C对应5，......。遍历第一个字串，把每个字母对应素数相乘。最终会得到一个整数。 利用上面字母和素数的对应关系，对应第二个字符串中的字母，然后轮询，用每个字母对应的素数除前面得到的整数。如果结果有余数，说明结果为false。如果整个过程中没有余数，则说明第二个字符串是第一个的子集了（判断是不是真子集，可以比较两个字符串对应的素数乘积，若相等则不是真子集）。 思路总结如下： 按照从小到大的顺序，用26个素数分别与字符'A'到'Z'一一对应。 遍历长字符串，求得每个字符对应素数的乘积。 遍历短字符串，判断乘积能否被短字符串中的字符对应的素数整除。 输出结果。 如前所述，算法的时间复杂度为O(m+n)的最好的情况为O(n)（遍历短的字符串的第一个数，与长字符串素数的乘积相除，即出现余数，便可退出程序，返回false），n为长字串的长度，空间复杂度为O(1)。由于这个方法太过机智，没有用java去实现，只是照搬了[1]中的C++代码. 1234567891011121314151617181920212223//此方法只有理论意义，因为整数乘积很大，有溢出风险。bool StringContain(string &amp;a,string &amp;b)&#123; const int p[26] = &#123;2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59,61, 67, 71, 73, 79, 83, 89, 97, 101&#125;; int f = 1; for (int i = 0; i &lt; a.length(); ++i) &#123; int x = p[a[i] - 'A']; if (f % x) &#123; f *= x; &#125; &#125; for (int i = 0; i &lt; b.length(); ++i) &#123; int x = p[b[i] - 'A']; if (f % x) &#123; return false; &#125; &#125; return true;&#125; 解法四(very tricky) 如果面试官继续追问，还有没有更好的办法呢？计数排序？除了计数排序呢？ 事实上，可以先把长字符串a中的所有字符都放入一个Hashtable里，然后轮询短字符串b，看短字符串b的每个字符是否都在Hashtable里，如果都存在，说明长字符串a包含短字符串b，否则，说明不包含。 再进一步，我们可以对字符串A，用位运算（26bit整数表示)计算出一个“签名”，再用B中的字符到A里面进行查找。 12345678910111213141516 //version 3 “最好的方法”，时间复杂度O(n + m)，空间复杂度O(1)public static boolean stringContain3(String longstr, String shortstr)&#123; char[] longstrArray=longstr.toCharArray(); char[] shortstrArray=shortstr.toCharArray(); int hash = 0; for(int i=0;i&lt;longstrArray.length;i++)&#123; hash=hash|(1&lt;&lt;(longstrArray[i]-'A')); &#125; for(int j=0;j&lt;shortstrArray.length;j++)&#123; if((hash &amp; 1&lt;&lt;(shortstrArray[j]-'A'))==0)&#123; return false; &#125; &#125; return true;&#125; 该方法将字符串转换成为二进制的位值表示，比如“ABCD”，通过第一个循环转换，按位或完之后成了0x0F,二进制表示的话是00001111，再用按位与的方式判断值是否为0，来进行判断是否存在，非常巧妙！这个方法的实质是用一个整数代替了hashtable，空间复杂度为O(1)，时间复杂度还是O(n + m) 举一反三 1、变位词 如果两个字符串的字符一样，但是顺序不一样，被认为是兄弟字符串，比如bad和adb即为兄弟字符串，现提供一个字符串，如何在字典中迅速找到它的兄弟字符串，请描述数据结构和查询过程。 大部分参考了: [1] The-Art-Of-Programming-By-July chapter1.02]]></content>
      <categories>
        <category>Data Struture and algorithm</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里产品经理实习申请之阿里音乐竞品分析]]></title>
    <url>%2F2014%2F05%2F09%2Falibaba%2F%E9%98%BF%E9%87%8C%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86%E5%AE%9E%E4%B9%A0%E7%94%B3%E8%AF%B7%E4%B9%8B%E9%98%BF%E9%87%8C%E9%9F%B3%E4%B9%90%E7%AB%9E%E5%93%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[阿里音乐竞品分析 之前参加的阿里产品经理实习招聘，招聘的链接在**这里** 阿里这种提交PPT的实习招聘形式，个人感觉还是不错的，毕竟产品经理的岗位是比较少的，挨个看简历，不如直接给个任务让你在一定的时间内做出来。这样一方面招聘方节省了成本，另一方面还可以针对阿里目前面临一些问题搜集一些方案，集思广益嘛，虽然可能到最后能用的，有意思的没几个，但是能有这么多人争着破头皮为自己想方案，是多么幸福的事啊。这不禁让我想起了大数据比赛。关于这种形式的讨论可以看**这里**，是的，问题也是我提的。 关于产品经理这个岗位感觉也是挺有意思的，毕竟大学没有一个专业说是一个产品专业。个人感觉这个岗位是完全靠自己对生活的理解去悟的，要用文字来定义的话，我一个工科男士做不到了。可能是这样：必须综合能力比较好，有美感，什么都懂点，视野宽广些，可能要懂点心理，还要懂点人性、佛学，还要有情怀，还得是处女座（这条我是在瞎扯我自己，其实这些条都是瞎扯我自己哈哈）。 话不多说，分享一下我的ppt吧，毕竟码农，请轻拍！ 有情怀的附上**PPT下载链接**]]></content>
      <tags>
        <tag>产品 阿里</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图像的采样与量化及灰度直方图]]></title>
    <url>%2F2014%2F04%2F26%2Flinks%2F2013-05-31%2F</url>
    <content type="text"></content>
      <tags>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Data Structure of Berkeley(1)]]></title>
    <url>%2F2014%2F04%2F26%2FData%20structure%20and%20Algorithms%2F2013-05-27%2F</url>
    <content type="text"><![CDATA[Data Structure of Berkeley(1) List Store a list of ints as an array. Disadvantages of array. Insert item of beginning or middle take time proportional to length of array Arrats have a fixed length. LINKED LISTS(a recursive data type) Made up of &quot;nodes&quot;. Each node has *an item *a reference to the next node public class ListNode{ int item; ListNode next; } //Let’s make some ListNodes. ListNode l1 = new ListNode(), l2 = new ListNode(), l3 = new ListNode(); l1.item = 7; l2.item = 0; l3.item = 6; /*------------- ------------- ------------- | ----- | | ----- | | ----- | | item| 7 | | | item| 0 | | | item| 6 | | l1--&gt;| ----- | l2--&gt;| ----- | l3--&gt;| ----- | | ----- | | ----- | | ----- | | next| ? | | | next| ? | | | next| ? | | | ----- | | ----- | | ----- | ------------- ------------- ------------Now let’s link them together.*/ l1.next = l2; l2.next = l3; /*What about the last node? We need a reference that doesn’t reference anything.In Java, this is called &quot;null&quot;.*/ l3.next = null; /* ------------- ------------- ------------- | ----- | | ----- | | ----- | | item| 7 | | | item| 0 | | | item| 6 | | l1--&gt;| ----- | l2--&gt;| ----- | l3--&gt;| ----- | | ----- | | ----- | | ----- | | next| .-+-+--------&gt;| next| .-+-+--------&gt;| next| X | | | ----- | | ----- | | ----- | ------------- ------------- ------------- */ Node Operations to simplify programming , public ListNode(int item , ListNode next) { this.item=item; this.next=next; }]]></content>
      <categories>
        <category>Data Struture and algorithm</category>
      </categories>
      <tags>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MSER笔记——随机抽样一致性算法（RANSAC）]]></title>
    <url>%2F2014%2F04%2F26%2Flinks%2F2013-05-25%2F</url>
    <content type="text"></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的数学之美（一）——RANSAC算法详解]]></title>
    <url>%2F2014%2F04%2F26%2Flinks%2F2013-05-25-1%2F</url>
    <content type="text"></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Data Structure of Berkeley（0）]]></title>
    <url>%2F2014%2F04%2F26%2FData%20structure%20and%20Algorithms%2F2013-05-17%2F</url>
    <content type="text"><![CDATA[Array char c[];//引用一个任意长度的字符数组 c=new char[4]; c[0]='b'; c[1]='l'; c[2]='u'; c[3]='e'; c[4]='s';//runtime error 编译时没有问题运行时出现问题 int l= c.length//获得数组的长度，这里.length是一个域，如果为string.length 则一个为方法。filed和method的区别，method为一段代码函数，filed为存储在某一位置的一个变量。 Primes Revisited 素数遍历 程序分三步，主要算法为古老的sieve of eratosthenes（埃拉托斯特尼筛法）,第一步假设所有的数都为素数，第二部，用2~根号n的除子去除这个数。第三步打印. //Part1 public static void printPrimes[int n]{ boolean[] prome = new boolean[n+1]; //Number 0~n 为何不用n-1为了是下标对应数值 int i ; for (i=2;i&lt;=n ;i ++) prime[i]= true; //Part2 for(int divisor =2 ;divisor *divisor &lt;=n; divisor++)//divisor从0~根号N递增 if prime(divisor){ //省去前面除子已经遍历出的非素数除子 for (i=2*divisor;i&lt;=n;i=i+divisor) //从两倍非素数除子开始，没遍历一次除子加一倍，并判断是否已超出范围 prime[i]=false; } //Part3 for(i=2;i&lt;=n;i++){ if(prime[i]){ System.out.print(“”+i)； } } } Pascal Triangle /*返回一个三角 1 1 1 1 2 1 1 3 3 1 1 4 6 4 1 1 5 10 10 5 1 */ 12345678910111213141516171819public static int [][] pascalTriangle(int n)&#123; int [][] pt = new int [n][]; for(int i= 0;i&lt;n;i++)&#123; pt[i]= new int [i+1]; pt[i][0]=1; for(int j = 1;j&lt;i;j++)&#123; pt[i][j]=pt[i-1][j-1]+pt[i-1][j]; &#125; pt[i][i]=1; &#125; return pt; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Data Struture and algorithm</category>
      </categories>
      <tags>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[献给写作者的 Markdown 新手指南]]></title>
    <url>%2F2014%2F04%2F26%2Flinks%2F%E7%8C%AE%E7%BB%99%E5%86%99%E4%BD%9C%E8%80%85%E7%9A%84%20Markdown%20%E6%96%B0%E6%89%8B%E6%8C%87%E5%8D%97(%E8%BD%AC)%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[排序大组合-选择、插入、希尔、Shuffling、归并、快速]]></title>
    <url>%2F2014%2F04%2F09%2FData%20structure%20and%20Algorithms%2F%E6%8E%92%E5%BA%8F%E5%A4%A7%E7%BB%84%E5%90%88%2F</url>
    <content type="text"><![CDATA[排序大组合-选择、插入、希尔、Shuffling、归并、快速 选择排序（SelectionSort） 1.在第i次迭代的时候，找i+1~N中的最小值对应下标min 2.swap a[i]和a[min]（一定要自己写一遍加深印象！） import java.util.Random; public class selectionSort{ public static void sort(Comparable[] a){ int len=a.length; for(int i=0;i&lt;len;i++){ int min=i; for(int j=i+1;j&lt;len;j++){ if (less(a[j],a[min])) min=j;}///非常需要注意，内部循环是不换值的，只找最小值 exch(a,i,min); } } private static boolean less(Comparable a,Comparable b){ return a.compareTo(b)&lt;0; } private static void exch(Comparable[] a,int i, int j){ Comparable temp; temp=a[i]; a[i]=a[j]; a[j]=temp; } //测试验证 public static void main(String[] args){ Comparable test[] =new Comparable[10]; for (int i=0;i&lt;10 ; i++) { test[i]=new java.util.Random().nextInt(15); } // Comparable test[]={'f','s','d','g'}; int N=test.length; for (int i=0;i&lt;N;i++){ System.out.println(test[i]); } selectionSort.sort(test); System.out.println(); for (int i=0;i&lt;N;i++){ System.out.println(test[i]); } } } 运行结果：（随机的，大家都不一样） 6 13 6 11 10 10 13 4 7 5 4 5 6 6 7 10 10 11 13 13 分析：算法复杂度O（N*N） 即使输入是已经排序，花费仍是Quadratic Time（二次的） 非常暴力原始的方法。 插入排序（insertionSort） 每次迭代[i]，把a[i]和它位于它左边的较大值互换。 public class Insertionsort{ public static void sort(Comparable[] a){ int N=a.length; for(int i=0;i&lt;N;i++) for(int j=i;j&gt;0;j--)//j始于i if (less(a[j],a[j-1])) exch(a,j,j-1); //内圈换值 else break; } private static boolean less(Comparable a,Comparable b){ return a.compareTo(b)&lt;0; } private static void exch(Comparable[] a,int i, int j){ Comparable temp; temp=a[i]; a[i]=a[j]; a[j]=temp; } //测试验证 public static void main(String[] args){ Comparable test[] =new Comparable[10]; for (int i=0;i&lt;10 ; i++) { test[i]=new java.util.Random().nextInt(15);//0~14随机数 } // Comparable test[]={'f','s','d','g'}; int N=test.length; for (int i=0;i&lt;N;i++){ System.out.print(test[i]+&quot; &quot;); } Insertionsort.sort(test); System.out.println(); for (int i=0;i&lt;N;i++){ System.out.print(test[i]+&quot; &quot;); } } } 运行结果： 11 3 10 6 12 0 11 9 3 12 0 3 3 6 9 10 11 11 12 12 [Finished in 0.8s] 复杂度： stability：stable，左移位置不多，不会跨越较多元素。 希尔排序（Shell 排序） 总体思想是，跳跃式的一组一组排，按h=3h+1，比如h=4时，先排a[0],a[4],a[8]....再排a[1],a[5]...再对每组按h=1排，就排好了 public static void sort(Comparable[] a) { int N = a.length; int h = 1; while (h &lt; N/3) h = 3*h + 1; // 1, 4, 13, 40, 121, 364, ... while (h &gt;= 1) { // h-sort the array. for (int i = h; i &lt; N; i++) { for (int j = i; j &gt;= h &amp;&amp; less(a[j], a[j-h]); j -= h) exch(a, j, j-h); } h = h/3; } }]]></content>
      <categories>
        <category>Data Struture and algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime Text 3 complie and run Java code]]></title>
    <url>%2F2014%2F04%2F05%2FTools%2FSublime%20Text%203%20complie%20and%20run%20Java%20code%2F</url>
    <content type="text"><![CDATA[Sublime Text 3 complie and run Java code Tools -&gt;Build System-&gt; New build system Paste: 1234567&#123; "cmd": ["javac", "$file_name","&amp;&amp;","java", "$file_base_name"], "file_regex": "^(...*?):([0-9]*):?([0-9]*)", **"path": "C:\\Program Files\\Java\\jdk1.6.0\\bin\\",** "selector": "source.java", "shell": true &#125; Note that replace your own path of jdk in ** **,and save it as &quot;java.sublime-bulid&quot; Chose Build system -&gt;java and press Ctrl-B to complie and run you java code.]]></content>
      <categories>
        <category>tools</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[矩阵按列按行归一化到L2范数的原理和最精简Matlab代码]]></title>
    <url>%2F2013%2F09%2F02%2FData%20structure%20and%20Algorithms%2F2013-09-02%2F</url>
    <content type="text"><![CDATA[矩阵按列按行归一化到L2范数的原理和最精简Matlab代码 在模式识别和机器学习的数据预处理过程中，对数据集按行或者按列进行L2范数归一化是一种常见的归一化方式，因此本文将介绍对向量进行L2范数归一化的原理和方法，并给出相关的Matlab源代码，供后学者作为基础知识参考使用。 由此，我们可以很块的写出最简单的matlab源代码如下： 首先按行归一化： % Examples A=[3 4;5 12]; [m n] = size(A); % normalize each row to unit for i = 1:m A(i,:)=A(i,:)/norm(A(i,:)); end 按列归一化： % normalize each column to unit A=[3 4;5 12]; for i = 1:n A(:,i)=A(:,i)/norm(A(:,i)); end 然而，上述代码最能实现功能，但并不是最优的，它只是一种对该过程的最佳理解代码。在Matlab中，for循环是一件非常费时间的结构，因此我们在代码中应该尽量少用for循环。由此，我们可以用repmat命令得到另一种更加简洁更加快速的代码，只是这种代码对于初学者理解起来比较费劲。可以看错是自己水平的一种进阶吧。 % normalize each row to unit A = A./repmat(sqrt(sum(A.^2,2)),1,size(A,2)); % normalize each column to unit A = A./repmat(sqrt(sum(A.^2,1)),size(A,1),1);]]></content>
      <categories>
        <category>Data Struture and algorithm</category>
      </categories>
      <tags>
        <tag>Matlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[20个数据可视化工具]]></title>
    <url>%2F2013%2F07%2F01%2Flinks%2F2013-07-01%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[线性判别分析(Linear Discriminant Analysis, LDA）算法分析]]></title>
    <url>%2F2013%2F06%2F15%2Flinks%2F2013-06-15%2F</url>
    <content type="text"></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何高效利用GitHub]]></title>
    <url>%2F2013%2F05%2F17%2Flinks%2F2013-05-17-1%2F</url>
    <content type="text"></content>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Theme for Farbox]]></title>
    <url>%2F2013%2F05%2F09%2Flinks%2F2013-05-09-3%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[MSER笔记]]></title>
    <url>%2F2013%2F05%2F09%2FTech%2FMSER%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[MSER笔记 宽基线和窄基线： 基线的本意是指立体视觉系统中两摄像机光心之间的距离。依据拍摄两幅图 像的视点位置关系可将对应点匹配问题分为宽基线(Wide Baseline)和窄基线匹配(Short Baseline)。宽基线一词用于匹配时，泛指两幅图像有明显不同的情况下的匹配。产生这种情况的原因有可能为摄像机之间的位置相差很大，也有可能由于摄像机旋转或焦距的变化等因素产生的。 宽基线匹配和窄基线匹配的分界不是很严格，但是在窄基线匹配中存在如下假设：摄像机焦距及其它内参数变化不大：摄像机位置不会相差很远，不会有大的转动，对应点的邻域是相似的。 宽基线匹配中则存在如下假设：对图像上的任意点，在另一图像上的对应点可以为任意位置；摄像机可以任意移动，且摄像机的焦距及其它内参数可以有较大的变化；一幅图像上的景物在另一幅图像上可能被遮挡；对应点的邻域有相似的地方，但由于摄像机位置的变化及光照的变化，单依靠邻域的相似不能得到正确的对应。 窄基线匹配中典型方法是利用邻域的互相关(Neighborhood Cross-Correlation)方法．但在宽基线的情况下，图像之间拍摄距离较远，成像条件存在较大差异，即使是空间同一特征，在图像中所表示出来的光学特性(灰度值，颜色值等)、几何特性(外形，大小等)及空间位置(图像中的位置，方向等)都有很大的不同，再加上噪声、遮挡等因素的存在，此时基于邻域互相关的匹配方法就失效了。在宽基线匹配中，仅仅使用特征本身的信息(比如边缘、角点的位置信息)是难以正确匹配的，研究学者将多个特征尤其是结构性特征予以组合，以形成稳定的特征向量(称为特征描述符)。这种对于图像的几何变形、光照变化等因素保持一定稳定性的特征向量称为不变量(Invariarlt)．不变量技术是宽基线匹配应用中的重要技术。 选自赵辉硕士论文。 仿射和投影：（affine and project）： 投影：在线性代数和泛函分析中，投影是从向量空间映射到自身的一种线性变换，是日常生活中“平行投影”概念的形式化和一般化。同现实中阳光将事物投影到地面上一样，投影变换将整个向量空间映射到一个它的一个子空间，并且在这个子空间中是恒等变换 仿射：在几何上，仿射几何是不涉及任何原点、长度或者角度概念的几何，但是有两点相减得到一个向量的概念。 它位于欧氏几何和射影几何之间。它是在域K上任意维仿射空间的几何。K为实数域的情况所包含的内容足够使人了解其大部分思想。 仿射变换：仿射变换，又称仿射映射，是指在几何中，一个向量空间进行一次线性变换并接上一个平移，变换为另一个向量空间。 一个对向量 平移 ，与旋转放大缩小 的仿射映射为 上式在 齐次坐标上，等价于下面的式子 在分形的研究里，收缩平移放射映射可以制造制具有自相似性的分形 极几何： 极几何：epipolar geometry，又称核面几何。 极几何是机器视觉中摄像机标定中的技术名词，在世界坐标系，观察坐标系，像素坐标系等坐标系转换中是很重要的一个概念。 对于双目视觉系统，即有两个摄像机，定义两个摄像机的光学中心点为 C、C‘，在三维空间中存在一个场景点X，这个点与两个摄像机光学中心点共同构成的平面就是极平面π（epipolar plane），每个摄像机都有一个图像平面，分别为Image1和Image2，CX交Image1于x点，C'X交Image2于x'点，而CC'连线分别交两个图像平面于e和e'，这两个点称为极点（epipoles），CC'称为基线（baseline）。极平面与图像平面相交于两条极线（epipolar line）l和l'，这两条极线的关系是对应的（correspondence），而x、e、x'、e'分别位于l和l'上。 随着三维场景点的移动，极平面将绕着基线转动，这些极平面共同构成一个极平面束（an epipolar pencil），这些极平面与图像平面所交汇成的极线族分别都交于两个极点e和e'。 假如我们只知道X投射在图像平面Image1上的投射点x，我们如何去 获知在另一个图像平面上（也就是Image2）x的相应点x'呢，这个相应点x'符合什么样一种几何规则呢？我们知道，极平面是由基线和xX共同构成的，从上面的介绍我们知道了这个相应点（现在还是未知点）也一定位于极平面π上，因此可以得出x'点位于极平面π与另一个图像平面Image2的交线l'上，也即l'是投射点x的反向投影在第二个视角（第二个图像平面）上的图像。这种关系在立体对应算（stereo correspondence algorithm）中有很大的好处，那就是不需要在整幅图像上寻找x的对应点（correspondence points），而只需要把范围限定在极线l'上即可。 图形说明]]></content>
      <categories>
        <category>computer vision</category>
      </categories>
      <tags>
        <tag>机器视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Robust wide-baseline stereo from maximally stable extremal regions》 译文]]></title>
    <url>%2F2013%2F05%2F09%2FTech%2F2013-05-09%2F</url>
    <content type="text"><![CDATA[Robust wide-baseline stereo from maximally stable extremal regions 译文 最近一直在研究MSER的区域特征提取方法，翻译了提出这种方法的文章，闲着没事发出来，有些地方翻的不好，就在这几天完善一下吧。 原文参见：这里 译文的pdf请参见：这里 摘要： 本文研究了宽基线立体问题，即在不同视角对同一对象拍摄的一组图片如何建立匹配。 同时引入了一种新的图像元素集到图像匹配中，即所谓的极值区域。极值区域拥有非常有用的一些特性：这个集合在图像原点坐标的连续变换和图像强度的单调变换的情况下都是封闭的。并提出了一种针对仿射不变的极值区域稳定子集的高效（接近线性复杂度）和快速的检测算法（接近帧频），即最大稳定极值区域法(MSER)。 进而本文提出了一种新的鲁棒的建立试探性匹配的相似性测量方法。该方法的鲁棒性保证了多测量区域（这些区域都由极值区域的不变结构获得）的仿射不变性。其中一些区域远大于MSER区域，因此更加有辨识度，可以用于建立试探性匹配。 我们通过对室内和室外的场景所获取的图片组进行宽基带试验，选择了多测量区域，验证了MSERs的高实用性和算法的鲁棒性。在测试过程实现了尺度强烈变化（3.5X），光照条件变化，平面外旋转，闭塞情况，局部非均值尺度变换和拍摄视角的3D变换。最终在核面几何上获得了良好的估计（极线上的匹配点的平均距离低于像元间距离的0.09）。 关键词：宽基线匹配；显著边缘；最大稳定极值区域；MSER；鲁棒度量 1. 引言 在全自动的三维场景重建的过程中，用不同的摄像机在不同的光照条件下在任意的视角对同一个场景拍摄得到的两张照片进行比对，来寻找可靠的特征匹配是个非常困难又关键的步骤。这个过程至关重要的问题是对于已经寻找到匹配的元素的选择。在宽基线成像过程中，局部图像变形不能够通过变换或者旋转变化被现实的近似，因此需要一个全仿射模型。由于在欧式几何中一些固定的形状比如说长方形或者圆等，他们的形状经过仿射变换后将改变。因此特征匹配不能够通过比较这些固定形状区域的边缘来获得。 大多数图像都有可以都会有一些一些区域可以检测出高重复性，因为这些区域有一些区别，不变性和稳定的特性。在本文中，我们把这些通常是数据依赖的区域称为显著区域（DRs），作为将要被放入到立体匹配或者目标识别的匹配当中的元素。 本文的第一件事是介绍了一个新的显著区域集合，即所谓的极值区域，极值区域拥有两个非常好的特性。一是在一对一图像坐标的连续变换中，集合是封闭的，二是在图像强度单调变换时，集合也是封闭的。并提出了一种针对仿射不变的极值区域稳定子集的高效（接近线性复杂度）和快速的检测算法（接近帧频），即最大稳定极值区域法(MSER)。对于某一特定累心的显著区域，它的鲁棒性依赖于图像数据，而且需要经过试验来测试。第四章将介绍了我们对于室内和室外的场景图进行的宽基带试验，试验的成功说明了MSERs非常有潜力。 在宽基线匹配中，对于一些为数不多的潜在匹配点，可靠的提取是一个匹配成功的必要但是不充分的条件。当有两个显著区域集市，匹配问题可以归结为匹配空间的搜索。形成一幅完整的限制区域两偶图，寻找匹配点的全局一致性子集显然对于计算来说是轻而易举的。最近的该领域的研究提出了一整套具有共同结构的立体匹配和目标识别算法。这些方法利用了局部的不变特征子，来减少试探性特征点的数量。这个步骤重要的设计决策包括：(1) 测量区域的选择，即在选在图像的哪部分进行不变性计算。(2)根据不变性描述如何选取试探匹配点和（3）不变量的选择。 通常，作为测量区域和试探匹配的特征区域或者它们的缩放版本，是通过利用马氏距离比较不变性后得到的。本文所做的第二件事是提出了一种鲁棒的相似性测量方法，来建立试探性的匹配，替代了原来的马氏距离测量方法。这种相似性测量方法允许我们可以使用一系列测量区域的不变性，有些区域甚至远大于相关的显著区域，非常具有鲁棒性。大区域的测量要么是非常容易识别（因为两个图片的大区域完全相同时非常不可能的）要么是完全错误的（比如如果区域的方向和深度不连续）。前者帮助建立了可靠的试探性匹配而由于该方法具有较强的鲁棒性，后者的影响很小。利用大多数的试探性匹配来寻找极几何一致性是 这整个宽基线算法的最后一步。就目前看来，RANSAC法是最被广泛采用的方法。而我们提出的算法采取了新奇的不走来增加匹配区域的数量，以及极几何的精度。为了更多的获取到区域匹配，依照试探性匹配进行了粗略的极几何预估。这样约束了极线的位置，预估了匹配区域之间的仿射映像。这个映像使得相关性过滤掉了不匹配。这个步骤显著的增加了极几何估计的精度。最后的平均内层窗极线间距离小于0.1像素。详情参见第三章。 相关工作：自从Schmid和Mohr发表的有影响的论文依赖，许多图像匹配和宽基线算法被提了出来。许多通常使用Harris特征点来做为显著区域。Tell和Carlsson提出了一种方法将Harris特征点 用线段与测量区域相连。这个测量因为尺度不变傅里叶系数而非常有特点Harris特征点检测法在一定范围的尺度内是稳定的，但是没有定义尺度或者仿射不变的测量区域。 本文余下的文章的组织结构是这样的。第二章定义了MSER，描述了MSER的检测算法。第三章给出新颖的鲁棒检测算法的细节。第四章这介绍了用未校准的摄像机拍摄的室内室外的图片的实验结果，第五种对实验进行了总结，在复述了本文所做的贡献。 ## MSER 这章我们介绍了一种在宽基线匹配中非常有用的新的图像元素，及最大稳定机制区域。这些区域是单独通过区域内合外围边界的强度函数的极值特性定义的。 MSER的可以这样来解释：对于一副灰度图像I假设所有可能的阈值进行阈值化。我们认为像素点低于阈值的为黑色，高于或者等于阈值的为白色。如果我们把阈值为t的阈值化后的图像记作It并且以动画的形式一帧帧播放。那么我们第一眼看到的将是一张全白图像。然后对应于局部强度最小值的黑点将逐渐出现并且增长。在某一时刻，对应于两个局部灰度极小值的区域将会合并。最终，最末的一张图片将变成全黑。所有这些动画的帧中的连接元件集合就是所有极大区域的集合；极小值区域的获得方法则是将图像灰度进行翻转后，做同样的过程。MSER概念的正式定义以及必要的辅助定义见表1 在许多图片中，特定区域的大范围局部二值化是稳定的。这些区域因为他们有如下的特性使得我们对它比较感兴趣： 图像强度仿射不变 邻域的方差保持（连续）在D-〉D的转换中 稳定，因为只有在一系列阈值内框架几乎没有变化的极值区域被选了出来。 多尺度检测，因为整个过程中没有进行平滑，所以被检测的区域框架非常完整，非常大 这些所有极值区域的的集合的算法复杂度是O（n log log n）其中n是图像的像点个数。 ## 提出的鲁棒宽基线算法 显著区域检测。第一步，检测显著区域，通过对于灰度图像的计算我们获得MSER+ 在通过对其反转图像的计算获得MSER-。 测量区域。如果构造过程是仿射协变的，每个显著区域都要和任意大小的测量区域相联系。更小的测量区域更可能满足平面条件，同时在深度和方向上不会出现不连续。另一方面，小区域更加没有辨识力，也就是说，他们更加不可能唯一。增加测量区域的大小又会有把两幅图片中完全不同的部分背景包括进去。所以显而易见。最优的测量区域是场景内来确定的，而且每个显著区域的测量区域是不同的。在参考文献[21]中Tuyetlaars 和Van Cool将椭圆的显著区域面积增加了一倍来增加辨别力。同时能够保证区域跨过对象边界的可能性在可接受范围内。 在我们的算法中。我们选用了多尺度的策略区域，包括1倍、1.5倍、2倍、3倍的DR多边形。因为我们的方法是非常的鲁邦，我们增加区域面积获得了更高的分辨力，同时没有被显著区域的杂乱或者不平面而严重的影响，所以这是我们的算法的新奇之处。一般来说，在测量区域匹配中使用的是马氏距离。然而，这种测法的不鲁棒性是的，一个单独的错误测量就会使整个匹配过程失败。 不变性描述。在所有试验中，在对显著区域的协方差矩阵应用了对角化变换后，都使用了基于复数矩的方向不变性。同时这还是一个仿射不变的过程。由于方向和仿射的不变性相结合，使得该方法在彩色模型下也有相似的结果。由于本身的原因，仿射不变在大尺寸变化的问题下不能够成功。 鲁棒匹配。在几近平面的场景用稳定的不变描述来进行的测量称作是“好的测量”。不稳定、在非平面上计算的或者在深度和方向上不连续的策略被称作“差的测量”。 鲁棒相似点是专业计算的。我们把区域A的测量记作MAi .对于另一张图片上离MAi最近的k个区域B1,…,Bk，对应 的测量记作MB1i,…MBki. 我们对B1,…,Bk做一个投票，看谁和A的匹配性最强。在所有测量完成后再对投票进行求和。 投票最多的显著区域将被用来做试探性匹配。实验中，我们发现把k设成区域数量的1%时，会有较好的实验结果。而去预定数量一般都在100~1000的范围内，因此，k的值一般都在1到10之间。在当前的实现中，每个尺寸都有216个不变量。也就是说，一共有864次测量。216转动不变量在参考文献8中有详细描述，4个尺度的选择是通过试差法获得的，是运行速度和性能的一个折中。 由于每幅图像的不变量的权重以及对应的噪声都不一样，改过程成功的可能性的概率预测是比较复杂的。所以我们只能假设，错误的测量产生的投票是随即的，而不是共谋产生高的分数，好的测量更可能为正确的匹配投票。]]></content>
      <categories>
        <category>computer vision</category>
      </categories>
      <tags>
        <tag>机器视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一篇日志，看看自己能坚持写多久]]></title>
    <url>%2F2013%2F05%2F07%2Flife%2F2013-05-07%2F</url>
    <content type="text"><![CDATA[第一篇日志，看看自己能坚持写多久 记录生活也好，技术转载也罢，我博客一个接一个，不知道自己能坚持多久。发张图爽快爽快！]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>闲言碎语</tag>
      </tags>
  </entry>
</search>